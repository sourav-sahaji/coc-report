\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{outlines}
\usepackage{graphicx}
\graphicspath{{./images/}}

\title{\textbf{\huge 6-DoF Semantics-Aware Condition- and Viewpoint-Invariant Visual SLAM}}
\author{\textbf{\Large Sourav Garg}}
\date{}

\begin{document}

\begin{figure}
\centering
\includegraphics[scale=0.8]{QUT-SEF-Logo}
\end{figure}

\vspace{5cm}

\maketitle

\vspace{1cm}
\begin{center}
\textbf{
\emph{\LARGE Confirmation of Candidature\\}
}
\vspace{2cm}
\textbf{
\Large \emph{Principal Supervisor}: \\ Associate Professor Michael Milford \\
}
\vspace{1cm}
\textbf
{
\Large \emph{Associate Supervisor}: \\ Dr. Niko Sunderhauf \\
} 
\end{center}

\vspace{2cm}

\begin{figure}
\centering
\includegraphics{QUT-Robots-ACRV-logo}
\end{figure}

\pagebreak

\abstract{
Visual SLAM is a key robotic competency that involves simultaneously mapping an environment and localizing within that map primarily using visual cues. Although it has been the subject of significant attention over the past decade, Visual SLAM techniques still lack the robustness required to their universal application across many practical applications. In this thesis research, we will explore how we can use semantic learning and understanding to robustify three key components of visual SLAM: visual place recognition for localization, visual odometry for ego-motion estimation and the representational map of the environment.

The use of visual cues implies using robust representations of raw images to be able to correctly recognize places in the map that a robot revisits as well as to accurately estimate ego-motion in order to create a map of environment. Such image representations need to be robust against variations in the viewpoint and environmental conditions or alternatively, be able to accordingly adapt to such variations. Further, the representational maps created by state-of-the-art methods are generally only geometric; researchers have only recently started adding semantics to the maps to obtain interactive 3D reconstructions of environment. However, a semantic SLAM system should not just require semantic mapping, it should also be able to exploit that semantic information to improve localization robustness, resulting in a closed loop semantic SLAM framework which is able to generate better semantic labels as well.

We will particularly address the following research questions leading us towards semantic SLAM:
\begin{itemize}
 \item How can we characterize a visual SLAM systemâ€™s response to variations in environment and ego-motion, and consequently make it more robust with regards to both visual place recognition and visual odometry processes?
 \item How can visual semantic information improve the robustness and generality of a visual SLAM system?
 \item How can we develop a general purpose 6-DoF, semantics-aware SLAM system that is robust to condition and viewpoint variations?
\end{itemize}

The answers to these questions will enable us to build a 3D, semantically-rich reconstruction of environment which is far more suited to use in plethora of practical robotic applications including indoor monitoring, grasping and navigation.
}

\pagebreak

\tableofcontents

\pagebreak

\section{Introduction}

\subsection{Robotics}
Robotics is a rapidly growing field with an increasing number of applications in industry, household and military. The ultimate goal of creating human-like, rather advanced, artificially intelligent, completely autonomous robots, capable of performing multiple complex tasks is yet a long way to go. However, gradual research and development within different aspects of robotics has enabled the use of robots in specific practical applications. 

An interesting and crucial robotic competency, which will be explored in this report is \emph{SLAM}, that is, Simultaneous Localization and Mapping. It is the capability of a mobile robot to create a map of its environment while also keeping track of its location within that map. Such an ability is really important for a mobile robot because the foremost requirement for accomplishing any complex task is \emph{awareness}, that is, a robot being aware of its operating environment as well as itself within that environment. While a number of practical applications may use pre-built map of the environment, identifying changes within that map, which is often the case, would still require SLAM. Autonomous vehicles or ADAS systems, retail store and warehouse stock management, deep sea bed modeling, domestic services, military explorations etc. are all the practical use cases of a SLAM based robotic solution.

\paragraph{SLAM in practice:} 
In most of the examples quoted above, one thing is common, that is, the mobility of robot. A robot performing sophisticated tasks would in general have mobility to explore its environment and then perform the task. SLAM, therefore, becomes a fundamental requirement for a mobile robot to enable such an exploration capability. Figure:\ref{fig:robots} shows different robots in various application domains, and all of them being mobile require some form of SLAM algorithm to perform their respective tasks unless they are hard-coded for path planning and navigation. For example, a cleaning and lawn mowing robot needs to know what part of the premises it has already visited and cleaned/mowed, and therefore what are the remaining portions to yet explore. Similarly, a social robot like Pepper cannot effectively interact with humans or its environment unless it has a capability of knowing what is around and how to approach the entities in its environment. Such robots in fact require a higher-level understanding of the world using the on-board sensors only, therefore, creating a requirement of performing SLAM using only, say, cameras. The vision-based SLAM is commonly termed as Visual SLAM and will be explored in details in subsequent sections.

\begin{figure}[htbp]
\centering
 \begin{tabular}{cc}
  \includegraphics[width=3cm,height=3.75cm]{robot-3-SoftBank_pepper} &
  \includegraphics[scale=0.15]{robot-2-Roomba} \\
 (a) Social Robot - Pepper &
  (b) Domestic Cleaning Robot - Roomba \\
  \includegraphics[scale=0.15]{robot-4-lawn-mower} &
  \includegraphics[width=3cm,height=3cm]{robot-1-Foundary-Automation} \\
  (c) Lawn Mowing Robot - RS630 &
  (d) Industrial Robot in a Foundry \\
 \end{tabular}
 \caption{Robots in various application domains. Source: (a) and (c)- Wikipedia, (b) Larry D. Moore CC BY-SA 3.0, (d) www.robotshop.com }
 \label{fig:robots}
\end{figure}

\subsection{SLAM: \emph{Is it solved?}}
This question here is not just asked generally by those who have heard of SLAM as a research term before, but by many in robotics community as well. Sebastian Thrun and Jose Neira in \cite{frese2010interview} answered to this in 2010, confirming that it is solved for static environments and the pertaining problems are: dynamic objects within the environment and semantic understanding of the environment. The interview also discussed the community's shift towards visual SLAM, given that the digital cameras are an affordable source of information-rich data. 

Cadena et al. \cite{cadena2016past} in 2016 highlighted the breadth of SLAM as a research topic and stated that the problem is well posed only after specifying the type of robot, the environment and the performance requirements. It is further stated that modern SLAM solutions can easily fail for challenging environment and rapid robot motion, and can be considered solved for only certain constrained settings. Finally, they discuss the research direction of SLAM system driven by the motivation of robust perception and characterized by robust performance, high-level understanding, resource awareness and task-driven perception.

We define the locus of our research using the thoughts discussed above and focus on visual SLAM, and solve some of the challenges related to dynamics of the environment and semantic understanding.

\subsection{Visual SLAM}
Visual SLAM is a robotic competency to simultaneously localize and map the environment using only visual cues. These visual cues can come from a monocular camera, a stereo-rig or RGB-D sensors. The key components of a visual SLAM system are: \emph{Visual Odometry}, \emph{Place Recognition} and \emph{Representation Map}. \emph{Visual Odometry} is the capability of a mobile robot to estimate ego-motion using only vision as input. \emph{Place Reconstruction} means the ability to recognize a place when the robot revisits that place. \emph{Representation Map} mainly implies the way of storing the visual information in form of a map of places or visual landmarks. We will briefly look into the significance of each of these components before reviewing the corresponding literature with the help of Figure:\ref{fig:slamExample}.

\paragraph{Visual Odometry:}
Visual Odometry in a 6-DoF framework estimates camera viewpoint and orientation in 3-D with respect to origin in some arbitrary frame of reference. Figure:\ref{fig:slamExample} shows blue-colored camera-shaped symbols representing the camera orientation and green-colored connections representing the camera trajectory, as the camera moves in the environment. Though it seems that visual odometry alone should suffice in forming a map of the environment and localize the camera with respect to its origin or previous locations, it is generally not the case. Visual odometry or any other source of odometry suffers from the problem of \emph{drift}. This \emph{drift} causes an error in the trajectory estimation which accumulates over time as there is no possible way to correct its trajectory for visual odometry by itself. Moreover, a visual odometry system considers its environment as an incremental infinite space open for infinite exploration, that is, it can never realize a revisit to a place or part of its environment, unless informed using external input. This external input actually arrives from a \emph{loop closure}, obtained using place recongition.

\paragraph{Visual Place Recognition:}
Visual Place Recognition essentially provides \emph{loop closures} to a visual odometry system to get rid of trajectory \emph{drift} and help realize that a place or part of environment can possibly be revisited, recognized and hence marked as a \emph{landmark} in its representation map. Visual Place Recognition requires a collection of places through the representation map or a separate reference database to match a query image. Once the query place is correctly matched to a place, the camera can then be re-localized with respect to the matched place. The trajectory drift can then be corrected and visual odometry system can gather further pose measurements with respect to the matched place. The unmatched query images form the part of the reference database. Figure:\ref{fig:slamExample} shows loops in the trajectory which means that there had been instances of revisiting some places in the environment, which led to nearly perfect 3-D reconstruction.

\paragraph{Representation Map:}
A 3-D reconstruction, a 2-D occupancy grid map, or a 1-D topological map, are all different ways of representing the environment witnessed by the robot. Most of the times, the type of representation is decided based on the application of visual SLAM. It is not possible to create a proper map without an accurate visual odometry and place recognition system. Figure:\ref{fig:slamExample} shows a 3-D reconstruction of the environment. Such a representation is sometimes useful in representing places within the map itself and matching new places with respect to the 3-D points or 3-D landmarks with or without the use of a separate reference database which is generally a set of 2-D images.

Now, that we understand the significance of each of the components of a visual SLAM system, we will explore the relevant literature to find out the research gaps.


\begin{figure}[htbp]
\centering
 \includegraphics[scale=0.8]{SLAM}
 \caption{3D reconstruction of environment using LSD-SLAM \cite{Engel2014lsd}}
 \label{fig:slamExample}
\end{figure}

\section{Literature Review}
\subsection{SLAM Overview}
The solutions to SLAM problem attempt to recover the \emph{landmarks} in the environment and the \emph{robot pose} with respect to the landmarks. These landmarks constitute the representation map of the environment being explored. The robot is able to then localize itself with respect to the landmarks in the map while also reporting newly discovered landmarks. The revisits to the landmarks are identified by recognizing landmark and hence a \emph{loop closure} is added to the system so that any accumulated trajectory drift is nullified.

The most common and traditional approaches for SLAM use probabilistic methods to maintain both the pose of the robot and the location of the landmarks. The robot poses and the landmarks are sequentially filtered as robot continues to observe the environment by updating the underlying probability distribution. This can be achieved using Extended Kalman Filter \cite{Chatila1985}, Rao-Blackwellized Particle Filter \cite{montemerlo2007fastslam}, Expectation Maximization \cite{thrun2005probabilistic} etc. The second set of methods uses sparse graph of constraints and non-linear optimization for recovering map and camera poses as in \cite{Olson2006, Thrun2006, Sunderhauf2012b}. These methods generally use a factor-graph to optimize for the robot pose given the set of relative poses with respect to the landmarks. 

The third and relatively newer paradigm, with respect to visual SLAM, includes Structure from Motion (SfM) \cite{beardsley1997sequential} techniques with help of Bundle Adjustment \cite{Triggs2000}, where the scene structure and camera motion, both are jointly recovered using optimization techniques \cite{Strasdat2010}. Bundle Adjustment is computationally expensive and had been the primary tool to solve SfM problem, where the goal is to reconstruct 3-D from a relatively small set of 2-D images using projective geometry. This doesn't necessarily require real-time operation, unlike SLAM, where real-time operation is required for most of its applications. Therefore, the concept of \emph{local} bundle adjustment is more popular for robotics community, where optimization is done over a local temporal window. Such methods had been initially used for visual odometry derived from stereo \cite{Nister2004, Pollefeys2004} or monocular vision \cite{Gr2015, Klein2007, Nister2004, Strasdat2010} and recently for visual SLAM \cite{Mur-Artal2015,Mouragnon2006}. 

The fourth and the last category of SLAM solutions is inspired from biology \cite{milford2008robot} and builds on the cognitive abilities of animals to localize and map their environment, for example, rodents as shown in \cite{Milford2004} and ants in \cite{Collett2010}.

\subsection{Visual Place Recognition}
A \emph{place} is defined as a distinct location or a landmark in the representation map of an environment. \emph{Visual Place Recognition}, therefore means recognizing a place using visual information whenever this place is observed again. In context of visual SLAM, place recognition is important for generating loop closure information. The recognized places provide the second source of information about localization within the map, first being the motion estimation based robot's pose with respect to the map. Therefore, extracting the relative pose for the loop closure provided by place recognition helps motion estimator (visual odometry in some cases) correct its accumulated trajectory and scale drift. 

The places can be visually described using the image features which can be broadly classified into two categories: \textbf{local} and \textbf{global} image features.

\paragraph{Local Image Features:} The local image features have a corresponding location within the image associated with each feature and are generally described using a local image descriptor. The most common and efficient approaches for place recognition using point-based local features employ Bag of Words (BoW) approach \cite{Nister2006,Sivic2003}. The image features like SIFT \cite{Lowe2004}, SURF \cite{Bay2008}, BRIEF \cite{Calonder2010}, ORB \cite{Rublee2011} etc. have been used in a BoW framework in FAB-MAP 2.0 \cite{Cummins2010}, LSD-SLAM \cite{Engel2014lsd}, ORB-SLAM \cite{Mur-Artal2015} and others \cite{Galvez-Lopez2012, Mur-Artal2014}. There exist further extensions to these methods for high performance, for example, building an incremental visual words vocabulary on-line \cite{Angeli2008}, using BoW-Pairs \cite{kejriwal2016high}, adding more geometrical constraints between the words \cite{CADENA} etc.. These methods and in general, point-feature based methods, work remarkably well with viewpoint invariance and can be used for wide-baseline stereo matching efficiently. However, they are susceptible to extreme variations in the appearance of the environment as well as feature redundancy that arises due to repeated, bland or texture-less environment. Such conditions lead to, what is know as, \textbf{perceptual aliasing} as shown in Figure:\ref{fig:perceptualAliasing}, that is, two different places (or features) appear to be similar. The variations in appearance, as mentioned, can also arise due to environmental conditions like different time of day, weather or season, or due to shadow, motion blur or varying illumination etc. for example in scenarios as shown in Figure:\ref{fig:conditionVariations}.

\begin{figure}[htbp]
\centering
 \includegraphics[scale=0.5,clip,trim=0cm 1.2cm 0cm 0cm]{perceptual-aliasing}
 \caption{\emph{Perceptual Aliasing} between images in first and second row shows perceptual similarity between these places, though belonging to different physical locations in the map. \cite{cummins2007probabilistic}}
 \label{fig:perceptualAliasing}
\end{figure}

\begin{figure}
 \centering
 \begin{minipage}{0.4\textwidth}
\raggedright
\includegraphics[scale=0.9]{nordland-seasons}
\centering(a)
 \end{minipage}
 \hspace{1cm}
\begin{minipage}{0.4\textwidth}
\raggedleft
  \includegraphics[scale=0.51]{Alderley-day-Image02477}\\
  \includegraphics[scale=0.51]{Alderley-night-Image03670}\\
  \centering(b)
\end{minipage}
\caption{Vast appearance changes in the environmental conditions can cause same place to appear very different due to variation in (a) seasons \cite{lopez2017appearance}, (b) weather and time of day \cite{Milford2012}.}
\label{fig:conditionVariations}
\end{figure}


\paragraph{Global Image Features:} The second method of describing places uses global image descriptors. Such features include HoG \cite{Naseer2014}, PCA \cite{Krose2001}, WI-SURF \cite{Badino2012}, BRIEF-GIST \cite{Sunderhauf2011} etc. that describe the whole image with a single feature. The methods like SeqSLAM \cite{Milford2012}, LSD-SLAM \cite{Engel2014lsd}, Photometric Bundle Adjustment \cite{alismail2016photometric} and others \cite{Naseer2014,McManus2015} have proven the successful use of whole-image matching for place recognition. These image matching approaches fall in the category of \textbf{direct} or \textbf{featureless} methods. Unlike local feature matching, these methods can be used efficiently for only narrow-baseline matching and therefore, are susceptible to large variations in the viewpoint of the scene. Though, they can guarantee sub-pixel accuracy in matching, the computation complexity goes upward, especially when an image pyramid is used to tackle viewpoint variations. As opposed to local point features, these techniques work well with vast changes in appearance and conditions of the environment. 

\paragraph{Hybrid and Deep-Learned:} There are some of the place recognition methods that make use of a combination of local and global image descriptors. Such approaches have been shown to work well with both condition and viewpoint variations as in \cite{McManus2014}, \cite{Milford2008} and \cite{Niko2015}, but they either require site-specific training, are not scalable or lack generality. Also, the use of deep-learned features as in \cite{chen2017deep,chen2014convolutional} has not been shown to be helpful for visual SLAM beyond 1D route traversals.

\paragraph{Summary:}
The local image features suffer from appearance and condition variations and global image features are susceptible to viewpoint variations. The condition- and viewpoint-invariant place representation using deep-learned features looks promising, but none of such representations have been ever used to estimate relative 3D pose between the pair of matching places, also termed as \textbf{loop closures}. Hence, it remains a challenge to develop a better place representation or use the existing ones in such a way that they can be integrated in a 6-DoF visual SLAM system. Such a system can then be used to create 3D maps, therefore, leading to numerous opportunities of interacting with such a map in order to develop practical SLAM applications. 

\subsection{Visual Odometry}
\paragraph{Motion Estimation:} A visual SLAM system needs a source of motion information in order to build a map with metric relationship among places that it represents. There are several methods that use different types of sensors to get this motion information, for example, IMU (Inertial Measurement Unit) in \cite{Kneip2011, Milford2014}, GPS (Global Positioning System) in \cite{Agrawal2006, Floros2013}, LASER in \cite{Kohlbrecher2011}, \cite{Paul2010}, robot-wheel odometry in \cite{Bazeille2011}, OBD (On-Board Diagnostics) data in \cite{pepperell2015automatic,Pepperell2014}, Hall-Effect sensor in \cite{Pepperell2014}, DVL (Doppler Velocity Log) dead reckoning in \cite{Mahon2008} etc. On the other hand, a purely vision based SLAM system only uses visual cues to estimate ego-motion. This is termed as \emph{Visual Odometry} and is used quite often in visual SLAM systems. Although the use of dedicated sensors provides an accurate and regular estimation of motion as compared to visual odometry, it is still preferable because it eliminates the use of an extra sensor and the overhead of interfacing and sensor fusion.

Visual Odometry methods extract the camera motion (ego-motion) information from the pair of consecutive images taken from a different viewpoint due to camera movement, but with visual overlap. A mobile robot mounted with a camera can use visual odometry to estimate its pose with respect to its origin and therefore have an estimate of its trajectory. Visual Odometry doesn't necessarily imply keeping track of the visual information in order to form a map of the environment. However, most of the recent approaches have the capability to reconstruct their environment in 3-D as we explore these methods in subsections below.

\paragraph{Feature Tracking or Direct Matching:} There are a number of methods devised for visual odometry that are based on local features which are either detected as \emph{corners} like Harris \cite{Harris1988}, FAST \cite{EdwardRosten} etc., \emph{blobs} like SIFT \cite{Lowe2004}, SURF \cite{Bay2008}, CenSurE \cite{Forsyth2008} etc. or \emph{edgelets} \cite{klein2008improving}. All these methods are based on local feature extraction and matching, and are therefore efficient in wide-baseline matching leading to its high viewpoint-invariance. On the other hand, there are methods that use direct whole image matching for semi-dense \cite{Forster2014,Engel2013} or dense \cite{Newcombe2011,Tykkala2011} 3-D reconstruction of environment. These methods are generally more suitable for narrow-baseline matching and are capable of sub-pixel accuracy. Though these methods cannot handle large viewpoint variations, they certainly perform better than local features based methods when it comes to robustness towards appearance variations or repeated and texture-less environments. The authors in \cite{alismail2016direct} have also shown robustness towards low-light environment using direct visual odometry.

\paragraph{Deep VO:}
The authors in \cite{konda2015learning} explicitly model visual odometry problem in a deep learning framework using stereo image pairs. The other deep-learned models that estimate pose between images and are closely related to visual odometry are Posenet \cite{kendall2015posenet}, Deep Tracking \cite{dequaire2016deep} and Sfm-Net \cite{vijayanarasimhan2017sfm}. The use of deep learning for motion estimation using visual cues is in its stage of infancy as all these methods are very recent and either require range information, environment-specific training or extensive engineering and have not be shown to be scalable similar to what a visual SLAM system would require. Moreover, all these systems use images under ideal environmental conditions and are likely to fail in adverse conditions because lack of generality in image data while training.

\paragraph{Summary:}
The visual odometry approaches suffer from similar issues like visual place recognition. The choice between local and global image features has trade-off between viewpoint- and condition-invariance of image representation. Moreover, most of the visual odometry methods require parameter tuning according to the environment and are prone to failures due to rapid camera motion, low-light environment and radiometric variations. The deep-learning based solutions haven't yet been convincingly able to offer better solution either. The failure of visual odometry is catastrophic for a visual SLAM system if it solely relies on it for motion estimation, hence it remains a challenge to develop robust motion estimation solution using visual odometry or some hybrid approach.

\subsection{Representation Map and 3D Reconstruction}
Mapping is an integral part of a SLAM system. It can also be considered as a product of SLAM system because the generated map is actually useful in various practical applications. The earliest version of SLAM comprised a map of landmarks which has now been modernized to comprise a realistic 3-D reconstruction of the environment. The maps generated using visual odometry and drift-corrected using visual place recognition jointly optimize the robot poses and the visual landmarks. 

\paragraph{Types of Maps:}
An environment representation map acts as the reference database for a place recognition system, unless it is based on pure image retrieval, where actual location of a place does not really matter as in \cite{Cummins2010} and \cite{Nister2006}. For all other cases, these representation maps can be of very different nature in terms of how they represent the environment. They can be classified into three categories: \textbf{topological}, \textbf{metric} and \textbf{topometric}. A \emph{topological} map has only a relative arrangement of places with respect to each other, for example, in \cite{Lui2012, Milford2012}. The places in such a map are merely set of points on a 1D route traversal and are connected with the logic of their order of occurrence. On the other hand, a \emph{metric} map is built using physical location information of a place as in \cite{Gutmann2008, Klein2007}. Here, the places are connected using metric distance (up to scale) and closely represent the real map of the environment. The third category of representation is a combination of both these approaches that uses topological-metric or \emph{topometric} map as in \cite{Angeli2009, Bazeille2011, Konolige2011}. These representations help in handling large-scale maps or defining a logical layer over the metric maps for an easier interface. For example, semantic maps of environment \cite{sunderhauf2016place,flint2010growing} can enable higher-level reasoning and planning.
\paragraph{3D Maps:}
The representation maps often describe places in 3D using range information either from LASER \cite{Paul2010,Stewart2012}, RGBD sensors \cite{Henry2012,Whelan2013} or stereo images \cite{CADENA,alismail2016direct,Davison2007}. Though explicitly sensing 3D information makes the problem simpler, it comes with other overheads of using more sensors and complex sensor fusion techniques. The 3D structure information of places in a map can also be extracted from moving cameras using Structure from Motion (SfM) \cite{beardsley1997sequential} and local Bundle Adjustment \cite{Triggs2000,Engels2006} techniques. Most of these methods use local image feature for sparse \cite{Davison2007, Mur-Artal2015}, semi-dense \cite{Engel2014lsd, Mur-Artal2015b} or dense \cite{Newcombe2011} reconstruction of environment. Some preliminary work has been done in \cite{alismail2016photometric} that performs featureless, and therefore direct photometric bundle adjustment.

\paragraph{Summary:}
The state-of-the-art visual SLAM methods use 3D maps demonstrating dense reconstruction of the environment. Such a representation is indeed important for enabling robot's interaction with the environment similar to humans. However, it still lacks semantics which are important to impart meaning to the pixels or segments in a 3D reconstruction for an effective interaction. The use of semantics in visual SLAM has been explored in next section.

\subsection{Semantics in visual SLAM}
\paragraph{Semantics Within and Across Places:}
The use of semantics in a visual SLAM system has been explored from two different perspectives. The first one explores the meaning \textbf{within a place} by semantically labeling the objects \cite{ranganathan2007semantic}, patches \cite{posner2008online}, pixels \cite{flint2010growing}, superpixels \cite{xiao2009multiple} etc. within the image, often in conjugation with range information. The second perspective is a broader view of these places that performs categorization \textbf{across the places} within a map, usually on a larger scale, for example, in \cite{stachniss2005semantic,sunderhauf2016place,pronobis2011semantic}. However, in both the scenarios, use of semantics enables higher-level reasoning and interactive modeling of the environment.

\paragraph{Object Semantics:}
There have been several attempts towards developing a semantic SLAM system based on objects encountered in the environment. The use of pre-trained 3D object models in \cite{civera2011towards} and semantic object-class segmentation in \cite{stuckler2012semantic}, is one way to incorporate object semantics in an off-the-shelf visual SLAM system. Another set of approaches include SLAM frameworks where objects are integral part of the optimization equation as in \cite{fioraio2013joint,salas2013slam++,vasudevan2007cognitive,galvez2016real}. The authors in \cite{salas2014dense} combine object recognition and semantic image segmentation for dense semantic SLAM. While all these approaches are \textbf{object-centric}, the authors in \cite{flint2010growing,nuchter2008towards} also consider semantics of structural elements like wall, floor, ceiling etc. to semantically characterize the visual SLAM system. Furthermore, the use of deep learning frameworks for more powerful semantic object recognition has also helped in sparse \cite{girshick2014rich} or dense \cite{long2015fully,chen2014semantic} object-centric semantic segmentation as shown in \ref{fig:denseObjectSegmentation}.

\begin{figure}
 \centering
 \includegraphics[scale=0.25]{denseObjectSegmentation}
 \caption{Dense Semantic Object Segmentation as proposed in \cite{long2015fully} using Fully Convolutional Networks.}
 \label{fig:denseObjectSegmentation}
\end{figure}


\paragraph{Place Semantics:}
Although, \emph{object-centric} approaches look very promising, they usually neglect the importance of visual scenes that do not really have any particular object in focus, for example, a mountain range, crop-field, train-station etc. The need of semantics for such a \textbf{place-centric} scene has been emphasized in \cite{zhou2014learning}, where the authors train a deep-convolutional network on \emph{place-centric} images to obtain semantic place categories. Similarly, authors in \cite{Patterson2012SunAttributes} use a plethora of semantic scene attributes to label different places. Furthermore, authors in \cite{laffont2014transient} use transient semantic attributes, like time of day, weather, season etc. to characterize places.

\paragraph{Summary:}
The use of semantics in most of the research work is limited by the type of semantics and the extent to which they are integrated within a visual SLAM framework. The semantics related to objects as opposed to places and those focusing on structural elements as opposed to transient attributes, limit the generality and scalability of their use. Also, very few of the proposed methods utilize semantics to an extent where both localization and mapping can benefit from it. Further, there are even fewer such methods that have demonstrated use of SLAM framework to improve semantic labeling of either objects or places. Hence, a holistic approach taking these challenges and limitations into account is currently needed to develop a \textbf{Semantic SLAM} system.


\section{Research Problem}

\subsection{Research Gap}
The research gap following the literature review can be summarized in following points:
\begin{itemize}
 \item It remains a challenge to develop a robust place recognition system that can be seamlessly integrated into a 6-DoF visual SLAM system.
 \item Visual Odometry solutions are brittle to variations in environmental conditions, rapid camera motion and radiometric variations.
 \item The failure of visual odometry is catastrophic for visual SLAM system, hence a hybrid approach towards motion estimation is a must.
 \item The geometric maps created by state-of-the-art methods lack semantics and therefore hinder effective interaction with the reconstructed environment.
 \item The use of semantics in visual SLAM is limited to either localization or mapping and lacks an integrated approach from which potentially both, semantic labeling and visual SLAM, can benefit.
\end{itemize}

\subsection{Problem Statement}

The overarching research problem following the literature review and the identified research gap can be stated as: \\
\textbf{How can we develop a general purpose 6-DoF semantics-aware visual SLAM system which is condition- and viewpoint-invariant or alternatively, adapts to changes in environment as well as ego-motion?}

\subsection{Research Questions}

In order to address the problem as described above and to enable such a SLAM system, we need to answer the following questions:
\begin{outline}
 \1 How can visual odometry and place recognition be made more robust to variations in environment and ego-motion?
 \2 How can we characterize the place recognition and visual odometry systems' response to variations in camera viewpoint and environmental conditions?
 \2 How can we estimate ego-motion under extreme environmental conditions and rapid camera movements?
%  \2 How can ego-motion information help improve visual place recognition?
 
 \1 How can visual semantic information improve the robustness and generality of a visual place recognition system?
 \2 How can semantic segmentation \emph{across} places within an environment help improve a place recognition system?
 \2 How can semantic segmentation \emph{within} places (images) be used to learn salient regions for place recognition system?
 
 \1 How can we develop a general purpose 6-DoF semantics-aware visual SLAM system that is robust to condition and viewpoint variations?
 \2 How can we estimate a 3-D relative pose for matched places under extreme appearance variations?
 \2 How can we derive a 6-DoF semantics-aware visual SLAM system using the components developed so far?
  
\end{outline}


\section{Research Plan}
The research plan discusses the possible solutions to the research questions mentioned in previous section. 
% Figure:\ref{fig:researchPlan} shows the schematic of research plan with all the research questions addressed within different modules of the visual SLAM system.

% \begin{figure}
%  \centering
%  \includegraphics[scale=0.45]{research-plan-block-diagram}
%  \label{fig:researchPlan}
% \end{figure}

\subsection{Characterization of a Visual SLAM System}
The visual SLAM system or any other similar competency developed as a robotic application can broadly be understood to comprise these three general components:
\begin{enumerate}
 \item \emph{Environment} where the robot operates or learns about its surroundings.
 \item \emph{Robot and Sensors} for interacting with the environment and enabling sensing and responding in some form.
 \item \emph{Algorithm Interface} is what enables the interaction between the robot and the environment and is designed specific to the application.
\end{enumerate}

In order to develop a well-performing robotic application, it is important to understand the aforementioned components and their characteristics that significantly impact the performance. Therefore, it remains one of the research questions to identify such characteristics specific to the application and appropriately model them.

\subsubsection{Characterizing Visual Odometry and Place Recognition}
Visual Simultaneous Localization and Mapping constitutes: \emph{Visual Odometry}, \emph{Visual Place Recognition} and \emph{Mapping} of environment. The state-of-the-art visual SLAM systems or any of its individual constituents are often developed based on certain assumptions with respect to its environment, its sensors or the operating characteristics. Though it is not possible to estimate all the possibilities of operating characteristics beforehand, it is vital to understand those significantly impacting the performance.

\paragraph{SLAM systems at disposal:}We plan to explore the state-of-the-art methods like SeqSLAM \cite{Milford2012}, ORB-SLAM \cite{Mur-Artal2015}, LSD-SLAM \cite{Engel2014lsd} etc. in order to understand their certain characteristics, for example, invariance to camera viewpoint and environmental conditions - both lie at the core of a general purpose visual SLAM system.

\paragraph{Data Gathering:}The simultaneous condition- and viewpoint-invariance of a 3D visual SLAM system has yet not been achieved due to challenges involved. Therefore, it is important to understand the performance sensitivity of the existing algorithms towards these variations. Such an analysis will also require the appropriate data to conduct tests. However, the availability of real or simulated data corresponding to condition and viewpoint variations is limited. This is mainly because it is difficult, time-consuming and expensive to collect real data with a sound ground-truth generation system especially for condition variations - which essentially means traversing places at different times of day, in different weather conditions and across seasons. Similarly, capturing images of a place for all possible camera viewpoints is not always feasible. It seems relatively easier to do the same in simulation, given there exists a 3D model of a city or alike with sufficient rendering, so that condition- and viewpoint-varied traverses could be infinitely generated, but such a ready-to-use model does not exist and needs a lots of effort.

\paragraph{Simulation:}We plan to collaborate with peers to simulate a city-like environment in 3D and obtain repeated traverses with varying viewpoint and conditions of the environment. Along with that, we will also make use of some of the existing and newly collected real world data which may not be sufficiently large as compared to the simulated one, but be rich enough with respect to the variations in the environment such that the performance curves so obtained will look similar to those using simulated world. The choice of relying on simulation than real world for characterizing the visual SLAM systems is merely due to large amount of effort required in the latter and collaboration opportunities at disposal for the former. 

\paragraph{Conclusion:}The characterization in this form will immensely help in performance benchmarking using simulated data for different robotic applications. This will also enable the enhanced understanding of parameters of the system that particularly drive its performance for the variations induced in the test data.

% \subsubsection{Adapting to Environment}

\subsubsection{\emph{Coarse} Ego-motion Estimation}
\paragraph{Motivation:}The ego-motion estimation is an important competency for robotic tasks that involve movement. Visual Odometry is the means of estimating ego-motion using visual cues. The state-of-the-art visual odometry solutions are often prone to failures because of fast camera motion, motion blur, photometric and radiometric changes in appearance of environment etc. The failure of visual odometry is catastrophic for a visual SLAM system as the robot immediately loses its localization information and cannot relate current motion estimates with those collected earlier. The camera motion also plays a significant role in place recognition especially when repeated traverses of an environment exhibit different motion pattern, which makes it harder to recognize places. This is because a fixed-size temporal neighborhood window centered at a place will no longer contain similar places in different traverses.

\paragraph{Speed Normalization:}We will look into the scope of improving place recognition using visual odometry. Most of the practical visual place recognition applications for example, an autonomous vehicle, do not exhibit uniform motion during the journey. The motion estimation can help in speed normalizing the collected imagery, so that the places are separated by a constant physical distance instead of constant number of frames.

\paragraph{Motion Estimation:}We also plan to develop a motion estimator or predictor as a backup switch for state-of-the-art visual odometry methods such that it provides some source of information to roughly localize the robot even if such an estimate is less accurate or less precise. It is important because such estimates can be improved at a later stage by either using visual place recognition or improved visual odometry.


\subsection{Semantics for Visual Place Recognition}
The use of semantic information has recently become easier because of deep-learned classifiers and regressors \cite{girshick2014rich,zhou2014learning} that can be trained on very large image data to precisely predict the semantics of the test image. These semantics can be used to achieve a meaningful interpretation of places and maps used in a visual SLAM system. We plan to make use of this semantic information to improve visual place recognition and visual SLAM system in different ways as described in following subsections.

\paragraph{Varying Semantic Representations}
One of the simplest use of semantics seems to be imparting meaning to the environment where robot operates in form of a semantic map. The place categorization \cite{zhou2014learning} and scene attributes detection \cite{Patterson2012SunAttributes} using semantics has not been explored within a place recognition framework so far. Unfortunately, these semantic classifiers do not provide sufficient information regarding environmental conditions like time of day, season or weather etc., but mostly characterize the structure of the scene leading to particular place category labels. These transient conditions of environment have been semantically explored separately in \cite{laffont2014transient}. Figure:\ref{fig:robustSemRep} shows the different image classifiers labeling the different attributes and categories pertaining to the image.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.2]{robust-semantic-representation}
 \caption{The image classifiers trained using different datasets and different attributes/categories provide a variety of semantic labels.}
 \label{fig:robustSemRep}
\end{figure}

% \paragraph{Planned Approach:}
% We plan to use a deep convolutional neural network like ImageNet-CNN \cite{krizhevsky2012imagenet} within a multi-label classification problem. The multiple labels are with respect to environment description in two dimensions, that is, the scene structure and the transient attributes. The former will characterize the environment in terms of what that place is and what objects are there within, and the latter will describe the environmental conditions especially for the outdoor scenes. The image database that we can use for this task will be derived from the existing large image databases like ImageNet \cite{deng2009imagenet}, Places2 \cite{zhou2014learning}, AMOS \cite{jacobs2007consistent} etc. The challenging part here would be to arrange the ground truth for the images. We plan to use cross-labeling of images using state-of-the-art image classifiers so that all the training images have both kinds of labels as we discussed above. This should also have a filtering mechanism so that only highly confident labels are passed to the training system. The other possibility is to use visual place recognition to generate and transfer labels from ideal appearance conditions to their counterparts under extreme appearance change. The 2-D labels also imply a multiplicative factor for the number of images that should be used for training in order to have different classes balanced in the system. 
% 
% \paragraph{Expected Outcome:}
% We expect to set up a performance benchmark for outdoor scene categorizations as current state-of-the-art approaches, mostly focused on objects, are not able to correctly recognize objects when environmental conditions are not ideal. We will also study the utility of these robust semantic representations of the environment for a visual place recognition system. It would be expected that such a place description would be able to handle the variations in environmental conditions better and hence, improve the performance of visual place recognition system.

\subsubsection{Semantic Segmentation of Environment}
% \paragraph{Motivation:}
The environment plays a key role in determining the performance of any robotic application because it is often the case that many of the applications are designed specific to certain environment types and therefore are brittle towards significant variations in the environmental settings. In context of visual place recognition and visual odometry, the variations in environment are induced mainly due to \emph{scene structure} and \emph{transient conditions}. The scene structure is actually the basis for differentiating places from each other, but frequent transitions within different environments call for different parameter settings for improved performance, for example, urban canyons versus highways or forests, or outdoor vs indoor etc. On the other hand, transient conditions like time of day, weather and season, though representing the same place, remarkably change the visual appearance of the scene. Therefore, it is necessary to understand the environment and allow possible adaptive robot behavior for variations in the environment.

\paragraph{Recognizing places in varying environment:}The condition-invariant place recognition methods such as SeqSLAM \cite{Milford2012}, SMART \cite{Pepperell2014}, and others \cite{Naseer2014,Niko2015,Maddern} have been shown to able to recognize places under the influence of extreme variations in conditions of the environment. These variations are mainly \emph{global} which means that they are static with respect to space but not time, for example, change in season, weather or time of day will only affect the appearance of a particular place in the environment when it is visited after a certain interval of time. On the other hand, the spatial neighborhood of that place will be similarly affected by the changes in the environmental conditions. 

We are rather interested in local variations in the environment where global conditions changes may or may not occur. Examples of such local variations are mainly related to transitioning from outdoor to indoor environments, texture-less to cluttered scenes, urban canyons to forest roads etc. The seamless transition between such environmental settings is only possible by segmenting the environment into different chunks based on their appearance attributes.

% \paragraph{Segmenting the environment:}The place recognition methods generally employ a measure of matching places which can be either whole-image based as in SeqSLAM \cite{Milford2012} or point features based collective score as in FAB-MAP \cite{Cummins2009}. These place matching scores effectively discriminate places from each other and hence could also be used to create segments based on these scores. The idea is to build a self-similarity matrix for a given dataset and then statistically perform segmentation or data clustering based on the affinity scores. The segments so formed can then be used to define neighborhood region for a place such that place recognition performance may be improved based on collective matching of different segments. This would also require us to collect datasets which possess transitions from one type of environment into the other quite often.
% 
% \paragraph{Conclusion:}The characterization of environment with respect to local variations in overall appearance of environment will help in dynamically adapting the robot behavior accordingly. For example, place matching and motion estimation can be improved by tuning the system parameters in order to accommodate the variations in environment.

% The segmentation of places in an environment has been explored in earlier sections based on place matching scores. 
The use of semantics for segmenting the environment can be advantageous because firstly, deep-learned features are more discriminative \cite{chen2017deep} as compared to hand-crafted ones and secondly, it can help in filtering or shortlisting of the place matching candidates by performing a semantic matching first.

\paragraph{Planned Approach:}
We plan to use semantic category or attribute labels for different places in the image database to temporally cluster them in order to create a semantic segmentation of the environment. For example, an office environment could have different semantic regions like cubicles, canteen, corridor, restrooms etc. Similarly, a vehicle traveling on road can encounter various semantic zones, like an urban canyon, highway, tunnel, forest road, ocean road etc.

\paragraph{Expected Outcome:}
The use of semantic labels to segment the environment can be firstly helpful in reducing the search space for place recognition and performing a coarse localization using the place categories or attributes. Therefore, using a very large database for place recognition, we will benchmark the gain in computation time and recognition accuracy using this coarse localization before performing \emph{exact} place recognition. Secondly, we will investigate the use of semantic segmentation for seamless transitions within environment when the appearance of the environment changes moderately for example, a bland to cluttered scene, or drastically, for example, an outdoor day time scene to low- or artificially-lit indoor environment. Such transitions pose challenges for state-of-the-art systems as they are generally parameter-tuned to single type of environmental settings. Hence, we expect improvement in localization performance for visual place recognition system. Figure:\ref{fig:semantic-across-example} shows an example of transition from an outdoor to indoor environment along with some images displaying variations in visual appearance of the environment.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.2]{Residence-IO-Map-Images}
 \caption{A coarse segmentation of a route traversed from outdoor to indoor. The different segments of the environment may have different visual appearance and lead to performance degradation of a visual place recognition system.}
 \label{fig:semantic-across-example}
\end{figure}

\subsubsection{Semantic Segmentation Within an Image}
The semantic segmentation of an image has been vastly explored in literature, but depending on the target application for such segmentation, most of them are not relevant for a visual SLAM system. The advances in semantic segmentation based on object recognition, whether sparse \cite{girshick2014rich} or dense \cite{long2015fully}, are useful in visual SLAM because it enables resolving of each individual entity within an image. Most of these methods use deep convolutional neural networks and will be a suitable choice for this thesis as well. The bottleneck that can be foreseen is the lack of generality with respect to the objects that are recognized. 

First, it does not cover each and every object in the world, which is obviously not an easy task, hence we will need a hybrid approach that also learns new object classes on-line. 

Second, object recognition focuses more on indoor environments where most of these objects are found. It is quite common to encounter images that don't have any ``object'', especially in outdoor scenarios. The way objects are defined usually for object recognition tasks do not always consider wall, ceiling or floor like entities as a separate object, rather it is considered as background and the focus is on the object categories defined for the problem. This shortcoming has been fulfilled separately by some authors by proposing semantic place categorization that emphasizes the concept of a \emph{place} as opposed to object-centric images.

Third, object recognition doesn't take into account the changing conditions of the environment. Even the semantic place categorization methods do not wholly represent a place with attributes that correspond to environmental conditions. In order to incorporate semantics \emph{within} an image into a visual SLAM system, it is important to have a semantic representation of an image that covers objects, place categories as well as the attributes corresponding to environmental conditions. 

\paragraph{Planned Approach:}
We plan to use an off-the-shelf semantic segmentation method to perform object- or pixel-level segmentation of an image. An densely annotated image with various semantic labels is shown in Figure:\ref{fig:semantic-within-example}. Such a semantic segmentation can be used for learning salient regions for place recognition for handling dynamic objects as well as variations in environmental conditions.

\paragraph{Expected Outcome}
We expect to obtain the semantic regions within the environment which are salient for condition-invariant place recognition and then mask these semantic regions beforehand while performing place recognition. This should ideally improve the recognition accuracy without requiring to explicitly reason about the important regions within an image. Secondly, dynamic objects in the environment are a challenge for not just a visual SLAM system and many other research problems that involve a mobile camera or robot. We expect to be able to identify such dynamic objects using this approach and ignore the corresponding pixel region. The performance improvement will be benchmarked using the datasets which exhibit both a large number of dynamic objects and extreme condition variations in the environment.

\begin{figure}[htbp]
 \centering
 \includegraphics[clip, trim=0cm 6cm 8cm 0cm]{semantic-segmentation-annotation}
 \caption{Semantically annotated image \cite{mottaghi2014role} with different objects and structures within the scene. Semantic segmentation within an image can help in learning salient regions for place recognition for handling dynamic objects as well as variations in environmental conditions}
 \label{fig:semantic-within-example}
\end{figure}

\subsection{6-DoF Semantics-Aware Visual SLAM System}

\subsubsection{Appearance-Robust 3-D Relative Pose Estimation}
We have explored visual place recognition systems that are condition-invariant \cite{Milford2012,Niko2015,chen2017deep}, but they lack the ability to generate a 3-D relative pose between the matching places, also termed as \emph{loop closures}. In order to obtain a 6-DoF visual SLAM system, it is important to have the 3-D pose estimation for condition-invariant representation of places.

\paragraph{Planned Approach:}
We plan to first explore the possibility of using a condition- and viewpoint-invariant robust place representation and then enable it in some way to estimate relative pose. The deep-learned features are the most likely option for this case. The second option is to use a place representation which is known to efficiently work in the pose estimation framework of visual SLAM and then make it robust towards condition variations. The best choice for this seems to be the whole image feature descriptors which can be used in direct image matching framework for relative pose estimation with a descriptor-constancy assumption instead of brightness-constancy. One such example is a Bit-Plane Descriptor \cite{alismail2016bit}, which is 8-Channel version of Local Binary Pattern (LBP \cite{ojala1996comparative}) descriptors, but with the benefit of being used in a mult-channel Lukas Kanade framework \cite{alismail2016bit}. Figure:\ref{fig:bit-plane-descriptor} shows an example of this descriptor.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{bit-planes-descriptor}
 \caption{8-Channel Bit-Plane Descriptor can be used within a multi-channel LK framework using SSD cost measure \cite{alismail2016bit}}
 \label{fig:bit-plane-descriptor}
\end{figure}

\paragraph{Expected Outcome:}
The 3-D relative pose estimation for places matched under extreme appearance changes will be useful for developing a 6-DoF Condition- and Viewpoint-Invariant visual SLAM system which is yet missing a seat in SLAM community. The performance evaluation for this system will be done with respect to state-of-the-art visual SLAM systems within an unconstrained environment.

% \subsubsection{Semantics From Recognized Places}
% We have so far explored in lots of detail the use of semantics for visual SLAM. In order to have a semantics-aware visual SLAM system, we also need to close the loop by creating the flow of information from SLAM towards semantics. For example, visual place recognition applied on conditionally varied places can help in applying same structural semantic labels to those places, but different corresponding label for environmental conditions.
% 
% \paragraph{Planned Approach:}
% We plan to use a state-of-the-art condition-invariant place recognition system which can match places despite vast changes in appearance of the scene. Then, the images from the non-ideal environmental conditions, say the night-time imagery, can be improved for its semantic labeling by transferring the labels from its day-time counterpart. This can be done using a form of a prior probability for object recognition framework to boost its confidence for that particular object label.
% 
% \paragraph{Expected Outcome:}
% We intend to establish the utility of a condition-invariant place recognition system for semantic labeling within a visual SLAM framework. The performance in terms of accuracy of object recognition using the priors generated from a visual SLAM system will be benchmarked using existing or new datasets.

\subsubsection{An Integrated System}
The use of semantics in visual SLAM framework has mainly focused on individual components of SLAM, that is, either localization or mapping. As also discussed in previous section, a holistic approach for semantic segmentation \emph{within} places is required which will help in semantics based localization within the map. However, at the same time, we also want a semantic map that will cover all the semantics whether \emph{within} or \emph{across} the places. Hence, it is important to understand that role of semantics is with respect to both \emph{space} and \emph{time}, that actually echoes our concept of semantics \emph{within} and \emph{across} the places respectively as discussed earlier as well. It is beyond the scope of this thesis to develeop a semantic SLAM system where both semantics and SLAM would benefit from each other. Hence, we stick to a semantics-aware visual SLAM such that a semantic understanding of environment exists with respect to place recognition which can be fed into the representation map, but this, in turn, does not necessarily improve semantic labelling.

We plan to use the modified and newly developed components as discussed in the research plan so far to develop an integrated 6-DoF semantics-aware condition- and viewpoint-invariant visual SLAM system along with a semantically-interactive 3-D map which can be used for higher-level understanding based tasks, for example, navigation goals based on semantic place categories.

\subsection{Timeline}
The proposed timeline for the thesis is attached below:

\begin{figure}[!htbp]
\centering
% \hspace{-1cm}
 \makebox[\textwidth][c]{\includegraphics[scale=0.625]{timeline}}
 \label{fig:Timeline}
\end{figure}


\section{Work Progress}

\subsection{Performance Evaluation Using High Fidelity Simulation}
The use of high fidelity simulation is an attractive option for most of the researchers primarily because it gives access to infinite data generation which is very close to real world. A simulated environment enables detailed performance evaluation of algorithms which helps in understanding the data-dependent characteristics of the system and optimizing its parameters. We evaluated various robotic vision algorithms like place recognition, visual odometry, visual SLAM and object recognition to study their performance variations with respect to variations in the input data. The high fidelity simulation of a city-like environment was initially developed and the required datasets were then generated which were finally fed to different algorithms for performance evaluation.. Figure:\ref{fig:simulatedDataImageGrid} shows a grid of sample images from a set of datasets used for evaluating place recognition performance. The images are taken from same place but at different time of day and varying camera viewpoints. The work was done with collaborative efforts from peers and the research components that are relevant to this report are described in subsequent subsections.

\begin{figure}
 \centering
 \includegraphics[scale=0.7]{dataset-image-variety-grid}
 \caption{Sample images from the datasets used, all images are taken from the same place at different times of day and viewpoint variations. From left to right, viewpoints are the base unmodified path, offset left 2m, offset right 2m, angled up $30^{\circ}$, and angled right $30^{\circ}$.}
 \label{fig:simulatedDataImageGrid}
\end{figure}


\subsubsection{Place Recognition - SeqSLAM}
The visual place recognition is the capability of a mobile robot to recognize a place during a revisit solely using visual cues. Thus, it requires repeated traverses of the environment with no restriction on viewpoint or environmental conditions like time of day, weather or season etc. during the subsequent visit. We evaluated performance of SeqSLAM - a condition-invariant place recognition algorithm using different datasets generated from the simulated environment as shown in Figure:\ref{fig:simulatedDataImageGrid}. These datasets vary with respect to 5 different times of day, that is, Dawn, Morning, Noon, Afternoon and Sunset as well as different camera viewpoints with variations in lateral offset, vertical orientation (pitch) and horizontal orientation (yaw). 

The performance evaluation curves for SeqSLAM as shown in Figure:\ref{fig:slamCharacterize}(a) show a decrease in performance with extreme viewpoint variations as per the expectations and a constant high performance with varying conditions of environment. 

We also performed some experiments on real world data along with its simulated version to observe the performance trends which were found to be similar. However, the results on simulated data were consistently better than the real world data which is also often the case with any simulation. Figure:\ref{fig:realVsSim} shows the real and simulated version of a street dataset and Figure:\ref{fig:real-world-results} shows the corresponding performance comparison.

\begin{figure}
\centering
%   \hspace{-1cm}
  \includegraphics[width=17cm,height=2cm]{real-data-lateralOffset}\\
  \includegraphics[width=17cm,height=2cm]{simulated-data-lateralOffset}\\
  
  \caption{Images for five different traversals of a street with different lateral offsets. This real world data (top) is used to compare the trend of performance change with change in lateral shifts as compared to the simulated data (bottom).}
  \label{fig:realVsSim}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[height=5cm]{real-world-results}
    \caption{Comparison of SeqSLAM performance falloff between simulated and real-world environments as lateral offset increases. As is often the case, simulated performance is better in an absolute sense, but the trend is the same in both cases.}
    \label{fig:real-world-results}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{cc}
% 	\hspace{-1cm}
	\includegraphics[scale=0.4]{SeqSLAM-results-offset}&
	\includegraphics[scale=0.4]{orbslam-offset-results}\\
	
	\includegraphics[scale=0.4]{SeqSLAM-results-pitch}&	
	\includegraphics[scale=0.4]{orbslam-pitch-results}\\
	
	\includegraphics[scale=0.4]{SeqSLAM-results-yaw}&
	\includegraphics[scale=0.4]{orbslam-yaw-results}\\
	
	(a) SeqSLAM & (b) ORB-SLAM \\
\end{tabular}
	\caption{ (a) SeqSLAM and (b) ORB-SLAM's performance for datasets varying with respect to lateral camera offset, vertical orientation and lateral orientation from top to bottom as well time of day. SeqSLAM shows expected behaviour mostly apart from few abrupt troughs. ORB-SLAM doesn't show any consistent performance change with respect to the imposed variations.}
	\label{fig:slamCharacterize}
\end{figure}

\subsubsection{Visual SLAM - ORB-SLAM}
ORB-SLAM is a state-of-the-art visual SLAM system proven to work well for both monocular and stereo or depth-based input. Similar to performance evaluation of visual place recognition, evaluating a visual SLAM system also requires repetitive traversing of an environment, but in a continuous loop. This is necessary to make sure that all the components of a SLAM system that is visual odometry, place recognition and mapping are effectively tested. Therefore, different traverses of the environment at different times of day and varying viewpoints as also described in previous subsection were always appended by a noon baseline dataset. Such an arrangement made sure that visual odometry component gets tested in the first part of the data along with its capability of continuity across the appended data in terms of local changes. The variation of first part across different datasets and second part within the dataset made sure that the place recognition component is effectively tested.

We used average trajectory error as a performance measure for generating the performance curves. It was observed that the performance of ORB-SLAM did not show any consistent pattern with variations to either viewpoint or environmental conditions as shown in Figure:\ref{fig:slamCharacterize}(b). In general, in a visual SLAM system, occurrence of even a single false loop closure or an instance of failure in visual odometry is catastrophic for the system. The former leads to an absurd trajectory with high error and the latter imlies an incomplete trajectory. Moreover, most of the visual SLAM algorithms use initialization methods that use random numbers, therefore, expecting a consistent pattern in a performance curve is probably not viable.

\subsubsection{Paper Accepted at IROS 2016}
The research work was accepted as a conference paper at IROS 2016 \cite{skinner2016high}.

\subsection{Place Recognition Using Semantics}
The use of semantics with respect to places has become popular only recently with deep-learned CNN models after its successful experiments with object recognition. The semantics for places are generally described with respect to a single image, and provide labels about the scene structure, texture, environmental conditions etc as shown in Figure:\ref{fig:semLabels}. Though, it enables place recognition in a course manner, recognizing \emph{specific} places in an environment which generally have similar semantic labels in their neighborhood, is the \emph{traditional} place recognition problem usually dealt in robotics. We explored the use of semantic labels for individual places within an image dataset in improving place recognition performance as described in subsections below.

\begin{figure}
 \centering
 \begin{tabular}{ccc}
  \includegraphics[scale=0.25]{1-corridor}&
  \includegraphics[scale=0.25]{2-staircase}&
  \includegraphics[scale=0.25]{5-lawn}\\
  \includegraphics[scale=0.25]{6-subwayStation}&
  \includegraphics[scale=0.25]{7-railroadTrack}&
  \includegraphics[scale=0.25]{8-trainStation}\\
 \end{tabular}
\caption{Semantically labelled images from Campus Indoor-Outdoor and CTA-Rail Dataset.}
\label{fig:semLabels}
\end{figure}


\begin{figure}[htbp]
\centering
\begin{tabular}{cc}
	\hspace{-1cm}
	\includegraphics[clip, trim=2cm 4cm 2cm 4cm,width=8cm,height=4.5cm]{cta-dataset-segmentation-1}&
	
	\includegraphics[clip, trim=0cm 4cm 0cm 2cm,scale=0.3]{flowchart}\\
	(a) & (b) \\
\end{tabular}
	\caption{ (a) shows semantic labels corresponding to different segments of the environment in CTA-Rail dataset. (b) shows a block diagram representing the flow of semantic information from the place categorization module to the place recognition module for improving place matching scores.}
	\label{fig:flowchart}
\end{figure}

\subsubsection{Semantic Segmentation of Environment}
One of the apparent ways of using semantic labels in place recognition is to coarsely filter the places that may match the query place. This will definitely reduce the computation time for place search within the database, and may also improve performance depending on the robustness of features used to semantically categorize places. Another way of utilizing the semantic information in context of place recognition is to exploit the temporal nature of place data encountered in both reference and query databases. A mobile robot traversing an environment is most likely to witness variations in its environment, both minor and major, especially in the applications of a visual SLAM system. 

We developed a system that performed semantic place categorization on an incoming stream of images to form temporal semantic segments of places which are appear similar. Figure:\ref{fig:flowchart}(a) shows a timeline of labels for different segments within one of the experimental datasets. The segmentation was then followed by a more \emph{specific} (or \emph{traditional}) place recognition system that utilized these temporal segments to define the neighborhood region around reference query places to bias their matching scores accordingly. Figure:\ref{fig:flowchart}(b) shows a block diagram reprsenting the flow of semantic information from place categorization to place recognition. 

The effectiveness of such a system is more pronounced for the image datasets having significant variations in the environment, for example, transiting between indoor and outdoor environments like a train running though tunnels as well as open areas (Figure-\ref{fig:flowchart}(a)) or a mobile robot within a university campus (Figure:\ref{fig:semLabels}); transiting between regions of varying illumination or texture like an urban canyon versus highway within a forest etc. We tested our system on a wide variety of datasets ranging from a 23 km train journey to short campus traverses, all exhibiting variations in environment with respect to scene structure, illumination, environmental conditions, texture etc. Figure:\ref{fig:rioTransImages} shows transition from outdoor to indoor of a house with varying illumnation. We observed a considerable performance gain using the proposed system for datasets with medium to extreme variations in their environment. Figure:\ref{fig:performanceChart} shows comparative results between vanilla method and proposed approach for two of the datasets we experimented with.

\newcommand{\imgW}{2.4cm}
\newcommand{\imgH}{1.4cm}

\begin{figure}[htbp]
\centering
\begin{tabular}{cccccc}

 \includegraphics[width=\imgW,height=\imgH]{rio-q1-2} &
 \includegraphics[width=\imgW,height=\imgH]{rio-q2-2} &
 \includegraphics[width=\imgW,height=\imgH]{rio-q4-2} &
 \includegraphics[width=\imgW,height=\imgH]{rio-q5} &
 \includegraphics[width=\imgW,height=\imgH]{rio-q6} &
 \includegraphics[width=\imgW,height=\imgH]{rio-q8} \\
 
 \includegraphics[width=\imgW,height=\imgH]{rio-rf1-2} &
 \includegraphics[width=\imgW,height=\imgH]{rio-rf2-2} &
 \includegraphics[width=\imgW,height=\imgH]{rio-rf4-2} &
 \includegraphics[width=\imgW,height=\imgH]{rio-rf5-2} &
 \includegraphics[width=\imgW,height=\imgH]{rio-rf6} &
 \includegraphics[width=\imgW,height=\imgH]{rio-rf8} \\
 
 \includegraphics[width=\imgW,height=\imgH]{rio-r1} &
 \includegraphics[width=\imgW,height=\imgH]{rio-r2} &
 \includegraphics[width=\imgW,height=\imgH]{rio-r4} &
 \includegraphics[width=\imgW,height=\imgH]{rio-r5-2} &
 \includegraphics[width=\imgW,height=\imgH]{rio-r6-2} &
 \includegraphics[width=\imgW,height=\imgH]{rio-r8-2} \\
 
\end{tabular}
\caption{\textbf{Top Row:} Ground truth images from the Residence Indoor-Outdoor dataset in a temporal order showing transition from nightly outdoor environment into bright indoor areas. \textbf{Middle Row:} Matched places using Vanilla SeqSLAM mostly showing false matching of images having similar lighting conditions. \textbf{Bottom Row:} Matched places using proposed method showing correct place recognition. It also shows the transition in environment from broad daylight to dark indoor areas. \emph{Note:} The images captured at night or in dark are shown here after manually brightening them only for sake of visualization.}
\label{fig:rioTransImages}
\end{figure}

\subsubsection{Paper Accepted at IROS 2017}
The proposed research work has been accepted as a conference paper in IROS 2017.

\begin{figure}[htbp]
\centering
 \begin{tabular}{cc}
 \hspace{-1cm}
\includegraphics[scale=0.30]{RPerformanceBar_parking_amrapali} &
\includegraphics[scale=0.30]{RPerformanceBar_cta-rail} \\
(a) Parking Lot & (b) CTA-Rail \\
 \end{tabular}
 \caption{Performance charts showing maximum F1 Score with respect to different R (Neighborhood Normalization Zone Width) values. R is varied to the maximum value, that is, the size of reference database, after which maximum F1 score becomes constant. The patch normalization window size ($P$) parameter with value 4 happens to perform better as compared to others most of the times.}
 \label{fig:performanceChart}
\end{figure}

\subsection{Motion Estimation under Unfavorable Conditions}
Visual Odometry being one of the important competencies for mobile robotics is also an integral part of a visual SLAM system. The correct estimation of ego-motion depends on the characteristics of both the camera motion and the environment. The state-of-the-art visual odometry methods, though able to generate a 6-DoF pose, are sensitive to extreme changes in the camera motion or the operating environment. We explored the applicability of these methods on some of the real world datasets that exhibit variations in camera speed, for example, a vehicle on a heavy traffic route with frequent halts, and variations in illumination, for example, transiting from an artificially-lit to unlit environment at night etc.

\begin{figure}
\centering
\begin{tabular}{cc}
	\includegraphics[scale=0.33]{fsv3-night-2-hybridVO}&
	\includegraphics[scale=0.33]{fsv3-day-hybridVO}\\
	(a) Night Traverse & (b) Day Traverse \\	
\end{tabular}
	\caption{The conventional state-of-the-art visual odometry suffers from failures due to high camera speed, sharp turns and low-light or featureless environment. It is shown here how the proposed motion estimator can be used to correctly estimate the displacement under such circumstances.}
	\label{fig:traj_vo}
\end{figure}

\subsubsection{Low Light and High Speed}
The low light environment means less visual features as compared to the scene captured in broad daylight. A gradual drop of visual features, for example, while moving from an artificially-lit, cluttered scene to an unlit bland environment at night, leads to failure of visual odometry. We performed experiments on different datasets exhibiting such variations in environment using visual odometry component of ORB-SLAM and LSD-SLAM. The test datasets also had varying camera speeds in different parts of environment which further led to failures because capturing large and small motion signals requires different parameters settings for the system.

We developed a method to generate a motion signal for unfavorable conditions as described above, using sum of absolute difference (SAD) score between down-sampled and patch-normalized consecutive images. This is equivalent to \emph{normalized photometric error} and closely relates to place recognition method used in SeqSLAM. The patch-normalization of images is able to handle the variations due to environmental conditions. It was also observed that SAD scores between images drops gradually with increasing frame separation between the images in an image sequence. Hence, we used a polynomial fitting method to estimate the camera speed according to the SAD score patterns. We were able to estimate the high-speed instances of camera motion within the test dataset where stat-of-the-art methods failed. This is shown in Figure:\ref{fig:traj_vo} with ground truth trajectory. The point of failures are mainly related to transition from well-lit to unlit areas and sudden increase in camera speed. We plan to use a hybrid approach that can make use of the proposed motion estimator to either tune the state-of-the-art method parameters to correctly estimate a 6-DoF pose or to club the output of both the systems to prevent failure and generate a consistent odometry information.

\newcommand{\scaleOne}{0.175}
\begin{figure}
\centering
\begin{tabular}{cccc}
\hspace{-1cm}
	\includegraphics[scale=\scaleOne,trim={0.5cm 3cm 0.5cm 5cm},clip]{fsv3-1-gtOverDiffMat-fixed} &
	\includegraphics[scale=\scaleOne,trim={0.5cm 3cm 0.5cm 5cm},clip]{fsv3-1-gtOverDiffMat-proposed} &
	\includegraphics[scale=\scaleOne]{sp-gtOverDiffMat-fixed} &
	\includegraphics[scale=\scaleOne]{sp-gtOverDiffMat-proposed} \\
	Vanilla & Proposed & Vanilla & Proposed \\
	\multicolumn{2}{c}{KG-Footpath} &			
	\multicolumn{2}{c}{Surfers Paradise} \\			
\end{tabular}
	\caption{(a) Ground Truth Matches plotted over the SAD score difference matrix between query and reference images for both vanilla and proposed method.}
	\label{fig:gtDiffMat}
\end{figure}


\subsubsection{Speed-Normalized Data Sampling}
We explored the scope of improving place recognition performance using state-of-the-art visual odometry methods. The motion information is often required in the place recognition framework in order to sample the images at a constant distance as opposed to constant frame separation. A robot moving uniformly will not be affected by such a choice, but in practice, the place recognition and visual SLAM applications contain reference and query imagery captured at varying camera speeds. A perfect example of such a scenario is an on-road vehicle moving at variable pace, due to varying traffic conditions on a particular route at different times of day. Figure:\ref{fig:gtDiffMat} shows similarity matrices between reference and query image databases for two different datasets with different methods of frame sampling.

The speed-normalized imagery can either be obtained using a separate sensor or a visual odometry solution. The challenges involved in using a state-of-the-art visual odometry solution are described in previous subsection. Hence, we resort to the use of proposed motion estimator, also described above, to speed-normalize the image databases for both query and reference. The motion estimator determines the ideal frame separation between the images based on the SAD score between them. This dynamic frame separation corresponds to the physical camera motion and is estimated high when camera moves slowly and low when camera moves fast. We observed a significant improvement in SeqSLAM's place recognition performance using the proposed speed-normalized data sampling before starting to match the places. Figure:\ref{fig:perfSpeedNorm} shows performance comparison between vanilla method and proposed approach with speed-normalized frame sampling.

\subsubsection{Planned Submission for ICRA 2018}
We plan to submit the proposed research work to ICRA 2018 with some additional research work on top of it.

\begin{figure}
\centering
\begin{tabular}{cc}
\hspace{-1cm}
	\includegraphics[scale=0.32]{fsv3-1-perf-speedNorm-3d}&
	\includegraphics[scale=0.32]{sp-perf-speedNorm-3d}\\
	(a) Kelvin Grove Footpath & (b) Surfers Paradise \\	
\end{tabular}
	\caption{Performance curves for vanilla and proposed approach for Kelvin Grove Footpath and Surfers Paradise datasets. Though the trends across the two important parameters - \emph{Sequence Length} and \emph{Search Range}, remain same as shown in contours; the overall performance gain is huge using proposed approach.}
	\label{fig:perfSpeedNorm}
\end{figure}

\section{Conclusion}
We described the visual SLAM system and its individual components, that is, visual place recognition, visual odometry and representation map. We also explored the possibilities of use of semantics in SLAM framework. We reviewed the relevant literature for all the related components and identified the challenges and viable research gaps. Then we formulated our research problem stating the need of a \emph{6-Dof Semantics-Aware Condition- and Viewpoint-Invariant Visual SLAM} system along with a set of research questions. The possible solutions to these research questions, addressing all the challenges are then detailed within a research plan for the thesis. Finally, the research work progress of last year in line with the research plan is described. The challenges that still remain to solve are summarized as following:
\begin{itemize}
 \item Developing a robust visual odometry solution for rapid camera motion and adverse environmental conditions.
 \item Estimating 3D relative pose for loop closures influenced by vast appearance changes.
 \item Exploring the use of semantics with an integrated approach towards a semantics-aware visual SLAM system.
\end{itemize}
With reference to the attached timeline, we plan to address all the challenges and fill these research gaps by the end of PhD with research publications.


\clearpage

\bibliographystyle{unsrt}
\bibliography{../mendeley}

\end{document}
