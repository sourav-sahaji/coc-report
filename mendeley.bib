@article{ojala1996comparative,
author = {Ojala, Timo and Pietik{\"{a}}inen, Matti and Harwood, David},
journal = {Pattern recognition},
number = {1},
pages = {51--59},
publisher = {Elsevier},
title = {{A comparative study of texture measures with classification based on featured distributions}},
volume = {29},
year = {1996}
}
@article{alismail2016bit,
author = {Alismail, Hatem and Browning, Brett and Lucey, Simon},
journal = {arXiv preprint arXiv:1602.00307},
title = {{Bit-planes: Dense subpixel alignment of binary descriptors}},
year = {2016}
}
@inproceedings{mottaghi2014role,
author = {Mottaghi, Roozbeh and Chen, Xianjie and Liu, Xiaobai and Cho, Nam-Gyu and Lee, Seong-Whan and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {891--898},
title = {{The role of context for object detection and semantic segmentation in the wild}},
year = {2014}
}
@inproceedings{deng2009imagenet,
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
organization = {IEEE},
pages = {248--255},
title = {{Imagenet: A large-scale hierarchical image database}},
year = {2009}
}
@inproceedings{jacobs2007consistent,
author = {Jacobs, Nathan and Roman, Nathaniel and Pless, Robert},
booktitle = {Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on},
organization = {IEEE},
pages = {1--6},
title = {{Consistent temporal variations in many outdoor scenes}},
year = {2007}
}
@inproceedings{krizhevsky2012imagenet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in neural information processing systems},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@article{lopez2017appearance,
author = {Lopez-Antequera, Manuel and Gomez-Ojeda, Ruben and Petkov, Nicolai and Gonzalez-Jimenez, Javier},
journal = {Pattern Recognition Letters},
publisher = {Elsevier},
title = {{Appearance-Invariant Place Recognition by Discriminatively Training a Convolutional Neural Network}},
year = {2017}
}
@inproceedings{cummins2007probabilistic,
author = {Cummins, Mark and Newman, Paul},
booktitle = {Robotics and automation, 2007 IEEE international conference on},
organization = {IEEE},
pages = {2042--2048},
title = {{Probabilistic appearance based navigation and loop closing}},
year = {2007}
}
@inproceedings{konda2015learning,
author = {Konda, Kishore Reddy and Memisevic, Roland},
booktitle = {VISAPP (1)},
pages = {486--490},
title = {{Learning Visual Odometry with a Convolutional Network.}},
year = {2015}
}
@inproceedings{kendall2015posenet,
author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
booktitle = {Proceedings of the IEEE international conference on computer vision},
pages = {2938--2946},
title = {{Posenet: A convolutional network for real-time 6-dof camera relocalization}},
year = {2015}
}
@article{dequaire2016deep,
author = {Dequaire, Julie and Rao, Dushyant and Ondruska, Peter and Wang, Dominic and Posner, Ingmar},
journal = {arXiv preprint arXiv:1609.09365},
title = {{Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks}},
year = {2016}
}
@article{vijayanarasimhan2017sfm,
author = {Vijayanarasimhan, Sudheendra and Ricco, Susanna and Schmid, Cordelia and Sukthankar, Rahul and Fragkiadaki, Katerina},
journal = {arXiv preprint arXiv:1704.07804},
title = {{SfM-Net: Learning of Structure and Motion from Video}},
year = {2017}
}
@article{chen2014convolutional,
author = {Chen, Zetao and Lam, Obadiah and Jacobson, Adam and Milford, Michael},
journal = {arXiv preprint arXiv:1411.1509},
title = {{Convolutional neural network-based place recognition}},
year = {2014}
}
@inproceedings{salas2013slam++,
author = {Salas-Moreno, Renato F and Newcombe, Richard A and Strasdat, Hauke and Kelly, Paul H J and Davison, Andrew J},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {1352--1359},
title = {{Slam++: Simultaneous localisation and mapping at the level of objects}},
year = {2013}
}
@article{vasudevan2007cognitive,
author = {Vasudevan, Shrihari and G{\"{a}}chter, Stefan and Nguyen, Viet and Siegwart, Roland},
journal = {Robotics and Autonomous Systems},
number = {5},
pages = {359--371},
publisher = {Elsevier},
title = {{Cognitive maps for mobile robots—an object based approach}},
volume = {55},
year = {2007}
}
@article{galvez2016real,
author = {G{\'{a}}lvez-L{\'{o}}pez, Dorian and Salas, Marta and Tard{\'{o}}s, Juan D and Montiel, J M M},
journal = {Robotics and Autonomous Systems},
pages = {435--449},
publisher = {Elsevier},
title = {{Real-time monocular object slam}},
volume = {75},
year = {2016}
}
@phdthesis{salas2014dense,
author = {Salas-Moreno, Renato F},
school = {Citeseer},
title = {{Dense semantic SLAM}},
year = {2014}
}
@inproceedings{fioraio2013joint,
author = {Fioraio, Nicola and {Di Stefano}, Luigi},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {1538--1545},
title = {{Joint detection, tracking and mapping by semantic bundle adjustment}},
year = {2013}
}
@article{nuchter2008towards,
author = {N{\"{u}}chter, Andreas and Hertzberg, Joachim},
journal = {Robotics and Autonomous Systems},
number = {11},
pages = {915--926},
publisher = {Elsevier},
title = {{Towards semantic maps for mobile robots}},
volume = {56},
year = {2008}
}
@inproceedings{stuckler2012semantic,
author = {St{\"{u}}ckler, J{\"{o}}rg and Biresev, Nenad and Behnke, Sven},
booktitle = {Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
organization = {IEEE},
pages = {3005--3010},
title = {{Semantic mapping using object-class segmentation of RGB-D images}},
year = {2012}
}
@inproceedings{civera2011towards,
author = {Civera, Javier and G{\'{a}}lvez-L{\'{o}}pez, Dorian and Riazuelo, Luis and Tard{\'{o}}s, Juan D and Montiel, J M M},
booktitle = {Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on},
keywords = {SURF,mono,object,semantic,sfm},
mendeley-tags = {SURF,mono,object,semantic,sfm},
organization = {IEEE},
pages = {1277--1284},
title = {{Towards semantic SLAM using a monocular camera}},
year = {2011}
}
@article{chen2014semantic,
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
journal = {arXiv preprint arXiv:1412.7062},
title = {{Semantic image segmentation with deep convolutional nets and fully connected crfs}},
year = {2014}
}
@inproceedings{long2015fully,
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
title = {{Fully convolutional networks for semantic segmentation}},
year = {2015}
}
@inproceedings{girshick2014rich,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@inproceedings{ranganathan2007semantic,
author = {Ranganathan, Ananth and Dellaert, Frank},
organization = {Georgia Institute of Technology},
title = {{Semantic modeling of places using objects}},
year = {2007}
}
@phdthesis{pronobis2011semantic,
author = {Pronobis, Andrzej},
school = {KTH Royal Institute of Technology},
title = {{Semantic mapping with mobile robots}},
year = {2011}
}
@inproceedings{furukawa2009manhattan,
author = {Furukawa, Yasutaka and Curless, Brian and Seitz, Steven M and Szeliski, Richard},
booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
organization = {IEEE},
pages = {1422--1429},
title = {{Manhattan-world stereo}},
year = {2009}
}
@article{posner2008online,
author = {Posner, Ingmar and Schroeter, Derik and Newman, Paul},
journal = {Robotics and Autonomous Systems},
keywords = {laser,outdoor,semantic,svm per class},
mendeley-tags = {laser,outdoor,semantic,svm per class},
number = {11},
pages = {901--914},
publisher = {Elsevier},
title = {{Online generation of scene descriptions in urban environments}},
volume = {56},
year = {2008}
}
@inproceedings{xiao2009multiple,
author = {Xiao, Jianxiong and Quan, Long},
booktitle = {Computer Vision, 2009 IEEE 12th International Conference on},
keywords = {MRF,sfm,superpixel},
mendeley-tags = {MRF,sfm,superpixel},
organization = {IEEE},
pages = {686--693},
title = {{Multiple view semantic segmentation for street view images}},
year = {2009}
}
@article{stachniss2005semantic,
author = {Stachniss, Cyrill and {Martinez Mozos}, Oscar and Rottmann, Axel and Burgard, Wolfram and Others},
keywords = {AdaBoost,HMMl,laser},
mendeley-tags = {AdaBoost,HMMl,laser},
title = {{Semantic labeling of places}},
year = {2005}
}
@article{alismail2016direct,
author = {Alismail, Hatem and Browning, Brett and Lucey, Simon},
journal = {arXiv preprint arXiv:1604.00990},
title = {{Direct visual odometry using bit-planes}},
year = {2016}
}
@inproceedings{pepperell2015automatic,
author = {Pepperell, Edward and Corke, Peter I and Milford, Michael J},
booktitle = {Robotics and Automation (ICRA), 2015 IEEE International Conference on},
organization = {IEEE},
pages = {1118--1124},
title = {{Automatic image scaling for place recognition in changing environments}},
year = {2015}
}
@inproceedings{flint2010growing,
author = {Flint, Alex and Mei, Christopher and Reid, Ian and Murray, David},
booktitle = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
organization = {IEEE},
pages = {467--474},
title = {{Growing semantically meaningful models for visual slam}},
year = {2010}
}
@article{alismail2016photometric,
author = {Alismail, Hatem and Browning, Brett and Lucey, Simon},
journal = {arXiv preprint arXiv:1608.02026},
title = {{Photometric Bundle Adjustment for Vision-Based SLAM}},
year = {2016}
}
@article{frese2010interview,
author = {Frese, Udo},
journal = {KI-K{\{}{\"{u}}{\}}nstliche Intelligenz},
number = {3},
pages = {255--257},
publisher = {Springer},
title = {{Interview: Is slam solved?}},
volume = {24},
year = {2010}
}
@inproceedings{skinner2016high,
author = {Skinner, John and Garg, Sourav and S{\"{u}}nderhauf, Niko and Corke, Peter and Upcroft, Ben and Milford, Michael},
booktitle = {Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on},
organization = {IEEE},
pages = {2737--2744},
title = {{High-fidelity simulation for evaluating robotic vision performance}},
year = {2016}
}
@article{laffont2014transient,
author = {Laffont, Pierre-Yves and Ren, Zhile and Tao, Xiaofeng and Qian, Chao and Hays, James},
journal = {ACM Transactions on Graphics (TOG)},
number = {4},
pages = {149},
publisher = {ACM},
title = {{Transient attributes for high-level understanding and editing of outdoor scenes}},
volume = {33},
year = {2014}
}
@article{liu2017robotic,
author = {Liu, Huaping and Sun, Fuchun and Fang, Bin and Zhang, Xinyu},
journal = {IEEE Transactions on Instrumentation and Measurement},
number = {1},
pages = {2--13},
publisher = {IEEE},
title = {{Robotic room-level localization using multiple sets of sonar measurements}},
volume = {66},
year = {2017}
}
@article{pepperell2016routed,
author = {Pepperell, Edward and Corke, Peter and Milford, Michael},
journal = {The International Journal of Robotics Research},
number = {9},
pages = {1057--1179},
publisher = {SAGE Publications Sage UK: London, England},
title = {{Routed roads: Probabilistic vision-based place recognition for changing conditions, split streets and varied viewpoints}},
volume = {35},
year = {2016}
}
@inproceedings{cascianelli2016robust,
author = {Cascianelli, Silvia and Costante, Gabriele and Bellocchio, Enrico and Valigi, Paolo and Fravolini, Mario L and Ciarfuglia, Thomas A},
booktitle = {Smart Cities Conference (ISC2), 2016 IEEE International},
organization = {IEEE},
pages = {1--6},
title = {{A robust semi-semantic approach for visual localization in urban environment}},
year = {2016}
}
@article{maddern20161,
author = {Maddern, Will and Pascoe, Geoffrey and Linegar, Chris and Newman, Paul},
journal = {The International Journal of Robotics Research},
pages = {0278364916679498},
publisher = {SAGE Publications},
title = {{1 year, 1000 km: The Oxford RobotCar dataset}},
year = {2016}
}
@article{lowry2016supervised,
author = {Lowry, Stephanie and Milford, Michael J},
journal = {IEEE Transactions on Robotics},
number = {3},
pages = {600--613},
publisher = {IEEE},
title = {{Supervised and unsupervised linear learning techniques for visual place recognition in changing environments}},
volume = {32},
year = {2016}
}
@article{chen2017deep,
author = {Chen, Zetao and Jacobson, Adam and Sunderhauf, Niko and Upcroft, Ben and Liu, Lingqiao and Shen, Chunhua and Reid, Ian and Milford, Michael},
journal = {arXiv preprint arXiv:1701.05105},
title = {{Deep Learning Features at Scale for Visual Place Recognition}},
year = {2017}
}
@inproceedings{linegar2016made,
author = {Linegar, Chris and Churchill, Winston and Newman, Paul},
booktitle = {Robotics and Automation (ICRA), 2016 IEEE International Conference on},
organization = {IEEE},
pages = {787--794},
title = {{Made to measure: Bespoke landmarks for 24-hour, all-weather localisation with a camera}},
year = {2016}
}
@article{McManus2015,
abstract = {This paper presents an alternative approach to the problem of outdoor, persistent visual localisation against a known map. Instead of blindly applying a feature detector/descriptor combination over all images of all places, we leverage prior experiences of a place to learn place-dependent feature detectors (i.e., features that are unique to each place in our map and used for localisation). Furthermore, as these features do not represent low-level structure, like edges or corners, but are in fact mid-level patches representing distinctive visual elements (e.g., windows, buildings, or silhouettes), we are able to localise across extreme appearance changes. Note that there is no requirement that the features posses semantic meaning, only that they are optimal for the task of localisation. This work is an extension on previous work (McManus et al. in Proceedings of robotics science and systems, 2014b) in the following ways: (i) we have included a landmark refinement and outlier rejection step during the learning phase, (ii) we have implemented an asynchronous pipeline design, (iii) we have tested on data collected in an urban environment, and (iv) we have implemented a purely monocular system. Using over 100 km worth of data for training, we present localisation results from Begbroke Science Park and central Oxford.},
author = {McManus, Colin and Upcroft, Ben and Newman, Paul},
doi = {10.1007/s10514-015-9463-y},
issn = {1573-7527},
journal = {Autonomous Robots},
number = {3},
pages = {363--387},
title = {{Learning place-dependant features for long-term vision-based localisation}},
url = {http://dx.doi.org/10.1007/s10514-015-9463-y},
volume = {39},
year = {2015}
}
@inproceedings{nelson2015dusk,
author = {Nelson, Peter and Churchill, Winston and Posner, Ingmar and Newman, Paul},
booktitle = {Robotics and Automation (ICRA), 2015 IEEE International Conference on},
organization = {IEEE},
pages = {5245--5252},
title = {{From dusk till dawn: Localisation at night using artificial light sources}},
year = {2015}
}
@inproceedings{linegar2015work,
author = {Linegar, Chris and Churchill, Winston and Newman, Paul},
booktitle = {Robotics and Automation (ICRA), 2015 IEEE International Conference on},
organization = {IEEE},
pages = {90--97},
title = {{Work smart, not hard: Recalling relevant experiences for vast-scale but time-constrained localisation}},
year = {2015}
}
@article{celeux2008selecting,
author = {Celeux, Gilles and Durand, Jean-Baptiste},
journal = {Computational Statistics},
number = {4},
pages = {541--564},
publisher = {Springer},
title = {{Selecting hidden Markov model state number with cross-validated likelihood}},
volume = {23},
year = {2008}
}
@article{beal2002infinite,
author = {Beal, Matthew J and Ghahramani, Zoubin and Rasmussen, Carl Edward},
title = {{The infinite hidden Markov model}}
}
@phdthesis{honkela2001nonlinear,
author = {Honkela, Antti},
school = {UNIVERSITY OF TECHNOLOGY},
title = {{Nonlinear switching state-space models}},
year = {2001}
}
@article{baum1966statistical,
author = {Baum, Leonard E and Petrie, Ted},
journal = {The annals of mathematical statistics},
number = {6},
pages = {1554--1563},
publisher = {JSTOR},
title = {{Statistical inference for probabilistic functions of finite state Markov chains}},
volume = {37},
year = {1966}
}
@article{kejriwal2016high,
author = {Kejriwal, Nishant and Kumar, Swagat and Shibata, Tomohiro},
journal = {Robotics and Autonomous Systems},
pages = {55--65},
publisher = {Elsevier},
title = {{High performance loop closure detection using bag of word pairs}},
volume = {77},
year = {2016}
}
@inproceedings{wang2015improved,
author = {Wang, Yujie and Hu, Xiaoping and Lian, Junxiang and Zhang, Lilian and Kong, Xianglong},
booktitle = {Intelligent Human-Machine Systems and Cybernetics (IHMSC), 2015 7th International Conference on},
organization = {IEEE},
pages = {260--264},
title = {{Improved Seq SLAM for Real-Time Place Recognition and Navigation Error Correction}},
volume = {1},
year = {2015}
}
@inproceedings{Cummins2009,
address = {Seattle, United States},
author = {Cummins, Mark and Newman, Paul},
booktitle = {Robotics: Science and Systems},
title = {{Highly scalable appearance-only SLAM - FAB-MAP 2.0}},
year = {2009}
}
@misc{ctaRail2014,
annote = {[Online; accessed 15-August-2016]},
author = {CTAConnections},
howpublished = {$\backslash$url{\{}https://youtu.be/n6xJFpPY{\_}7s{\}}},
title = {{CTA Ride the Rails: Blue Line to O'Hare in Real Time}},
year = {2014}
}
@misc{cnnPlaces365Github,
annote = {[Online; accessed 15-August-2016]},
author = {Zhou, Bolei},
howpublished = {$\backslash$url{\{}https://github.com/metalbubble/places365{\}}},
title = {{The Places365-CNNs}},
year = {2016}
}
@misc{ctaRail2015,
annote = {[Online; accessed 15-August-2016]},
author = {CTAConnections},
howpublished = {$\backslash$url{\{}https://youtu.be/Kw{\_}BbQoDv8o{\}}},
title = {{CTA Ride the Rails: Blue Line to O'Hare in Real Time (2015)}},
year = {2015}
}
@misc{ctaTrajGMap,
annote = {[Online; accessed 15-August-2016]},
author = {Maps, Google},
howpublished = {$\backslash$url{\{}https://goo.gl/maps/fX6aSDMnJpC2{\}}},
title = {{Google Maps: Directions from Forest Park, IL, USA to O'Hare International Airport, IL, USA}},
year = {2016}
}
@inproceedings{sunderhauf2016place,
author = {Sunderhauf, Niko and Dayoub, Feras and McMahon, Sean and Talbot, Ben and Schulz, Ruth and Corke, Peter and Wyeth, Gordon and Upcroft, Ben and Milford, Michael},
booktitle = {Proceedings of the International Conference on Robotics and Automation},
organization = {IEEE},
title = {{Place categorization and semantic mapping on a mobile robot}},
year = {2016}
}
@inproceedings{tapus2006cognitive,
author = {Tapus, Adriana and Siegwart, Roland},
booktitle = {Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.},
organization = {IEEE},
pages = {1188--1193},
title = {{A cognitive modeling of space using fingerprints of places for mobile robot navigation}},
year = {2006}
}
@inproceedings{Patterson2012SunAttributes,
author = {Patterson, Genevieve and Hays, James},
booktitle = {Proceeding of the 25th Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{SUN Attribute Database: Discovering, Annotating, and Recognizing Scene Attributes}},
year = {2012}
}
@inproceedings{wu2009visual,
author = {Wu, Jianxin and Christensen, Henrik I and Rehg, James M},
booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems},
organization = {IEEE},
pages = {4763--4770},
title = {{Visual place categorization: Problem, dataset, and algorithm}},
year = {2009}
}
@article{Cummins2008,
annote = {1377517},
author = {Cummins, Mark and Newman, Paul},
isbn = {0278-3649},
journal = {International Journal of Robotics Research},
number = {6},
pages = {647--665},
title = {{FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance}},
volume = {27},
year = {2008}
}
@inproceedings{Milford2013a,
address = {Karlsruhe, Germany},
author = {Milford, M and Jacobson, A},
booktitle = {International Conference on Robotics and Automation},
publisher = {IEEE},
title = {{Brain-inspired Sensor Fusion for Navigating Robots}},
year = {2013}
}
@inproceedings{corke2013dealing,
author = {Corke, Peter and Paul, Rohan and Churchill, Winston and Newman, Paul},
booktitle = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
organization = {IEEE},
pages = {2085--2092},
title = {{Dealing with shadows: Capturing intrinsic scene appearance for image-based outdoor localisation}},
year = {2013}
}
@misc{indoorOutdoor1,
annote = {[Online; accessed 15-August-2016]},
author = {RoboticsAtHsUlm},
howpublished = {$\backslash$url{\{}https://youtu.be/{\_}ConuKUOXH4{\}}},
title = {{Seamless Indoor and Outdoor Navigation based on OpenStreetMap}},
year = {2016}
}
@inproceedings{zhou2014learning,
author = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
booktitle = {Advances in neural information processing systems},
pages = {487--495},
title = {{Learning deep features for scene recognition using places database}},
year = {2014}
}
@inproceedings{torralba2003context,
author = {Torralba, Antonio and Murphy, Kevin P and Freeman, William T and Rubin, Mark A},
booktitle = {Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on},
organization = {IEEE},
pages = {273--280},
title = {{Context-based vision system for place and object recognition}},
year = {2003}
}
@inproceedings{ranganathan2011visual,
author = {Ranganathan, Ananth and Lim, Jongwoo},
booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
organization = {IEEE},
pages = {3982--3989},
title = {{Visual place categorization in maps}},
year = {2011}
}
@inproceedings{mohan2015environment,
author = {Mohan, Mahesh and G{\'{a}}lvez-L{\'{o}}pez, Dorian and Monteleoni, Claire and Sibley, Gabe},
booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {5487--5494},
title = {{Environment selection and hierarchical place recognition}},
year = {2015}
}
@inproceedings{ulrich2000appearance,
author = {Ulrich, Iwan and Nourbakhsh, Illah},
booktitle = {Robotics and Automation, 2000. Proceedings. ICRA'00. IEEE International Conference on},
organization = {Ieee},
pages = {1023--1029},
title = {{Appearance-based place recognition for topological localization}},
volume = {2},
year = {2000}
}
@inproceedings{mcmanus2014shady,
author = {McManus, Colin and Churchill, Winston and Maddern, Will and Stewart, Alexander D and Newman, Paul},
booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {901--906},
title = {{Shady dealings: Robust, long-term visual localisation using illumination invariance}},
year = {2014}
}
@misc{timelineRWT,
annote = {[Online; accessed 15-August-2016]},
author = {ReadWriteThink},
howpublished = {$\backslash$url{\{}http://www.readwritethink.org/files/resources/interactives/timeline{\_}2/{\}}},
title = {{ReadWriteThink - http://www.readwritethink.org/}},
year = {2016}
}
@inproceedings{thompson1993vision,
author = {Thompson, William B and {Pick Jr}, H},
booktitle = {Proc. ARPA Image Understanding Workshop},
organization = {Citeseer},
title = {{Vision-based navigation}},
year = {1993}
}
@misc{indoorOutdoor2,
annote = {[Online; accessed 15-August-2016]},
author = {RoboticsAtHsUlm},
howpublished = {$\backslash$url{\{}https://youtu.be/fS3PJMswlH4{\}}},
title = {{Demonstrating System Integration by Composition: Seamless Indoor and Outdoor Navigation}},
year = {2015}
}
@inproceedings{Stewart2012,
author = {Stewart, Alexander D. and Newman, Paul},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224750},
file = {::},
isbn = {978-1-4673-1405-3},
keywords = {3D laser pointcloud,3D lidar data,6DoF monocular camera localisation,Cameras,LAPS,NID,Robot kinematics,Robot sensing systems,Robustness,Splines (mathematics),Vehicles,autonomous transport systems,camera motion inference,cameras,control engineering computing,cross-modal approach,inference mechanisms,lidar pointcloud,localisation using appearance of prior structure,low cost vehicles,minimisation,mobile robots,monocular cameras,normalised information distance,optical radar,overlapping images,pose estimation,preprocessed 3D lidar workspaces surveys,prior pointclouds,road vehicle,road vehicle radar,road vehicles,scene lighting,workspace prior},
month = {may},
pages = {2625--2632},
publisher = {IEEE},
title = {{LAPS - localisation using appearance of prior structure: 6-DoF monocular camera localisation using prior pointclouds}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6224750},
year = {2012}
}
@article{Maddern2014,
author = {Maddern, Will and Stewart, Alexander D and McManus, Colin and Upcroft, Ben and Churchill, Winston and Newman, Paul},
journal = {Proceedings of the Visual Place Recognition in Changing Environments Workshop, IEEE International Conference on Robotics and Automation},
title = {{Illumination Invariant Imaging: Applications in Robust Vision-based Localisation, Mapping and Classification for Autonomous Vehicles}},
year = {2014}
}
@inproceedings{Maddern2014a,
author = {Maddern, Will and Stewart, Alexander D. and Newman, Paul},
booktitle = {2014 IEEE Intelligent Vehicles Symposium Proceedings},
doi = {10.1109/IVS.2014.6856471},
isbn = {978-1-4799-3638-0},
month = {jun},
pages = {330--337},
publisher = {IEEE},
title = {{LAPS-II: 6-DoF day and night visual localisation with prior 3D structure for autonomous road vehicles}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6856471},
year = {2014}
}
@inproceedings{Badino2012a,
author = {Badino, Hernan and Huber, Daniel and Kanade, Takeo},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224716},
file = {::},
isbn = {978-1-4673-1405-3},
keywords = {3D features,Bayes methods,Bayesian filter,Databases,Feature extraction,GPS denied situations,GPS-equipped vehicle,Global Positioning System,Measurement,Probability density function,Vehicles,Visualization,autonomous vehicles,feature extraction,geometry,kidnapped robot problem,metric method geometric accuracy,mobile robots,path planning,range information,real-time topometric localization,robot vision,route navigation,vehicle localization,visual features,visual imagery},
month = {may},
pages = {1635--1642},
publisher = {IEEE},
title = {{Real-time topometric localization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6224716},
year = {2012}
}
@inproceedings{Xu2014,
author = {Xu, Danfei and Badino, Hernan and Huber, Daniel},
booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2014.6943043},
file = {::},
isbn = {978-1-4799-6934-0},
keywords = {Databases,GPS-based device,Global Positioning System,Measurement,Probability density function,Roads,Vehicles,Visualization,computer vision,highway scenario,road curvature descriptor,road curvature estimates,road network,road traffic control,suburban scenario,topometric localization,vision},
month = {sep},
pages = {3448--3455},
publisher = {IEEE},
title = {{Topometric localization on a road network}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6943043},
year = {2014}
}
@article{Maddern,
abstract = {— Robust and reliable visual localisation at any time of day is an essential component towards low-cost autonomy for road vehicles. We present a method to perform online 6-DoF visual localisation across a wide range of outdoor illumination conditions throughout the day and night using a 3D scene prior collected by a survey vehicle. We propose the use of a one-dimensional illumination invariant colour space which stems from modelling the spectral properties of the camera and scene illumination in conjunction. We combine our previous work on Localisation with Appearance of Prior Structure (LAPS) with this illumination invariant colour space to demonstrate a marked improvement in our ability to localise throughout the day compared to using a conventional RGB colour space. Our ultimate goal is robust and reliable any-time localisation -an attractive proposition for low-cost autonomy for road vehicles. Accordingly, we demonstrate our technique using 32km of data collected over a full 24-hour period from a road vehicle.},
author = {Maddern, Will and Stewart, Alexander D and Newman, Paul},
file = {:home/sourav/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maddern, Stewart, Newman - Unknown - LAPS-II 6-DoF Day and Night Visual Localisation with Prior 3D Structure for Autonomous Road Vehicle.pdf:pdf},
keywords = {()},
title = {{LAPS-II: 6-DoF Day and Night Visual Localisation with Prior 3D Structure for Autonomous Road Vehicles}}
}
@inproceedings{Yu2014,
author = {Yu, Yufeng and Zhao, Huijing and Davoine, Franck and Cui, Jinshi and Zha, Hongbin},
booktitle = {2014 IEEE Intelligent Vehicles Symposium Proceedings},
doi = {10.1109/IVS.2014.6856539},
file = {::},
isbn = {978-1-4799-3638-0},
keywords = {Cameras,Feature extraction,GPS localization,GPS/IMU data,Global Positioning System,Image segmentation,RSF,Roads,Vehicles,Visualization,dynamic traffic environments,geometric information,geometry,monocular visual localization,road structural features,road traffic,traffic information systems,video streams},
month = {jun},
pages = {693--699},
publisher = {IEEE},
title = {{Monocular visual localization using road structural features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6856539},
year = {2014}
}
@inproceedings{Wu2013,
author = {Wu, Tao and Ranganathan, Ananth},
booktitle = {2013 IEEE Intelligent Vehicles Symposium (IV)},
doi = {10.1109/IVS.2013.6629627},
file = {::},
isbn = {978-1-4673-2755-8},
keywords = {Boosting,Cameras,Estimation,Global Positioning System,Lighting,Roads,Vehicles,feature detection,feature extraction,global localization,lane-level visual localization,lighting,lighting conditions,object detection,object recognition,pose estimation,relative positioning information,reliable visual localization,road markings automatic recognition,traffic engineering computing,traffic signs,vehicle localization},
month = {jun},
pages = {1185--1190},
publisher = {IEEE},
title = {{Vehicle localization using road markings}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6629627},
year = {2013}
}
@article{Derner2014,
author = {Derner, Erik and Svoboda, Tom{\'{a}}{\v{s}}},
file = {::},
issn = {1213-2365},
journal = {Research Reports of CMP},
number = {25},
title = {{Indexing Images for Visual Memory by Using DNN Descriptors – Preliminary Experiments}},
url = {www: http://cmp.felk.cvut.cz},
year = {2014}
}
@article{Veronese2015,
author = {Veronese, Lucas de Paula and {Auat Cheein}, Fernando and Bastos-Filho, Teodiano and {Ferreira De Souza}, Alberto and de Aguiar, Edilson},
doi = {10.1002/rob.21594},
file = {::},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {jun},
title = {{A Computational Geometry Approach for Localization and Tracking in GPS-denied Environments}},
url = {http://doi.wiley.com/10.1002/rob.21594},
year = {2015}
}
@inproceedings{Suganuma2015,
author = {Suganuma, N. and Yamamoto, D.},
booktitle = {2015 IEEE/SICE International Symposium on System Integration (SII)},
doi = {10.1109/SII.2015.7405024},
file = {::},
isbn = {978-1-4673-7242-8},
keywords = {Autonomous automobiles,Cities and towns,Global Positioning System,Ishikawa prefecture,Japanese University,Kanazawa University,Laser radar,Real-time systems,Reflectivity,Roads,Suzu city government,Vehicles,autonomous vehicle,map based localization,mobile robots,path planning,public road experiments,public transport,public transport system,public urban road driving evaluation,public urban road driving test,public urban road experiment,road vehicles},
month = {dec},
pages = {467--471},
publisher = {IEEE},
title = {{Map based localization of autonomous vehicle and its public urban road driving evaluation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7405024},
year = {2015}
}
@inproceedings{Balac2014,
author = {Balac, Katarina and {Di Giulio}, Pablo Andres and Taddeo, Antonio V. and Prevostini, Mauro},
booktitle = {2014 IEEE International Conference on Pervasive Computing and Communication Workshops (PERCOM WORKSHOPS)},
doi = {10.1109/PerComW.2014.6815226},
file = {::},
isbn = {978-1-4799-2736-4},
keywords = {Calibration,Distance measurement,Error compensation,Lugano,Measurement uncertainty,Noise,Roads,Switzerland,Vehicles,channel noise,distance estimation,emergency management,error compensation,in-tunnel vehicle localization,localization accuracy,multipath channels,multipath propagation,radiowave propagation,time of flight error compensation,tunnels,vehicle localization system,wireless channels,wireless signals},
month = {mar},
pages = {326--331},
publisher = {IEEE},
title = {{Time of flight error compensation for in-tunnel vehicle localization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6815226},
year = {2014}
}
@inproceedings{Aly2015,
author = {Aly, Heba and Basalamah, Anas and Youssef, Moustafa},
booktitle = {2015 IEEE International Conference on Pervasive Computing and Communications (PerCom)},
doi = {10.1109/PERCOM.2015.7146523},
file = {::},
isbn = {978-1-4799-8033-8},
keywords = {Accuracy,Estimation,LaneQuest system,Markov processes,Probabilistic logic,Roads,Sensors,Vehicles,ambiguous location,car angular velocity,car lane estimation,car lane position detection,commodity smart-phones,energy-efficient lane detection system,fuzzy lane anchors,lane-level anchors,lane-span distribution,low-energy inertial sensors,mobile computing,object detection,outdoor localization techniques,phone sensors,position distribution,probabilistic lane estimation algorithm,probability,sensor noise,sensor placement,traffic engineering computing,unsupervised crowdsourcing approach},
month = {mar},
pages = {163--171},
publisher = {IEEE},
title = {{LaneQuest: An accurate and energy-efficient lane detection system}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7146523},
year = {2015}
}
@article{Yan2014,
author = {Yan, Tan and Zhang, Wensheng and Wang, Guiling},
doi = {10.1109/TWC.2013.122313.130547},
file = {::},
issn = {1536-1276},
journal = {IEEE Transactions on Wireless Communications},
keywords = {Distance measurement,Equations,GOT,GPS navigators,GPS signals,Global Positioning System,Mathematical model,Satellites,VANET,Vehicles,Vehicular ad hoc networks,convenience problems,error,fuzzy geometric relationship,fuzzy systems,grid-based,grid-based on-road localization system,linear,linear error propagation,localization,pattern,satellite communication,satellite signal,vehicular ad hoc network,vehicular ad hoc networks},
month = {feb},
number = {2},
pages = {861--875},
publisher = {IEEE},
title = {{A Grid-Based On-Road Localization System in VANET with Linear Error Propagation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6697933},
volume = {13},
year = {2014}
}
@inproceedings{Schreiber2013,
author = {Schreiber, Markus and Knoppel, Carsten and Franke, Uwe},
booktitle = {2013 IEEE Intelligent Vehicles Symposium (IV)},
doi = {10.1109/IVS.2013.6629509},
file = {::},
isbn = {978-1-4673-2755-8},
keywords = {Accuracy,Cameras,GNSS position,Global Navigation Satellite Systems,LaneLoc,Position measurement,Roads,Robustness,Vehicles,autonomous driving,centimeter-range precision,curbs,driver assistance systems,driver information systems,extended sensor setup,global navigation satellite systems,highly accurate maps,image sensors,lane marking based localization,object detection,real-world traffic scenarios,road markings,roads,rural roads,satellite navigation,stereo camera system,stereo image processing},
month = {jun},
pages = {449--454},
publisher = {IEEE},
title = {{LaneLoc: Lane marking based localization using highly accurate maps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6629509},
year = {2013}
}
@article{Zarza2015,
author = {Zarza, Himan and Yousefi, Saleh and Benslimane, Abderrahim},
doi = {10.1002/wcm.2604},
file = {::},
issn = {15308669},
journal = {Wireless Communications and Mobile Computing},
keywords = {GPS,inertial navigation system (INS),localization,locus circle,ranging error,roadside unit (RSU),vehicular network},
month = {jun},
pages = {n/a--n/a},
title = {{RIALS: RSU/INS-aided localization system for GPS-challenged road segments}},
url = {http://doi.wiley.com/10.1002/wcm.2604},
year = {2015}
}
@article{Zarza2015a,
author = {Zarza, H and Yousefi, S},
journal = {and Mobile Computing},
title = {{RIALS: RSU/INS‐aided localization system for GPS‐challenged road segments}},
year = {2015}
}
@article{Savic2013,
author = {Savic, V and Wymeersch, H},
journal = {Fusion (FUSION), 2013  {\ldots}},
title = {{Simultaneous sensor localization and target tracking in mine tunnels}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6641167},
year = {2013}
}
@article{Mur-Artal2015,
abstract = {This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
archivePrefix = {arXiv},
arxivId = {1502.00956},
author = {Mur-Artal, Raul and Montiel, J M M and Tardos, Juan D},
doi = {10.1109/TRO.2015.2463671},
eprint = {1502.00956},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
month = {feb},
pages = {1--17},
title = {{ORB-SLAM: A Versatile and Accurate Monocular SLAM System}},
url = {http://arxiv.org/abs/1502.00956},
year = {2015}
}
@article{Collett2010,
abstract = {Many animals learn to follow habitual routes between important locations, but how they encode their routes is still largely unknown. Desert ants traveling between their nest and a food site develop stable, visually guided routes that can wind through desert scrub without the use of trail pheromones. Their route memories are sufficiently robust that if a nest-bound ant is caught at the end of its route and replaced somewhere earlier along it, the ant will recapitulate the route from the release site. Insects appear to use panoramas to recognize when they are on a familiar route. I examine here the cues then used for their guidance. Several mechanisms are known for straight segments of a route; but how does an ant encode a curved route along which both the views it sees, and the directions it takes, are constantly changing? The results here suggest that when an ant travels past a landmark on a familiar route, it uses the gradually changing direction of the landmark to trigger a set of associated learned heading directions. A route through a complex 2D environment could thus be encoded and followed economically if it is divided into panorama-defined segments, with each segment controlled by such a 1D mapping. The solution proposed for the ants would be simple to implement in an autonomous robot.},
author = {Collett, Matthew},
doi = {10.1073/pnas.1001401107},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Algorithms,Animal,Animals,Ants,Ants: physiology,Behavior,Bio,Brain Mapping,Cues,Desert Climate,Exploratory Behavior,Feeding Behavior,Insects,Learning,Memory,Orientation,Space Perception,Spatial Behavior},
mendeley-tags = {Bio},
month = {jun},
number = {25},
pages = {11638--11643},
pmid = {20534539},
title = {{How desert ants use a visual landmark for guidance along a habitual route.}},
url = {http://www.pnas.org/content/107/25/11638.short},
volume = {107},
year = {2010}
}
@article{Widmann2013,
author = {Widmann, D and Balac, K and Taddeo, AV},
file = {::},
journal = {(WCNC), 2013 IEEE},
title = {{Characterization of in-tunnel distance measurements for vehicle localization}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6554921},
year = {2013}
}
@article{Strasdat2010a,
abstract = {State of the art visual SLAM systems have recently been presented which are capable of accurate, large-scale and real-time performance, but most of these require stereo vision. Important application areas in robotics and beyond open up if similar performance can be demonstrated using monocular vision, since a single camera will always be cheaper, more compact and easier to calibrate than a multi-camera rig. With high quality estimation, a single camera moving through a static scene of course effectively provides its own stereo geometry via frames distributed over time. However, a classic issue with monocular visual SLAM is that due to the purely projective nature of a single camera, motion estimates and map structure can only be recovered up to scale. Without the known inter-camera distance of a stereo rig to serve as an anchor, the scale of locally constructed map portions and the corresponding motion estimates is therefore liable to drift over time. In this paper we describe a new near real-time visual SLAM system which adopts the continuous keyframe optimisation ap- proach of the best current stereo systems, but accounts for the additional challenges presented by monocular input. In particular, we present a new pose-graph optimisation technique which allows for the efficient correction of rotation, translation and scale drift at loop closures. Especially, we describe the Lie group of similarity transformations and its relation to the corresponding Lie algebra.We also present in detail the systems new image processing front-end which is able accurately to track hundreds of features per frame, and a filter-based approach for feature initialisation within keyframe-based SLAM. Our approach is proven via large-scale simulation and real-world experiments where a camera completes large looped trajectories.},
author = {Strasdat, Hauke and Montiel, J M M and Davison, Andrew J},
doi = {10.1.1.165.7975},
isbn = {978-0-262-51681-5},
journal = {Robotics: Science and Systems},
pages = {5},
title = {{Scale Drift-Aware Large Scale Monocular SLAM}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.7975{\&}rep=rep1{\&}type=pdf},
volume = {2},
year = {2010}
}
@inproceedings{Lourakis,
author = {Lourakis, Manolis and Hourdakis, Emmanouil},
booktitle = {Proc. of the 13th Symposium on Advanced Space Technologies in Automation and Robotics (ASTRA'15). European Space Agency},
title = {{Planetary rover absolute localization by combining visual odometry with orbital image measurements}},
year = {2015}
}
@inproceedings{Gr2015,
author = {Grater, Johannes and Schwarze, Tobias and Lauer, Martin},
booktitle = {Intelligent Vehicles Symposium (IV), 2015 IEEE},
organization = {IEEE},
pages = {475--480},
title = {{Robust scale estimation for monocular visual odometry using structure from motion and vanishing points}},
year = {2015}
}
@inproceedings{Sunderhauf2012b,
abstract = {Current SLAM back-ends are based on least squares optimization and thus are not robust against outliers like data association errors and false positive loop closure detections. Our paper presents and evaluates a robust back-end formulation for SLAM using switchable constraints. Instead of proposing yet another appearance-based data association technique, our system is able to recognize and reject outliers during the optimization. This is achieved by making the topology of the underlying factor graph representation subject to the optimization instead of keeping it fixed. The evaluation shows that the approach can deal with up to 1000 false positive loop closure constraints on various datasets. This largely increases the robustness of the overall SLAM system and closes a gap between the sensor-driven front-end and the back-end optimizers.},
author = {Sunderhauf, Niko and Protzel, Peter},
booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6385590},
isbn = {978-1-4673-1736-8},
issn = {2153-0858},
keywords = {Convergence,Cost function,Estimation,Robustness,SLAM (robots),SLAM back-ends,SLAM system,Simultaneous localization and mapping,Switches,appearance-based data association technique,back-end formulation,back-end optimizers,factor graph representation,least squares optimization,optimisation,robust pose graph SLAM,sensor fusion,sensor-driven front-end,switchable constraints},
month = {oct},
pages = {1879--1884},
publisher = {IEEE},
shorttitle = {Intelligent Robots and Systems (IROS), 2012 IEEE/R},
title = {{Switchable constraints for robust pose graph SLAM}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6385590},
year = {2012}
}
@article{Thrun2006,
abstract = {This article presents GraphSLAM, a unifying algorithm for the offline SLAM problem. GraphSLAM is closely related to a recent sequence of research papers on applying optimization techniques to SLAM problems. It transforms the SLAM   posterior into a graphical network, representing the log-likelihood of the data. It then reduces this graph using variable elimination techniques, arriving at a   lower-dimensional problems that is then solved using conventional optimization techniques. As a result, GraphSLAM can generate maps with 108 or more   features. The paper discusses a greedy algorithm for data association, and presents results for SLAM in urban environments with occasional GPS measurements.},
author = {Thrun, S.},
doi = {10.1177/0278364906065387},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
month = {may},
number = {5-6},
pages = {403--429},
title = {{The Graph SLAM Algorithm with Applications to Large-Scale Mapping of Urban Structures}},
url = {http://ijr.sagepub.com/content/25/5-6/403.short},
volume = {25},
year = {2006}
}
@inproceedings{Olson2006,
abstract = {A robot exploring an environment can estimate its own motion and the relative positions of features in the environment. Simultaneous localization and mapping (SLAM) algorithms attempt to fuse these estimates to produce a map and a robot trajectory. The constraints are generally non-linear, thus SLAM can be viewed as a non-linear optimization problem. The optimization can be difficult, due to poor initial estimates arising from odometry data, and due to the size of the state space. We present a fast non-linear optimization algorithm that rapidly recovers the robot trajectory, even when given a poor initial estimate. Our approach uses a variant of stochastic gradient descent on an alternative state-space representation that has good stability and computational properties. We compare our algorithm to several others, using both real and synthetic data sets},
author = {Olson, E. and Leonard, J. and Teller, S.},
booktitle = {Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.},
doi = {10.1109/ROBOT.2006.1642040},
isbn = {0-7803-9505-0},
issn = {1050-4729},
keywords = {Constraint optimization,Fuses,Iterative algorithms,Motion estimation,Orbital robotics,Robots,Simultaneous localization and mapping,State estimation,State-space methods,Trajectory,alternative state-space representation,fast iterative alignment,mobile robots,motion control,motion estimation,nonlinear control systems,nonlinear optimization problem,optimisation,path planning,pose graphs,position control,robot trajectory,simultaneous localization and mapping algorithms,stability,stochastic gradient descent},
pages = {2262--2269},
publisher = {IEEE},
shorttitle = {Robotics and Automation, 2006. ICRA 2006. Proceedi},
title = {{Fast iterative alignment of pose graphs with poor initial estimates}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1642040},
year = {2006}
}
@inproceedings{concha2014using,
author = {Concha, Alejo and Civera, Javier},
booktitle = {Robotics and Automation (ICRA), 2014 IEEE International Conference on},
organization = {IEEE},
pages = {365--372},
title = {{Using superpixels in monocular SLAM}},
year = {2014}
}
@incollection{stuhmer2010real,
abstract = {We present a novel variational approach to estimate dense depth maps from multiple images in real-time. By using robust penalizers for both data term and regularizer, our method preserves discontinuities in the depth map. We demonstrate that the integration of multiple images substantially increases the robustness of estimated depth maps to noise in the input images. The integration of our method into recently published algorithms for camera tracking allows dense geometry reconstruction in real-time using a single handheld camera. We demonstrate the performance of our algorithm with real-world data.},
author = {St{\"{u}}hmer, Jan and Gumhold, Stefan and Cremers, Daniel},
booktitle = {Pattern Recognition},
keywords = {dense,mono},
mendeley-tags = {dense,mono},
pages = {11--20},
publisher = {Springer},
title = {{Real-time dense geometry from a handheld camera}},
year = {2010}
}
@inproceedings{Schops2014,
abstract = {We present a direct monocular visual odometry system which runs in real-time on a smartphone. Being a direct method, it tracks and maps on the images themselves instead of extracted features such as keypoints. New images are tracked using direct image alignment, while geometry is represented in the form of a semi-dense depth map. Depth is estimated by filtering over many small-baseline, pixel-wise stereo comparisons. This leads to significantly less outliers and allows to map and use all image regions with sufficient gradient, including edges. We show how a simple world model for AR applications can be derived from semi-dense depth maps, and demonstrate the practical applicability in the context of an AR application in which simulated objects can collide with real geometry.},
author = {Schops, Thomas and Engel, Jakob and Cremers, Daniel},
booktitle = {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
doi = {10.1109/ISMAR.2014.6948420},
isbn = {978-1-4799-6184-9},
keywords = {3D Reconstruction,AR,AR applications,Accuracy,Cameras,Direct Visual Odometry,Feature extraction,Image resolution,Mapping,Mobile Devices,NEON,Real-time systems,Semi-Dense,Simultaneous localization and mapping,Tracking,VO,augmented reality,depth estimation,direct image alignment,direct monocular visual odometry system,feature extraction,filtering theory,image tracking,mono,object tracking,pixel-wise stereo comparison,semi dense,semidense depth map,semidense visual odometry,smart phone,smart phones,stereo image processing},
mendeley-tags = {VO,mono,semi dense},
month = {sep},
pages = {145--150},
publisher = {IEEE},
shorttitle = {Mixed and Augmented Reality (ISMAR), 2014 IEEE Int},
title = {{Semi-dense visual odometry for AR on a smartphone}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6948420},
year = {2014}
}
@inproceedings{Engel2013,
abstract = {We propose a fundamentally novel approach to real-time visual odometry for a monocular camera. It allows to benefit from the simplicity and accuracy of dense tracking - which does not depend on visual features - while running in real-time on a CPU. The key idea is to continuously estimate a semi-dense inverse depth map for the current frame, which in turn is used to track the motion of the camera using dense image alignment. More specifically, we estimate the depth of all pixels which have a non-negligible image gradient. Each estimate is represented as a Gaussian probability distribution over the inverse depth. We propagate this information over time, and update it with new measurements as new images arrive. In terms of tracking accuracy and computational speed, the proposed method compares favorably to both state-of-the-art dense and feature-based visual odometry and SLAM algorithms. As our method runs in real-time on a CPU, it is of large practical value for robotics and augmented reality applications.},
author = {Engel, Jakob and Sturm, Jurgen and Cremers, Daniel},
booktitle = {2013 IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.183},
isbn = {978-1-4799-2840-8},
issn = {1550-5499},
keywords = {Accuracy,CPU,Cameras,Gaussian distribution,Gaussian probability distribution,Noise,Real-time systems,Robustness,SLAM,SLAM algorithm,Simultaneous localization and mapping,VO,Visualization,augmented reality application,camera motion tracking,cameras,computational speed,dense,dense image alignment,dense tracking accuracy,feature-based visual odometry,image gradient,image motion analysis,mono,monocular,monocular camera,pixel depth estimation,real-time visual odometry,robotics application,semi dense,semidense inverse depth map estimation,semidense visual odometry,stereo,visual odometry},
mendeley-tags = {VO,mono,semi dense},
month = {dec},
pages = {1449--1456},
publisher = {IEEE},
shorttitle = {Computer Vision (ICCV), 2013 IEEE International Co},
title = {{Semi-dense Visual Odometry for a Monocular Camera}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751290},
year = {2013}
}
@inproceedings{Pizzoli2014,
abstract = {In this paper, we solve the problem of estimating dense and accurate depth maps from a single moving camera. A probabilistic depth measurement is carried out in real time on a per-pixel basis and the computed uncertainty is used to reject erroneous estimations and provide live feedback on the reconstruction progress. Our contribution is a novel approach to depth map computation that combines Bayesian estimation and recent development on convex optimization for image processing. We demonstrate that our method outperforms state-of-the-art techniques in terms of accuracy, while exhibiting high efficiency in memory usage and computing power. We call our approach REMODE (REgularized MOnocular Depth Estimation). Our CUDA-based implementation runs at 30Hz on a laptop computer and is released as open-source software.},
author = {Pizzoli, Matia and Forster, Christian and Scaramuzza, Davide},
booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2014.6907233},
isbn = {978-1-4799-3685-4},
keywords = {Bayes methods,Bayesian estimation,CUDA-based implementation,Cameras,Estimation,Image reconstruction,Measurement uncertainty,Noise measurement,Robot vision systems,convex optimization,convex programming,dense,dense map estimation,depth map computation,depth map estimation,image processing,image reconstruction,laptop computer,memory usage,mono,monocular dense reconstruction,moving camera,open-source software,parallel architectures,probabilistic depth measurement,regularized monocular depth estimation,robot perception,robot vision},
mendeley-tags = {dense,mono},
month = {may},
pages = {2609--2616},
publisher = {IEEE},
shorttitle = {Robotics and Automation (ICRA), 2014 IEEE Internat},
title = {{REMODE: Probabilistic, monocular dense reconstruction in real time}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6907233},
year = {2014}
}
@inproceedings{Concha2014,
abstract = {Monocular SLAM and Structure from Motion have been traditionally based on finding point correspondences in highly-textured image areas. Large textureless regions, usually found in indoor and urban environments, are difficult to reconstruct by these systems. In this paper we augment for the first time the traditional point-based monocular SLAM maps with superpixels. Superpixels are middle-level features consisting of image regions of homogeneous texture. We propose a novel scheme for superpixel matching, 3D initialization and optimization that overcomes the difficulties of salient point-based approaches in these areas of homogeneous texture. Our experimental results show the validity of our approach. First, we compare our proposal with a state-of-the-art multiview stereo system; being able to reconstruct the textureless regions that the latest cannot. Secondly, we present experimental results of our algorithm integrated with the point-based PTAM [1]; estimating, now in real-time, the superpixel textureless areas. Finally, we show the accuracy of the presented algorithm with a quantitative analysis of the estimation error.},
author = {Concha, Alejo and Civera, Javier},
booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2014.6906883},
isbn = {978-1-4799-3685-4},
keywords = {3D initialization,Cameras,Image reconstruction,Optimization,Robustness,SLAM (robots),Simultaneous localization and mapping,Three-dimensional displays,Visualization,estimation error,homogeneous texture,image matching,image regions,mono,multiview stereo system,point-based PTAM,point-based monocular SLAM maps,quantitative analysis,stereo image processing,superpixel matching,superpixel textureless areas},
mendeley-tags = {mono},
month = {may},
pages = {365--372},
publisher = {IEEE},
shorttitle = {Robotics and Automation (ICRA), 2014 IEEE Internat},
title = {{Using superpixels in monocular SLAM}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6906883},
year = {2014}
}
@inproceedings{Pirker2011,
abstract = {When performing large-scale perpetual localization and mapping one faces problems like memory consumption or repetitive and dynamic scene elements requiring robust data association. We propose a visual SLAM method which handles short- and long-term scene dynamics in large environments using a single camera only. Through visibility-dependent map filtering and efficient keyframe organization we reach a considerable performance gain only through incorporation of a slightly more complex map representation. Experiments on a large, mixed indoor/outdoor dataset over a time period of two weeks demonstrate the scalability and robustness of our approach.},
author = {Pirker, Katrin and Ruther, Matthias and Bischof, Horst},
booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6094588},
isbn = {978-1-61284-456-5},
issn = {2153-0858},
keywords = {Buildings,CD SLAM,Cameras,Histograms,Optimization,Robustness,SLAM (robots),Simultaneous localization and mapping,Space exploration,complex map representation,continuous localization and mapping,dynamic scene elements,filtering theory,keyframe organization,memory consumption,mobile robot,mobile robots,perpetual localization and mapping,repetitive scene elements,robot dynamics,robot vision,robust data association,sensor fusion,visibility-dependent map filtering,visual SLAM method},
month = {sep},
pages = {3990--3997},
publisher = {IEEE},
shorttitle = {Intelligent Robots and Systems (IROS), 2011 IEEE/R},
title = {{CD SLAM - continuous localization and mapping in a dynamic world}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6094588},
year = {2011}
}
@incollection{klein2008improving,
author = {Klein, Georg and Murray, David},
booktitle = {Computer Vision--ECCV 2008},
pages = {802--815},
publisher = {Springer},
title = {{Improving the agility of keyframe-based SLAM}},
year = {2008}
}
@book{Forsyth2008,
abstract = {We explore the suitability of different feature detectors for the task of image registration, and in particular for visual odometry, using two criteria: stability (persistence across viewpoint change) and accuracy (consistent localization across viewpoint change). In addition to the now-standard SIFT, SURF, FAST, and Harris detectors, we introduce a suite of scale-invariant center-surround detectors (CenSurE) that outperform the other detectors, yet have better computational characteristics than other scale-space detectors, and are capable of real-time implementation.},
address = {Berlin, Heidelberg},
author = {Agrawal, Motilal and Kurt, Konolige and Rufus, Blas Morten},
doi = {10.1007/978-3-540-88693-8},
isbn = {978-3-540-88692-1},
issn = {0302-9743},
keywords = {CenSurE},
mendeley-tags = {CenSurE},
pages = {102--115},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{CenSurE: Center Surround Extremas for Realtime Feature Detection and Matching}},
url = {http://www.springerlink.com/index/10.1007/978-3-540-88693-8},
volume = {5305},
year = {2008}
}
@inproceedings{EdwardRosten,
author = {Rosten, Edward and Drummond, Tom},
booktitle = {European Conference on Computer Vision (ECCV)},
keywords = {FAST},
mendeley-tags = {FAST},
pages = {430--443},
title = {{Machine learning for high-speed corner detection}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.3991},
year = {2006}
}
@article{Makadia2007,
author = {Makadia, Ameesh and Geyer, Christopher and Daniilidis, Kostas},
doi = {10.1007/s11263-007-0035-2},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {BA,SfM,Vo,correspondence-free motion,harmonic analysis,motion estimation,registration,structure from motion},
mendeley-tags = {BA,SfM,Vo},
month = {feb},
number = {3},
pages = {311--327},
publisher = {Kluwer Academic Publishers},
title = {{Correspondence-free Structure from Motion}},
url = {http://dl.acm.org/citation.cfm?id=1288790.1288796},
volume = {75},
year = {2007}
}
@inproceedings{Mouragnon2006,
abstract = {This paper describes a new vision based method for the simultaneous localization and mapping of mobile robots. The only data used is a video input from a moving calibrated monocular camera. From the detection and matching of interest points in images at video rate, robust estimates of the camera poses are computed in real-time and a 3D map of the environment is reconstructed. The computed 3D structure is constantly refined thanks to the introduction of a fast and local bundle adjustment method that makes this approach particularly accurate and reliable. Actually, this method can be seen as a new visual tool that may be used in conjunction with usual systems (GPS, inertia sensors, etc) in SLAM applications},
author = {Mouragnon, E. and Lhuillier, M. and Dhome, M. and Dekeyser, F. and Sayd, P.},
booktitle = {18th International Conference on Pattern Recognition (ICPR'06)},
doi = {10.1109/ICPR.2006.810},
isbn = {0-7695-2521-0},
issn = {1051-4651},
keywords = {3D structure,BA,Cameras,Computer vision,Embedded computing,Image reconstruction,Mobile robots,Optimization methods,Robot vision systems,Robustness,Simultaneous localization and mapping,Uncertainty,bundle adjustment method,calibrated monocular camera,image detection,image matching,image motion analysis,image reconstruction,mobile robot,mobile robots,mono,monocular vision,robot vision,simultaneous localization and mapping,video input,video signal processing},
mendeley-tags = {BA,mono},
pages = {1027--1031},
publisher = {IEEE},
shorttitle = {Pattern Recognition, 2006. ICPR 2006. 18th Interna},
title = {{Monocular Vision Based SLAM for Mobile Robots}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1699701},
volume = {3},
year = {2006}
}
@article{Strasdat2010,
abstract = {While the most accurate solution to off-line structure from motion (SFM) problems is undoubtedly to extract as much correspondence information as possible and perform global optimisation, sequential methods suitable for live video streams must approximate this to fit within fixed computational bounds. Two quite different approaches to real-time SFM - also called monocular SLAM (Simultaneous Localisation and Mapping) - have proven successful, but they sparsify the problem in different ways. Filtering methods marginalise out past poses and summarise the information gained over time with a probability distribution. Keyframe methods retain the optimisation approach of global bundle adjustment, but computationally must select only a small number of past frames to process. In this paper we perform the first rigorous analysis of the relative advantages of filtering and sparse optimisation for sequential monocular SLAM. A series of experiments in simulation as well using a real image SLAM system were performed by means of covariance propagation and Monte Carlo methods, and comparisons made using a combined cost/accuracy measure. With some well-discussed reservations, we conclude that while filtering may have a niche in systems with low processing resources, in most modern applications keyframe optimisation gives the most accuracy per unit of computing time.},
author = {Strasdat, Hauke and Montiel, J. M M and Davison, Andrew J.},
doi = {10.1109/ROBOT.2010.5509636},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {BA,Computational modeling,Data mining,Information filtering,Information filters,Monte Carlo methods,Optimization methods,Performance analysis,Performance evaluation,Probability distribution,SLAM (robots),SfM,Simultaneous localization and mapping,Streaming media,filtering,filtering theory,mono,monocular SLAM,robot vision,simultaneous localisation and mapping,sparse optimisation,structure from motion problems,video streams},
mendeley-tags = {BA,SfM,mono},
month = {may},
pages = {2657--2664},
pmid = {5509636},
publisher = {IEEE},
shorttitle = {Robotics and Automation (ICRA), 2010 IEEE Internat},
title = {{Real-time monocular SLAM: Why filter?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5509636},
volume = {30},
year = {2010}
}
@article{beardsley1997sequential,
abstract = {A structure from motion algorithm is described which recovers structure and camera position, modulo a projective ambiguity. Camera calibration is not required, and camera parameters such as focal length can be altered freely during motion. The structure is updated sequentially over an image sequence, in contrast to schemes which employ a batch process. A specialisation of the algorithm to recover structure and camera position modulo an affine transformation is described, together with a method to periodically update the affine coordinate frame to prevent drift over time. We describe the constraint used to obtain this specialisation. Structure is recovered from image corners detected and matched automatically and reliably in real image sequences. Results are shown for reference objects and indoor environments, and accuracy of recovered structure is fully evaluated and compared for a number of reconstruction schemes. A specific application of the work is demonstrated—affine structure is used to compute free space maps enabling navigation through unstructured environments and avoidance of obstacles. The path planning involves only affine constructions.},
author = {Beardsley, Paul A and Zisserman, Andrew and Murray, David W},
journal = {International journal of computer vision},
keywords = {SfM},
mendeley-tags = {SfM},
number = {3},
pages = {235--259},
publisher = {Springer},
title = {{Sequential updating of projective and affine structure from motion}},
volume = {23},
year = {1997}
}
@inproceedings{Nister2004,
abstract = {We present a system that estimates the motion of a stereo head or a single moving camera based on video input. The system operates in real-time with low delay and the motion estimates are used for navigational purposes. The front end of the system is a feature tracker. Point features are matched between pairs of frames and linked into image trajectories at video rate. Robust estimates of the camera motion are then produced from the feature tracks using a geometric hypothesize-and-test architecture. This generates what we call visual odometry, i.e. motion estimates from visual input alone. No prior knowledge of the scene nor the motion is necessary. The visual odometry can also be used in conjunction with information from other sources such as GPS, inertia sensors, wheel encoders, etc. The pose estimation method has been applied successfully to video from aerial, automotive and handheld platforms. We focus on results with an autonomous ground vehicle. We give examples of camera trajectories estimated purely from images over previously unseen distances and periods of time.},
author = {Nister, D. and Naroditsky, O. and Bergen, J.},
booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
doi = {10.1109/CVPR.2004.1315094},
isbn = {0-7695-2158-4},
issn = {1063-6919},
keywords = {Cameras,Delay estimation,Global Positioning System,Head,Layout,Motion estimation,Navigation,Real time systems,Robustness,SfM,Tracking,VO,aerial platforms,automotive platforms,autonomous ground vehicle,camera motion,camera trajectories,feature tracker,geometric hypothesize-and-test architecture,global positioning system,handheld platforms,image matching,image trajectories,inertial navigation,mono,motion estimation,navigation,pose estimation method,robust estimation,single moving camera,stereo,stereo head,vehicles,video cameras,video rate,video signal processing,visual odometry},
mendeley-tags = {SfM,VO,mono,stereo},
pages = {652--659},
publisher = {IEEE},
shorttitle = {Computer Vision and Pattern Recognition, 2004. CVP},
title = {{Visual odometry}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1315094},
volume = {1},
year = {2004}
}
@book{thrun2005probabilistic,
author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
title = {{Probabilistic robotics}},
year = {2005}
}
@article{Harris1988,
abstract = {An explicit three-dimensional (3D) representation is constructed from feature points extracted from a sequence of images taken by a moving camera. The points are tracked through the sequence, and their 3D locations are accurately determined by use of Kalman filters. The egomotion of the camera is also determined.},
author = {Harris, C.G. and Pike, J.M.},
doi = {10.1016/0262-8856(88)90003-0},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {3D scene representation,Kalman filter,SfM,egomotion,robot vision},
mendeley-tags = {SfM},
month = {may},
number = {2},
pages = {87--90},
title = {{3D positional integration from image sequences}},
url = {http://www.sciencedirect.com/science/article/pii/0262885688900030},
volume = {6},
year = {1988}
}
@book{montemerlo2007fastslam,
author = {Montemerlo, Michael and Thrun, Sebastian},
publisher = {Springer},
title = {{FastSLAM: A scalable method for the simultaneous localization and mapping problem in robotics}},
volume = {27},
year = {2007}
}
@inproceedings{Chatila1985,
abstract = {In order to understand its environment, a mobile robot should be able to model consistently this environment, and to locate itself correctly. One major difficulty to be solved is the inaccuracies introduced by the sensors. The approach proposed in this paper to cope with this problem relies on 1) defining general principles to deal with uncertainties : the use of a multisensory system, favo ring of the data collected by the more accurate sensor in a given situation, averaging of different but consistent measurements of the same entity weighted with their associated uncertainties, and 2) a methodology enabling a mobile robot to define its own reference landmarks while exploring its environment. These ideas are presented together with an example of their application on the mobile robot HILARE.},
author = {Chatila, R. and Laumond, J.},
booktitle = {Proceedings. 1985 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.1985.1087373},
keywords = {Degradation,EKF,Mobile robots,Navigation,Orbital robotics,Robot kinematics,Robot sensing systems,Robot vision systems,Sensor systems,Solid modeling,Trajectory},
mendeley-tags = {EKF},
pages = {138--145},
publisher = {Institute of Electrical and Electronics Engineers},
shorttitle = {Robotics and Automation. Proceedings. 1985 IEEE In},
title = {{Position referencing and consistent world modeling for mobile robots}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1087373},
volume = {2},
year = {1985}
}
@book{milford2008robot,
author = {Milford, Michael John},
keywords = {Bio},
mendeley-tags = {Bio},
publisher = {Springer Science {\&} Business Media},
title = {{Robot navigation from nature: Simultaneous localisation, mapping, and path planning based on hippocampal models}},
volume = {41},
year = {2008}
}
@article{Collett2010,
abstract = {Many animals learn to follow habitual routes between important locations, but how they encode their routes is still largely unknown. Desert ants traveling between their nest and a food site develop stable, visually guided routes that can wind through desert scrub without the use of trail pheromones. Their route memories are sufficiently robust that if a nest-bound ant is caught at the end of its route and replaced somewhere earlier along it, the ant will recapitulate the route from the release site. Insects appear to use panoramas to recognize when they are on a familiar route. I examine here the cues then used for their guidance. Several mechanisms are known for straight segments of a route; but how does an ant encode a curved route along which both the views it sees, and the directions it takes, are constantly changing? The results here suggest that when an ant travels past a landmark on a familiar route, it uses the gradually changing direction of the landmark to trigger a set of associated learned heading directions. A route through a complex 2D environment could thus be encoded and followed economically if it is divided into panorama-defined segments, with each segment controlled by such a 1D mapping. The solution proposed for the ants would be simple to implement in an autonomous robot.},
author = {Collett, Matthew},
doi = {10.1073/pnas.1001401107},
file = {::},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Algorithms,Animals,Ants,Ants: physiology,Behavior, Animal,Bio,Brain Mapping,Cues,Desert Climate,Exploratory Behavior,Feeding Behavior,Insects,Learning,Memory,Orientation,Space Perception,Spatial Behavior},
mendeley-tags = {Bio},
month = {jun},
number = {25},
pages = {11638--43},
pmid = {20534539},
title = {{How desert ants use a visual landmark for guidance along a habitual route.}},
url = {http://www.pnas.org/content/107/25/11638.short},
volume = {107},
year = {2010}
}
@inproceedings{Milford2004,
abstract = {The work presents a new approach to the problem of simultaneous localization and mapping - SLAM - inspired by computational models of the hippocampus of rodents. The rodent hippocampus has been extensively studied with respect to navigation tasks, and displays many of the properties of a desirable SLAM solution. RatSLAM is an implementation of a hippocampal model that can perform SLAM in real time on a real robot. It uses a competitive attractor network to integrate odometric information with landmark sensing to form a consistent representation of the environment. Experimental results show that RatSLAM can operate with ambiguous landmark information and recover from both minor and major path integration errors.},
author = {Milford, M.J. and Wyeth, G.F. and Prasser, D.},
booktitle = {IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004},
doi = {10.1109/ROBOT.2004.1307183},
isbn = {0-7803-8232-3},
issn = {1050-4729},
keywords = {Bio,Computational modeling,Computer networks,Hippocampus,Intelligent robots,Mobile robots,Navigation,Orbital robotics,Robot sensing systems,Rodents,Simultaneous localization and mapping,competitive attractor network,distance measurement,hippocampal model,mobile robots,navigation tasks,odometric information,path planning,rodent hippocampus model,simultaneous localization and mapping},
mendeley-tags = {Bio},
pages = {403--408 Vol.1},
publisher = {IEEE},
shorttitle = {Robotics and Automation, 2004. Proceedings. ICRA '},
title = {{RatSLAM: a hippocampal model for simultaneous localization and mapping}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1307183},
volume = {1},
year = {2004}
}
@inproceedings{Floros2013,
abstract = {In this paper we propose an approach for global vehicle localization that combines visual odometry with map information from OpenStreetMaps to provide robust and accurate estimates for the vehicle's position. The main contribution of this work comes from the incorporation of the map data as an additional cue into the observation model of a Monte Carlo Localization framework. The resulting approach is able to compensate for the drift that visual odometry accumulates over time, significantly improving localization quality. As our results indicate, the proposed approach outperforms current state-of-the-art visual odometry approaches, indicating in parallel the potential that map data can bring to the global localization task.},
author = {Floros, Georgios and van der Zander, Benito and Leibe, Bastian},
booktitle = {2013 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6630703},
isbn = {978-1-4673-5643-5},
issn = {1050-4729},
keywords = {GPS,Image edge detection,Monte Carlo localization framework,Monte Carlo methods,OpenStreetMaps,OpenStreetSLAM,SLAM (robots),VO,automobiles,cartography,distance measurement,global vehicle localization,image matching,localization quality improvement,map data,map information,observation model,robot vision,vehicle position estimation,visual odometry},
mendeley-tags = {GPS,VO},
month = {may},
pages = {1054--1059},
publisher = {IEEE},
shorttitle = {Robotics and Automation (ICRA), 2013 IEEE Internat},
title = {{OpenStreetSLAM: Global vehicle localization using OpenStreetMaps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6630703},
year = {2013}
}
@inproceedings{Bazeille2011,
abstract = {We address the problem of simultaneous localization and mapping by combining visual loop-closure detection with metrical information given by the robot odometry. The proposed algorithm builds in real-time topo-metric maps of an unknown environment, with a monocular or omnidirectional camera and odometry gathered by motors encoders. A dedicated improved version of our previous work on purely appearance-based loop-closure detection [1] is used to extract potential loop-closure locations. Potential locations are then verified and classified using a new validation stage. The main contributions we bring are the generalization of the validation method for the use of monocular and omnidirectional camera with the removal of the camera calibration stage, the inclusion of an odometry-based evolution model in the Bayesian filter which improves accuracy and responsiveness, and the addition of a consistent metric position estimation. This new SLAM method does not require any calibration or learning stage (i.e. no a priori information about environment). It is therefore fully incremental and generates maps usable for global localization and planned navigation. This algorithm is moreover well suited for remote processing and can be used on toy robots with very small computational power.},
author = {Bazeille, Stephane and Filliat, David},
booktitle = {2011 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5979908},
isbn = {978-1-61284-386-5},
issn = {1050-4729},
keywords = {Bayes methods,Bayesian filter,Cameras,Computational modeling,Odometry,Robot kinematics,Robot vision systems,SLAM,SLAM (robots),Simultaneous localization and mapping,calibration,camera calibration,distance measurement,global localization,hybrid topo-metric map,metric position estimation,mobile robots,monocular camera,motors encoders,object detection,odometry-based evolution model,omnidirectional camera,path planning,planned navigation,real-time topo metric maps,remote processing,robot odometry,robot vision,simultaneous localization and mapping,topometric,uncalibrated camera,visual loop closure detection},
mendeley-tags = {Odometry,topometric},
month = {may},
pages = {4067--4073},
publisher = {IEEE},
shorttitle = {Robotics and Automation (ICRA), 2011 IEEE Internat},
title = {{Incremental topo-metric SLAM using vision and robot odometry}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5979908},
year = {2011}
}
@inproceedings{Kohlbrecher2011,
abstract = {For many applications in Urban Search and Rescue (USAR) scenarios robots need to learn a map of unknown environments. We present a system for fast online learning of occupancy grid maps requiring low computational resources. It combines a robust scan matching approach using a LIDAR system with a 3D attitude estimation system based on inertial sensing. By using a fast approximation of map gradients and a multi-resolution grid, reliable localization and mapping capabilities in a variety of challenging environments are realized. Multiple datasets showing the applicability in an embedded hand-held mapping system are provided. We show that the system is sufficiently accurate as to not require explicit loop closing techniques in the considered scenarios. The software is available as an open source package for ROS.},
author = {Kohlbrecher, Stefan and von Stryk, Oskar and Meyer, Johannes and Klingauf, Uwe},
booktitle = {2011 IEEE International Symposium on Safety, Security, and Rescue Robotics},
doi = {10.1109/SSRR.2011.6106777},
isbn = {978-1-61284-769-6},
keywords = {3D,3D attitude estimation system,Equations,IMU,Inertial Navigation,LIDAR system,Laser,Laser radar,Navigation,Robust and Fast Localization,SLAM (robots),Simultaneous Localization and Mapping,Simultaneous localization and mapping,Three dimensional displays,full 3D motion estimation,inertial sensing,map gradients,motion estimation,occupancy grid maps,online learning,optical radar,robust scan matching approach,scalable SLAM system,unknown environments,urban search and rescue scenario},
mendeley-tags = {3D,IMU,Laser},
month = {nov},
pages = {155--160},
publisher = {IEEE},
shorttitle = {Safety, Security, and Rescue Robotics (SSRR), 2011},
title = {{A flexible and scalable SLAM system with full 3D motion estimation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6106777},
year = {2011}
}
@inproceedings{Agrawal2006,
abstract = {We describe a real-time, low-cost system to localize a mobile robot in outdoor environments. Our system relies on stereo vision to robustly estimate frame-to-frame motion in real time (also known as visual odometry). The motion estimation problem is formulated efficiently in the disparity space and results in accurate and robust estimates of the motion even for a small-baseline configuration. Our system uses inertial measurements to fill in motion estimates when visual odometry fails. This incremental motion is then fused with a low-cost GPS sensor using a Kalman filter to prevent long-term drifts. Experimental results are presented for outdoor localization in moderately sized environments (ges100 meters)},
author = {Agrawal, M. and Konolige, K.},
booktitle = {18th International Conference on Pattern Recognition (ICPR'06)},
doi = {10.1109/ICPR.2006.962},
isbn = {0-7695-2521-0},
issn = {1051-4651},
keywords = {Cameras,GPS,GPS sensor,Global Positioning System,IMU,Kalman filter,Kalman filters,Mobile robots,Motion estimation,Robot sensing systems,Robot vision systems,Robustness,Sensor systems,Simultaneous localization and mapping,Stereo vision,VO,frame-to-frame motion,inertial measurement,mobile robot,mobile robots,motion estimation,outdoor localization,robot vision,stereo,stereo image processing,stereo vision,visual odometry},
mendeley-tags = {GPS,IMU,VO,stereo},
pages = {1063--1068},
publisher = {IEEE},
shorttitle = {Pattern Recognition, 2006. ICPR 2006. 18th Interna},
title = {{Real-time Localization in Outdoor Environments using Stereo Vision and Inexpensive GPS}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1699709},
volume = {3},
year = {2006}
}
@inproceedings{Kneip2011,
author = {Kneip, Laurent and Chli, Margarita and Siegwart, Roland Yves},
booktitle = {British Machine Vision Conference},
doi = {10.3929/ethz-a-010025746},
keywords = {IMU,VO},
mendeley-tags = {IMU,VO},
publisher = {Eidgen{\"{o}}ssische Technische Hochschule Z{\"{u}}rich, Autonomous Systems Lab},
title = {{Robust Real-Time Visual Odometry with a Single Camera and an IMU}},
url = {http://e-collection.library.ethz.ch/view/eth:7683},
year = {2011}
}
@article{Mahon2008,
abstract = {This paper presents a simultaneous localization and mapping algorithm suitable for large-scale visual navigation. The estimation process is based on the viewpoint augmented navigation (VAN) framework using an extended information filter. Cholesky factorization modifications are used to maintain a factor of the VAN information matrix, enabling efficient recovery of state estimates and covariances. The algorithm is demonstrated using data acquired by an autonomous underwater vehicle performing a visual survey of sponge beds. Loop-closure observations produced by a stereo vision system are used to correct the estimated vehicle trajectory produced by dead reckoning sensors.},
author = {Mahon, I. and Williams, S.B. and Pizarro, O. and Johnson-Roberson, M.},
doi = {10.1109/TRO.2008.2004888},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
keywords = {AUV SIRIUS,Autonomous underwater vehicle (AUV) navigation,Cholesky factorization,Cholesky factorization modifications,Covariance matrix,Information filters,Kalman filters,Large-scale systems,Navigation,SLAM,SLAM (robots),Simultaneous localization and mapping,State estimation,Stereo vision,autonomous underwater vehicle,dead reckoning,extended information filter,extended information filter (EIF),information matrix,large-scale visual navigation,loop-closure observations,metric,mobile robots,odometry,remotely operated vehicles,robot vision,simultaneous localization and mapping (SLAM),simultaneous localization and mapping algorithm,stereo,underwater vehicles,viewpoint augmented navigation framework,visual loop closures},
language = {English},
mendeley-tags = {AUV SIRIUS,metric,odometry,stereo},
month = {oct},
number = {5},
pages = {1002--1014},
title = {{Efficient View-Based SLAM Using Visual Loop Closures}},
url = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000004652565},
volume = {24},
year = {2008}
}
@book{thrun2008simultaneous,
address = {Berlin, Heidelberg},
author = {Thrun, Sebastian and Leonard, John J.},
doi = {10.1007/978-3-540-30301-5},
editor = {Siciliano, Bruno and Khatib, Oussama},
isbn = {978-3-540-23957-4},
keywords = {SLAM methods},
mendeley-tags = {SLAM methods},
pages = {871--889},
publisher = {Springer Berlin Heidelberg},
title = {{Simultaneous Localization and Mapping}},
url = {http://link.springer.com/10.1007/978-3-540-30301-5},
year = {2008}
}
@inproceedings{Klein2007,
abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
author = {Klein, Georg and Murray, David},
booktitle = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2007.4538852},
isbn = {978-1-4244-1749-0},
keywords = {Algorithm design and analysis,Cameras,Concurrent computing,Handheld computers,Layout,PTAM,Robot vision systems,Robustness,SLAM (robots),SLAM algorithms,Simultaneous localization and mapping,Tracking,Yarn,augmented reality,batch optimisation techniques,hand-held camera,metric,mono,parallel mapping,parallel tracking,robot vision,robotic exploration},
mendeley-tags = {PTAM,metric,mono},
month = {nov},
pages = {1--10},
publisher = {IEEE},
shorttitle = {Mixed and Augmented Reality, 2007. ISMAR 2007. 6th},
title = {{Parallel Tracking and Mapping for Small AR Workspaces}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538852},
year = {2007}
}
@inproceedings{Konolige2011,
abstract = {We present an approach for navigation in hybrid maps consisting of a topological graph overlaid with local occupancy grids. The topological graph is built on top of a graph SLAM system, which can be efficiently optimized even for very large environments. The novel feature of our system is that it navigates locally using local metric maps, while the overall plan is formed on the topological graph. Unlike many current SLAM methods, we never reconstruct a full occupancy grid of the environment for localization or path planning. We show that our method generates near-optimal plans, and deals gracefully with changes to the map.},
author = {Konolige, Kurt and Marder-Eppstein, Eitan and Marthi, Bhaskara},
booktitle = {2011 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5980074},
isbn = {978-1-61284-386-5},
issn = {1050-4729},
keywords = {Measurement,Navigation,Planning,Robot kinematics,SLAM (robots),Simultaneous localization and mapping,graph SLAM system,graph theory,hybrid metric-topological maps,local metric maps,local occupancy grid,navigation,path planning,topological graph,topometric},
mendeley-tags = {topometric},
month = {may},
pages = {3041--3047},
publisher = {IEEE},
shorttitle = {Robotics and Automation (ICRA), 2011 IEEE Internat},
title = {{Navigation in hybrid metric-topological maps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5980074},
year = {2011}
}
@inproceedings{Angeli2009,
abstract = {Visual localization and mapping for mobile robots has been achieved with a large variety of methods. Among them, topological navigation using vision has the advantage of offering a scalable representation, and of relying on a common and affordable sensor. In previous work, we developed such an incremental and real-time topological mapping and localization solution, without using any metrical information, and by relying on a Bayesian visual loop-closure detection algorithm. In this paper, we propose an extension of this work by integrating metrical information from robot odometry in the topological map, so as to obtain a globally consistent environment model. Also, we demonstrate the performance of our system on the global localization task, where the robot has to determine its position in a map acquired beforehand.},
author = {Angeli, A. and Doncieux, S. and Meyer, J.-A. and Filliat, D.},
booktitle = {2009 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2009.5152501},
isbn = {978-1-4244-2788-8},
issn = {1050-4729},
keywords = {Bayes methods,Bayesian methods,Bayesian visual loop-closure detection algorithm,Cameras,Detection algorithms,Machine vision,Mobile robots,Navigation,Robot sensing systems,Robot vision systems,Robotics and automation,SLAM (robots),Simultaneous localization and mapping,global localization,metrical information,mobile robots,real-time topological mapping,robot odometry,topological navigation,topometric,visual localization,visual topological SLAM},
mendeley-tags = {topometric},
month = {may},
pages = {4300--4305},
publisher = {IEEE},
shorttitle = {Robotics and Automation, 2009. ICRA '09. IEEE Inte},
title = {{Visual topological SLAM and global localization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5152501},
year = {2009}
}
@article{Gutmann2008,
abstract = {A humanoid robot that can go up and down stairs, crawl underneath obstacles or simply walk around requires reliable perceptual capabilities for obtaining accurate and useful information about its surroundings. In this work we present a system for generating three-dimensional (3D) environment maps from data taken by stereo vision. At the core is a   method for precise segmentation of range data into planar segments based on the algorithm of scan-line grouping extended to cope with the noise dynamics of stereo vision. In off-line   experiments we demonstrate that our extensions achieve a more precise segmentation. When compared to a previously developed patch-let method, we  obtain a richer segmentation with a higher accuracy while also requiring far less computations. From the obtained segmentation we then build a 3D environment map using occupancy grid and floor height maps.   The resulting representation classifies areas into one of six different types while also providing object height information. We apply our perception method   for the navigation of the humanoid robot QRIO and present experiments of the robot stepping through narrow space, walking up and down stairs and crawling   underneath a table.},
author = {Gutmann, J.-S. and Fukuchi, M. and Fujita, M.},
doi = {10.1177/0278364908096316},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {metric},
mendeley-tags = {metric},
month = {oct},
number = {10},
pages = {1117--1134},
title = {{3D Perception and Environment Map Generation for Humanoid Robot Navigation}},
url = {http://ijr.sagepub.com/content/27/10/1117.short},
volume = {27},
year = {2008}
}
@article{Engels2006,
abstract = {In this paper we investigate the status of bundle adjustment as a component of a real-time camera tracking system and show that with current computing hardware a significant amount of bundle adjustment can be performed every time a new frame is added, even under stringent real-time constraints. We also show, by quantifying the failure rate over long video sequences, that the bundle adjustment is able to significantly decrease the rate of gross failures in the camera tracking. Thus, bundle adjustment does not only bring accuracy improvements. The accuracy improvements also suppress error buildup in a way that is crucial for the performance of the camera tracker. Our experimental study is performed in the setting of tracking the trajectory a calibrated camera moving in 3D for various types of motion, showing that bundle adjustment should be considered an important component for a state-of-the-art real-time camera tracking system.},
author = {Engels, Chris and Stew{\'{e}}nius, H and Nist{\'{e}}r, D},
doi = {10.1.1.126.6901},
journal = {Photogrammetric computer vision},
keywords = {BA,bundle adjustment,camera tracking,structure from motion},
mendeley-tags = {BA},
pages = {266--271},
pmid = {11239158},
title = {{Bundle adjustment rules}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.6901{\&}rep=rep1{\&}type=pdf},
year = {2006}
}
@inproceedings{Fraundorfer2007,
abstract = {In this paper we present a highly scalable vision-based localization and mapping method using image collections. A topological world representation is created online during robot exploration by adding images to a database and maintaining a link graph. An efficient image matching scheme allows real-time mapping and global localization. The compact image representation allows us to create image collections containing millions of images, which enables mapping of very large environments. A path planning method using graph search is proposed and local geometric information is used to navigate in the topological map. Experiments show the good performance of the image matching for global localization and demonstrate path planning and navigation.},
author = {Fraundorfer, Friedrich and Engels, Christopher and Nister, David},
booktitle = {2007 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2007.4399123},
isbn = {978-1-4244-0911-2},
keywords = {Cameras,Image databases,Image matching,Image representation,Legged locomotion,Navigation,Path planning,Robot vision systems,Spatial databases,USA Councils,graph search,image collection,image database,image matching,image matching scheme,image representation,mobile robots,navigation,path planning,path planning method,real-time mapping,real-time systems,search problems,topological,topological mapping,topological world representation,vision-based localization,visual databases},
mendeley-tags = {topological},
month = {oct},
pages = {3872--3877},
publisher = {IEEE},
shorttitle = {Intelligent Robots and Systems, 2007. IROS 2007. I},
title = {{Topological mapping, localization and navigation using image collections}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4399123},
year = {2007}
}
@article{Pollefeys2004,
abstract = {In this paper a complete system to build visual models from camera images is presented. The system can deal with uncalibrated image sequences acquired with a hand-held camera. Based on tracked or matched features the relations between multiple views are computed. From this both the structure of the scene and the motion of the camera are retrieved. The ambiguity on the reconstruction is restricted from projective to metric through self-calibration. A flexible multi-view stereo matching scheme is used to obtain a dense estimation of the surface geometry. From the computed data different types of visual models are constructed. Besides the traditional geometry- and image-based approaches, a combined approach with view-dependent geometry and texture is presented. As an application fusion of real and virtual scenes is also shown. visual modeling structure-from-motion projective reconstruction self-calibration multi-view stereo matching dense reconstruction 3D reconstruction image-based rendering augmented video hand-held camera},
author = {Pollefeys, Marc and {Van Gool}, Luc and Vergauwen, Maarten and Verbiest, Frank and Cornelis, Kurt and Tops, Jan and Koch, Reinhard},
doi = {10.1023/B:VISI.0000025798.50602.3a},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {BA,SfM,stereo},
mendeley-tags = {BA,SfM,stereo},
month = {sep},
number = {3},
pages = {207--232},
title = {{Visual Modeling with a Hand-Held Camera}},
url = {http://link.springer.com/10.1023/B:VISI.0000025798.50602.3a},
volume = {59},
year = {2004}
}
@article{Mur-Artal2015b,
abstract = {In the last years several direct (i.e. featureless) monocular SLAM approaches have appeared showing impressive semi-dense or dense scene reconstructions. These works have questioned the need of features, in which consolidated SLAM techniques of the last decade were based. In this paper we present a novel feature-based monocular SLAM system that is more robust, gives more accurate camera poses, and obtains comparable or better semi-dense reconstructions than the current state of the art. Our semi-dense mapping operates over keyframes, optimized by local bundle adjustment, allowing to obtain accurate triangulations from wide baselines. Our novel method to search correspondences, the measurement fusion and the inter-keyframe depth consistency tests allow to obtain clean reconstructions with very few outliers. Against the current trend in direct SLAM, our experiments show that by decoupling the semi-dense reconstruction from the trajectory computation, the results obtained are better. This opens the discussion on the benefits of features even if a semi-dense reconstruction is desired.},
author = {{Raul Mur-Artal and Juan D. Tard´os}},
journal = {Proceedings of Robotics: Science and Systems, Rome, Italy},
keywords = {Semi-Dense},
mendeley-tags = {Semi-Dense},
title = {{Probabilistic Semi-Dense Mapping from Highly Accurate Feature-Based Monocular SLAM}},
url = {https://www.researchgate.net/profile/Raul{\_}Mur-Artal/publication/282807894{\_}Probabilistic{\_}Semi-Dense{\_}Mapping{\_}from{\_}Highly{\_}Accurate{\_}Feature-Based{\_}Monocular{\_}SLAM/links/561cd04308ae6d17308ce267.pdf},
year = {2015}
}
@inproceedings{Engel2014lsd,
abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by ltering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on sim(3), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the e ect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.},
address = {Cham},
author = {Engel, Jakob and Sch{\"{o}}ps, Thomas and Cremers, Daniel},
booktitle = {European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-10605-2},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
isbn = {978-3-319-10604-5},
keywords = {LSD-SLAM,VO,mono,semi dense},
mendeley-tags = {LSD-SLAM,VO,mono,semi dense},
pages = {834--849},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{LSD-SLAM: Large- Scale Direct Monocular SLAM,}},
url = {http://link.springer.com/10.1007/978-3-319-10605-2},
volume = {8690},
year = {2014}
}
@inproceedings{Newcombe2011,
abstract = {DTAM is a system for real-time camera tracking and reconstruction which relies not on feature extraction but dense, every pixel methods. As a single hand-held RGB camera flies over a static scene, we estimate detailed textured depth maps at selected keyframes to produce a surface patchwork with millions of vertices. We use the hundreds of images available in a video stream to improve the quality of a simple photometric data term, and minimise a global spatially regularised energy functional in a novel non-convex optimisation framework. Interleaved, we track the camera's 6DOF motion precisely by frame-rate whole image alignment against the entire dense model. Our algorithms are highly parallelisable throughout and DTAM achieves real-time performance using current commodity GPU hardware. We demonstrate that a dense model permits superior tracking performance under rapid motion compared to a state of the art method using features; and also show the additional usefulness of the dense model for real-time scene interaction in a physics-enhanced augmented reality application.},
author = {Newcombe, Richard A. and Lovegrove, Steven J. and Davison, Andrew J.},
booktitle = {2011 International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126513},
isbn = {978-1-4577-1102-2},
issn = {1550-5499},
keywords = {Cameras,DTAM,GPU hardware,Image reconstruction,Optimization,Real time systems,Robustness,Tracking,Vectors,augmented reality,cameras,concave programming,dense,dense model,dense tracking and mapping,energy functional,graphics processing units,hand-held RGB camera,image alignment,image motion analysis,image reconstruction,image texture,nonconvex optimisation,object tracking,photometric data term,physics-enhanced augmented reality,real-time camera reconstruction,real-time camera tracking,real-time scene interaction,textured depth map,video stream},
mendeley-tags = {DTAM,dense},
month = {nov},
pages = {2320--2327},
publisher = {IEEE},
shorttitle = {Computer Vision (ICCV), 2011 IEEE International Co},
title = {{DTAM: Dense tracking and mapping in real-time}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126513},
year = {2011}
}
@book{Triggs2000,
address = {Berlin, Heidelberg},
author = {{Bill Triggs, Philip McLauchlan, Richard Hartley}, Andrew Fitzgibbon},
doi = {10.1007/3-540-44480-7},
isbn = {978-3-540-67973-8},
keywords = {BA},
mendeley-tags = {BA},
month = {apr},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Bundle Adjustment – A Modern Synthesis}},
url = {http://link.springer.com/10.1007/3-540-44480-7},
volume = {1883},
year = {2000}
}
@article{Whelan2014,
abstract = {We present a new simultaneous localization and mapping (SLAM) system capable of producing high-quality globally consistent surface reconstructions over hundreds of meters in real time with only a low-cost commodity RGB-D sensor. By using a fused volumetric surface reconstruction we achieve a much higher quality map over what would be achieved using raw RGB-D point clouds. In this paper we highlight three key techniques associated with applying a volumetric fusion-based mapping system to the SLAM problem in real time. First, the use of a GPU-based 3D cyclical buffer trick to efficiently extend dense every-frame volumetric fusion of depth maps to function over an unbounded spatial region. Second, overcoming camera pose estimation limitations in a wide variety of environments by combining both dense geometric and photometric camera pose constraints. Third, efficiently updating the dense map according to place recognition and subsequent loop closure constraints by the use of an  as-rigid-as-possible' space deformation. We present results on a wide variety of aspects of the system and show through evaluation on de facto standard RGB-D benchmarks that our system performs strongly in terms of trajectory estimation, map quality and computational performance in comparison to other state-of-the-art systems.},
author = {Whelan, T. and Kaess, M. and Johannsson, H. and Fallon, M. and Leonard, J. J. and McDonald, J.},
doi = {10.1177/0278364914551008},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {RGB-D},
mendeley-tags = {RGB-D},
month = {dec},
number = {4-5},
pages = {598--626},
title = {{Real-time large-scale dense RGB-D SLAM with volumetric fusion}},
url = {http://ijr.sagepub.com/content/34/4-5/598.short},
volume = {34},
year = {2014}
}
@misc{McManus2014,
abstract = {This paper is about localising across extreme lighting and weather conditions. We depart from the traditional point-feature-based approach as matching under dramatic appearance changes is a brittle and hard thing. Point feature detectors are fixed and rigid procedures which pass over an image examining small, low-level structure such as corners or blobs. They apply the same criteria applied all images of all places. This paper takes a contrary view and asks what is possible if instead we learn a bespoke detector for every place. Our localisation task then turns into curating a large bank of spatially indexed detectors and we show that this yields vastly superior performance in terms of robustness in exchange for a reduced but tolerable metric precision. We present an unsupervised system that produces broad-region detectors for distinctive visual elements, called scene signatures, which can be associated across almost all appearance changes. We show, using 21km of data collected over a period of 3 months, that our system is capable of producing metric localisation estimates from night-to-day or summer-to-winter conditions.},
author = {McManus, Colin and Upcroft, Ben and Newmann, Paul},
keywords = {080106 Image Processing},
month = {jul},
title = {{Scene signatures : localised and point-less features for localisation}},
url = {http://eprints.qut.edu.au/76158/2/p23.pdf},
year = {2014}
}
@misc{Pepperell2014,
abstract = {This paper presents Sequence Matching Across Route Traversals (SMART); a generally applicable sequence-based place recognition algorithm. SMART provides invariance to changes in illumination and vehicle speed while also providing moderate pose invariance and robustness to environmental aliasing. We evaluate SMART on vehicles travelling at highly variable speeds in two challenging environments; firstly, on an all-terrain vehicle in an off-road, forest track and secondly, using a passenger car traversing an urban environment across day and night. We provide comparative results to the current state-of-the-art SeqSLAM algorithm and investigate the effects of altering SMART's image matching parameters. Additionally, we conduct an extensive study of the relationship between image sequence length and SMART's matching performance. Our results show viable place recognition performance in both environments with short 10-metre sequences, and up to 96{\%} recall at 100{\%} precision across extreme day-night cycles when longer image sequences are used.},
author = {Pepperell, Edward and Corke, Peter and Milford, Michael},
booktitle = {Proceedings of the International Conference on Robotics and Automation},
keywords = {Localisation,Place Recognition,Robot Vision},
month = {jun},
publisher = {IEEE},
title = {{All-environment visual place recognition with SMART}},
url = {http://eprints.qut.edu.au/72732/3/72732.pdf},
year = {2014}
}
@inproceedings{Liu2013a,
abstract = {We propose a method in this paper to perform sequence-based appearance SLAM in an efficient and effective way. Sequence-based SLAM (or SeqSLAM for short) makes use of the image descriptors extracted from a series of consecutive frames and matching is done between two such image sequences. It has been shown to be effective in dealing with significant illumination change where localization and mapping can be conducted under different time periods and weather conditions. To address the computational issue that can arise from the exhaustive search of the candidate sequences with the increase of map size, we use a particle filter to implement the Bayes filtering framework of estimating the true match. The resampling of the particles allows us to maintain only a small number of hypotheses while still capturing the true distribution of the robot location. Our method is highly scalable and efficient, validated on a large dataset with comparable results to the original algorithm in terms of performance.},
author = {Liu, Yang and Zhang, Hong},
booktitle = {2013 IEEE International Conference on Mechatronics and Automation},
doi = {10.1109/ICMA.2013.6618095},
isbn = {978-1-4673-5560-5},
keywords = {Bayes filtering framework,Bayes methods,Robots,SLAM (robots),SeqSLAM,consecutive frame series,exhaustive search,illumination change,image descriptors,image matching,image sequence matching,image sequences,lighting,map size,mobile robots,particle filter,particle filtering (numerical methods),robot vision,search problems,sequence-based SLAM efficiency improvement,sequence-based appearance SLAM,true match estimation,true robot location distribution,weather conditions},
mendeley-tags = {SeqSLAM},
month = {aug},
pages = {1261--1266},
publisher = {IEEE},
shorttitle = {Mechatronics and Automation (ICMA), 2013 IEEE Inte},
title = {{Towards improving the efficiency of sequence-based SLAM}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6618095},
year = {2013}
}
@inproceedings{Sunderhauf2011,
abstract = {The ability to recognize known places is an essential competence of any intelligent system that operates autonomously over longer periods of time. Approaches that rely on the visual appearance of distinct scenes have recently been developed and applied to large scale SLAM scenarios. FAB-Map is maybe the most successful of these systems. Our paper proposes BRIEF-Gist, a very simplistic appearance-based place recognition system based on the BRIEF descriptor. BRIEF-Gist is much more easy to implement and more efficient compared to recent approaches like FAB-Map. Despite its simplicity, we can show that it performs comparably well as a front-end for large scale SLAM. We benchmark our approach using two standard datasets and perform SLAM on the 66 km long urban St. Lucia dataset.},
author = {Sunderhauf, N. and Protzel, P.},
booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6048590},
isbn = {978-1-61284-456-5},
issn = {2153-0858},
keywords = {Educational institutions,Global Positioning System,Optimization,Robustness,Simultaneous localization and mapping,Visualization,Vocabulary},
language = {English},
month = {sep},
pages = {1234--1241},
publisher = {IEEE},
title = {{BRIEF-Gist - Closing the loop by simple means}},
url = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000006048590},
year = {2011}
}
@inproceedings{Badino2012,
author = {Badino, Hernan and Huber, Daniel and Kanade, Takeo},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224716},
isbn = {978-1-4673-1405-3},
month = {may},
pages = {1635--1642},
publisher = {IEEE},
title = {{Real-time topometric localization}},
url = {http://repository.cmu.edu/robotics/823},
year = {2012}
}
@article{Krose2001,
abstract = {In this paper we present a method for an appearance-based modeling of the environment of a mobile robot. We describe the task (localization of the robot) in a probabilistic framework. Linear image features are extracted using a Principal Component Analysis. The appearance model is represented as a probability density function of the image feature vector given the location of the robot. We estimate this density model from the data with a kernel estimation method. We show how the parameters of the model influence the localization performance. We also study how many features and which features are needed for good localization.},
author = {Kr{\"{o}}se, B.J.A and Vlassis, N and Bunschoten, R and Motomura, Y},
doi = {10.1016/S0262-8856(00)00086-X},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Feature extraction,PCA,Probabilistic modeling,Robot localization},
mendeley-tags = {PCA},
month = {apr},
number = {6},
pages = {381--391},
title = {{A probabilistic model for appearance-based robot localization}},
url = {http://www.sciencedirect.com/science/article/pii/S026288560000086X},
volume = {19},
year = {2001}
}
@article{Henry2012,
abstract = {RGB-D cameras (such as the Microsoft Kinect) are novel sensing systems that capture RGB images along with per-pixel depth information. In this paper we investigate how such cameras can be used for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. We present RGB-D Mapping, a full 3D mapping system that utilizes a novel joint optimization algorithm combining visual features and shape-based alignment. Visual and depth information are also combined for view-based loop-closure detection, followed by pose optimization to achieve globally consistent maps. We evaluate RGB-D Mapping on two large indoor environments, and show that it effectively combines the visual and shape information available from RGB-D cameras.},
author = {Henry, P. and Krainin, M. and Herbst, E. and Ren, X. and Fox, D.},
doi = {10.1177/0278364911434148},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {3D,RGB-D},
mendeley-tags = {3D,RGB-D},
month = {feb},
number = {5},
pages = {647--663},
title = {{RGB-D mapping: Using Kinect-style depth cameras for dense 3D modeling of indoor environments}},
url = {http://ijr.sagepub.com/content/31/5/647.short},
volume = {31},
year = {2012}
}
@inproceedings{Paul2010,
abstract = {This paper describes a probabilistic framework for appearance based navigation and mapping using spatial and visual appearance data. Like much recent work on appearance based navigation we adopt a bag-of-words approach in which positive or negative observations of visual words in a scene are used to discriminate between already visited and new places. In this paper we add an important extra dimension to the approach. We explicitly model the spatial distribution of visual words as a random graph in which nodes are visual words and edges are distributions over distances. Care is taken to ensure that the spatial model is able to capture the multi-modal distributions of inter-word spacing and account for sensor errors both in word detection and distances. Crucially, these inter-word distances are viewpoint invariant and collectively constitute strong place signatures and hence the impact of using both spatial and visual appearance is marked. We provide results illustrating a tremendous increase in precision-recall area compared to a state-of-the-art visual appearance only systems.},
author = {Paul, Rohan and Newman, Paul},
booktitle = {2010 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2010.5509587},
isbn = {978-1-4244-5038-1},
issn = {1050-4729},
keywords = {BoW,Computational modeling,FAB-MAP 3D,Feature extraction,Histograms,Kernel,LASER,Robot sensing systems,Three dimensional displays,Visualization,appearance based navigation,bag of words approach,interword spacing,mobile robots,multimodal distributions,probability,random graph,robot vision,sensor errors,spatial distribution,topological mapping,topology,visual words,word detection},
language = {English},
mendeley-tags = {BoW,LASER},
month = {may},
pages = {2649--2656},
publisher = {IEEE},
title = {{FAB-MAP 3D: Topological mapping with spatial and visual appearance}},
url = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000005509587},
year = {2010}
}
@inproceedings{Sivic2003,
abstract = {We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.},
annote = {Visual BoW},
author = {Sivic, J. and Zisserman, A.},
booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2003.1238663},
isbn = {0-7695-1950-4},
keywords = {Image databases,Image representations,Machine vision,Video Google,computer vision,content-based retrieval,document rankings,image matching,image representation,invariant region descriptors,inverted file systems,noise reduction,object matching,object retrieval,partial occlusion,scene retrieval,text retrieval approach,unstable regions,vector quantization,video signal processing,visual databases},
language = {English},
pages = {1470--1477 vol.2},
publisher = {IEEE},
title = {{Video Google: a text retrieval approach to object matching in videos}},
url = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000001238663},
year = {2003}
}
@article{CADENA,
abstract = {We propose a place recognition algorithm for simultaneous localization and mapping (SLAM) systems using stereo cameras that considers both appearance and geometric information of points of interest in the images. Both near and far scene points provide information for the recognition process. Hypotheses about loop closings are generated using a fast appearance-only technique based on the bag-of-words (BoW) method. We propose several important improvements to BoWs that profit from the fact that, in this problem, images are provided in sequence. Loop closing candidates are evaluated using a novel normalized similarity score that measures similarity in the context of recent images in the sequence. In cases where similarity is not sufficiently clear, loop closing verification is carried out using a method based on conditional random fields (CRFs). We build on CRF matching with two main novelties: We use both image and 3-D geometric information, and we carry out inference on a minimum spanning tree (MST), instead of a densely connected graph. Our results show that MSTs provide an adequate representation of the problem, with the additional advantages that exact inference is possible and that the computational cost of the inference process is limited. We compare our system with the state of the art using visual indoor and outdoor data from three different locations and show that our system can attain at least full precision (no false positives) for a higher recall (fewer false negatives).},
annote = {BoW + CRF + MST + stereo},
author = {CADENA, C{\'{e}}sar and GALVEZ-LOPEZ, Dorian and TARDOS, Juan D. and NEIRA, Jos{\'{e}}},
issn = {1552-3098},
journal = {IEEE transactions on robotics},
keywords = {Arbol MST,Arbre recouvrant minimal,Bag of words,Bag of words (BoW),Bolsa de palabras,B{\'{u}}squeda informaci{\'{o}}n,Cartograf{\'{i}}a,Cartographie,Cartography,Computer vision,Conditional probability,Connected graph,Grafo conexo,Graph method,Graphe connexe,Image processing,Image sequence,Indoor installation,Inference,Inferencia,Information retrieval,Inf{\'{e}}rence,Instalaci{\'{o}}n exterior,Instalaci{\'{o}}n interior,Installation ext{\'{e}}rieure,Installation int{\'{e}}rieure,Interest region,Localisation,Localizaci{\'{o}}n,Localization,Metric,Minimum spanning tree MST,Moving robot,M{\'{e}}thode graphe,M{\'{e}}todo grafo,M{\'{e}}trico,M{\'{e}}trique,Outdoor installation,Probabilidad condicional,Probabilit{\'{e}} conditionnelle,Procesamiento imagen,Recherche information,Regi{\'{o}}n inter{\`{e}}s,Robot mobile,Robot m{\'{o}}vil,Robotics,Robotique,Rob{\'{o}}tica,R{\'{e}}gion int{\'{e}}r{\^{e}}t,Sac de mots,Secuencia imagen,Similarity,Similitud,Similitude,Stereopsis,S{\'{e}}quence image,Traitement image,Vision ordinateur,Vision st{\'{e}}r{\'{e}}oscopique,Visi{\'{o}}n estereosc{\'{o}}pica,Visi{\'{o}}n ordenador,computer vision,conditional random fields (CRFs),recognition,simultaneous localization and mapping (SLAM)},
language = {eng},
number = {4},
pages = {871--885},
publisher = {Institute of Electrical and Electronics Engineers},
title = {{Robust Place Recognition With Stereo Sequences}},
url = {http://cat.inist.fr/?aModele=afficheN{\&}cpsidt=28268381},
volume = {28},
year = {2012}
}
@inproceedings{Angeli2008,
abstract = {In robotic applications of visual simultaneous localization and mapping, loop-closure detection and global localization are two issues that require the capacity to recognize a previously visited place from current camera measurements. We present an online method that makes it possible to detect when an image comes from an already perceived scene using local shape information. Our approach extends the bag of visual words method used in image recognition to incremental conditions and relies on Bayesian filtering to estimate loop- closure probability. We demonstrate the efficiency of our solu- tion by real-time loop-closure detection under strong perceptual aliasing conditions in an indoor image sequence taken with a handheld camera.},
author = {Angeli, Adrien and Doncieux, Stephane and Meyer, Jean-Arcady and Filliat, David},
booktitle = {2008 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2008.4543475},
isbn = {978-1-4244-1646-2},
keywords = {navigation,robotique,vision},
language = {en},
month = {may},
pages = {1842--1847},
publisher = {IEEE},
title = {{Real-time visual loop-closure detection}},
url = {https://hal-ensta.archives-ouvertes.fr/hal-00647371/},
year = {2008}
}
@inproceedings{Mur-Artal2014,
abstract = {In this paper we present for the first time a relocalisation method for keyframe-based SLAM that can deal with severe viewpoint change, at frame-rate, in maps containing thousands of keyframes. As this method relies on local features, it permits the interoperability between cameras, allowing a camera to relocalise in a map built by a different camera. We also perform loop closing (detection {\&}{\#}x002B; correction), at keyframerate, in loops containing hundreds of keyframes. For both relocalisation and loop closing, we propose a bag of words place recognizer with ORB features, which is able to recognize places spending less than 39 ms, including feature extraction, in databases containing 10K images (without geometrical verification). We evaluate the performance of this recognizer in four different datasets, achieving high recall and no false matches, and getting better results than the state-of-art in place recognition, being one order of magnitude faster.},
annote = {BoW with ORB},
author = {Mur-Artal, Raul and Tardos, Juan D.},
booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2014.6906953},
isbn = {978-1-4799-3685-4},
keywords = {Cameras,Databases,Feature extraction,Image recognition,Simultaneous localization and mapping,Three-dimensional displays,Vocabulary},
language = {English},
month = {may},
pages = {846--853},
publisher = {IEEE},
title = {{Fast relocalisation and loop closing in keyframe-based SLAM}},
url = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000006906953},
year = {2014}
}
@inproceedings{Rublee2011,
author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
booktitle = {2011 International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126544},
isbn = {978-1-4577-1102-2},
month = {nov},
pages = {2564--2571},
publisher = {IEEE},
title = {{ORB: An efficient alternative to SIFT or SURF}},
url = {http://dl.acm.org/citation.cfm?id=2355573.2356268},
year = {2011}
}
@inproceedings{Calonder2010,
annote = {BRIEF},
author = {Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
booktitle = {European Conference on Computer Vision (ECCV)},
isbn = {3-642-15560-X, 978-3-642-15560-4},
month = {sep},
pages = {778--792},
publisher = {Springer-Verlag},
title = {{BRIEF: binary robust independent elementary features}},
url = {http://dl.acm.org/citation.cfm?id=1888089.1888148},
year = {2010}
}
@article{Bay2008,
abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision.},
annote = {SURF},
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition},
month = {jun},
number = {3},
pages = {346--359},
title = {{Speeded-Up Robust Features (SURF)}},
url = {http://www.sciencedirect.com/science/article/pii/S1077314207001555},
volume = {110},
year = {2008}
}
@article{Lowe2004,
annote = {SIFT},
author = {Lowe, David G.},
doi = {10.1023/B:VISI.0000029664.99615.94},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {nov},
number = {2},
pages = {91--110},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
volume = {60},
year = {2004}
}
@inproceedings{Nister2006,
abstract = {A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD{\&}{\#}146;s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.},
annote = {Visual BoW},
author = {Nister, D. and Stewenius, H.},
booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR'06)},
doi = {10.1109/CVPR.2006.264},
isbn = {0-7695-2597-0},
issn = {1063-6919},
keywords = {Computer vision,Frequency,Image databases,Image recognition,Indexing,Quantization,Robustness,Spatial databases,Visualization,Vocabulary},
pages = {2161--2168},
publisher = {IEEE},
shorttitle = {Computer Vision and Pattern Recognition, 2006 IEEE},
title = {{Scalable Recognition with a Vocabulary Tree}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1641018},
volume = {2},
year = {2006}
}
@article{Galvez-Lopez2012,
abstract = {We propose a novel method for visual place recognition using bag of words obtained from accelerated segment test (FAST)+BRIEF features. For the first time, we build a vocabulary tree that discretizes a binary descriptor space and use the tree to speed up correspondences for geometrical verification. We present competitive results with no false positives in very different datasets, using exactly the same vocabulary and settings. The whole technique, including feature extraction, requires 22 ms/frame in a sequence with 26 300 images that is one order of magnitude faster than previous approaches.},
author = {Galvez-López, D. and Tardos, J. D.},
doi = {10.1109/TRO.2012.2197158},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
keywords = {Bag of binary words,Cameras,FAST±BRIEF features,Feature extraction,Indexes,Robots,SLAM (robots),Vectors,Vocabulary,accelerated segment test,bags of binary words,binary descriptor space,computer vision,fast place recognition,feature extraction,geometrical verification,geometry,image sequences,mobile robots,object recognition,place recognition,simultaneous localization and mapping,simultaneous localization and mapping (SLAM),trees (mathematics),visual place recognition,vocabulary tree},
month = {oct},
number = {5},
pages = {1188--1197},
shorttitle = {Robotics, IEEE Transactions on},
title = {{Bags of Binary Words for Fast Place Recognition in Image Sequences}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6202705},
volume = {28},
year = {2012}
}
@article{Cummins2010,
abstract = {We describe a new formulation of appearance-only SLAM suitable for very large scale place recognition. The system navigates in the space of appearance,   assigning each new observation to either a new or a previously visited location, without reference to metric position. The system is demonstrated performing   reliable online appearance mapping and loop-closure detection over a 1000 km trajectory, with mean filter update times of 14 ms. The scalability of the   system is achieved by defining a sparse approximation to the FAB-MAP model suitable for implementation using an inverted index. Our formulation of the   problem is fully probabilistic and naturally incorporates robustness against perceptual aliasing. We also demonstrate that the approach substantially   outperforms the standard term-frequency inverse-document-frequency (tf-idf) ranking measure. The 1000 km data set comprising almost a terabyte of omni-directional and stereo imagery is available for use, and we hope that it  will serve as a benchmark for future systems.},
author = {Cummins, M. and Newman, P.},
doi = {10.1177/0278364910385483},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
month = {nov},
number = {9},
pages = {1100--1123},
title = {{Appearance-only SLAM at large scale with FAB-MAP 2.0}},
url = {http://ijr.sagepub.com/content/30/9/1100.short},
volume = {30},
year = {2010}
}
@article{Mur-Artal2015,
abstract = {This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
archivePrefix = {arXiv},
arxivId = {1502.00956},
author = {Mur-Artal, Raul and Montiel, J. M. M. and Tardos, Juan D.},
doi = {10.1109/TRO.2015.2463671},
eprint = {1502.00956},
file = {::},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
month = {feb},
pages = {1--17},
title = {{ORB-SLAM: A Versatile and Accurate Monocular SLAM System}},
url = {http://arxiv.org/abs/1502.00956},
year = {2015}
}
@article{Lui2012,
abstract = {In this paper, we present a feasible solution to the problem of autonomous navigation in initially unknown environments using a pure vision-based approach. The mobile robot performs range sensing with a unique omnidirectional stereovision system, estimates its motion using visual odometry and detects loop closures via a place recognition system as it performs topological map building and localization concurrently. Owing to the importance of performing loop closing regularly, the mobile robot is equipped with an active loop closure detection and validation system that assists it to return to target loop closing locations, validates ambiguous loop closures and provides it with the ability to overturn the decision of an incorrectly committed loop closure. A refined incremental probabilistic framework for an appearance-based place recognition system is fully described and the final system is validated in multiple experiments conducted in indoor, semioutdoor and outdoor environments. Lastly, the performance of the probabilistic framework is compared with the rank-based framework with additional experiments conducted in the semi-autonomous mode, where the mobile robot, provided with a priori information in the form of a topological map that is built in a separate occasion in an offline manner, is required to reach its target destination.},
annote = {Looks like a full SLAM system with PR and VO.},
author = {Lui, W. L. D. and Jarvis, R.},
doi = {10.1177/0278364911435160},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {VO,stereo},
mendeley-tags = {VO,stereo},
month = {feb},
number = {4},
pages = {403--428},
title = {{A pure vision-based topological SLAM system}},
url = {http://ijr.sagepub.com/content/early/2012/02/27/0278364911435160.abstract},
volume = {31},
year = {2012}
}
@inproceedings{Milford2014,
author = {Milford, Michael and Firn, Jennifer and Beattie, James and Jacobson, Adam and Pepperell, Edward and Mason, Eugene and Kimlin, Michael and Dunbabin, Matthew},
booktitle = {Australasian Conference on Robotics and Automation},
pages = {1--10},
title = {{Automated sensory data alignment for environmental and epidermal change monitoring}},
year = {2014}
}
@article{Milford2013,
abstract = {In this paper we use the algorithm SeqSLAM to address the question, how little and what quality of visual information is needed to localize along a familiar route? We conduct a comprehensive investigation of place recognition performance on seven datasets while varying image resolution (primarily 1 to 512 pixel images), pixel bit depth, field of view, motion blur, image compression and matching sequence length. Results confirm that place recognition using single images or short image sequences is poor, but improves to match or exceed current benchmarks as the matching sequence length increases. We then present place recognition results from two experiments where low-quality imagery is directly caused by sensor limitations; in one, place recognition is achieved along an unlit mountain road by using noisy, long-exposure blurred images, and in the other, two single pixel light sensors are used to localize in an indoor environment. We also show failure modes caused by pose variance and sequence aliasing, and discuss ways in which they may be overcome. By showing how place recognition along a route is feasible even with severely degraded image sequences, we hope to provoke a re-examination of how we develop and test future localization and mapping systems.},
author = {Milford, Michael},
doi = {10.1177/0278364913490323},
isbn = {0278364913490},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {low resolution,place recognition,seqslam,vision,vision-based place recognition},
number = {7},
pages = {766--789},
title = {{Vision-based place recognition: how low can you go?}},
url = {http://ijr.sagepub.com/content/32/7/766.abstract{\%}5Cnhttp://ijr.sagepub.com/cgi/doi/10.1177/0278364913490323},
volume = {32},
year = {2013}
}
@article{Naseer2014,
author = {Naseer, Tayyab and Spinello, Luciano and Stachniss, Cyrill},
journal = {Proceedings of the National Conference on Artificial Intelligence (AAAI)},
title = {{Robust Visual Robot Localization Across Seasons using Network Flows}},
year = {2014}
}
@article{Fuentes-Pacheco2012,
abstract = {Visual SLAM (simultaneous localization and mapping) refers to the problem of using images, as the only source of external information, in order to establish the position of a robot, a vehicle, or a moving camera in an environment, and at the same time, construct a representation of the explored zone. SLAM is an essential task for the autonomy of a robot. Nowadays, the problem of SLAM is considered solved when range sensors such as lasers or sonar are used to built 2D maps of small static environments. However SLAM for dynamic, complex and large scale environments, using vision as the sole external sensor, is an active area of research. The computer vision techniques employed in visual SLAM, such as detection, description and matching of salient features, image recognition and retrieval, among others, are still susceptible of improvement. The objective of this article is to provide new researchers in the field of visual SLAM a brief and comprehensible review of the state-of-the-art. {\textcopyright} 2012 Springer Science+Business Media Dordrecht.},
author = {Fuentes-Pacheco, Jorge and Ruiz-Ascencio, Jos{\'{e}} and Rend{\'{o}}n-Mancha, Juan Manuel},
doi = {10.1007/s10462-012-9365-8},
isbn = {02692821 (ISSN)},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Data association,Image matching,Salient feature selection,Topological and metric maps,Visual SLAM},
pages = {1--27},
title = {{Visual simultaneous localization and mapping: a survey}},
year = {2012}
}
@article{Davison2007,
abstract = {We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the "pure vision" domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to Structure from Motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware. This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas. We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera.},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Davison, Andrew J. and Reid, Ian D. and Molton, Nicholas D. and Stasse, Olivier},
doi = {10.1109/TPAMI.2007.1049},
eprint = {there is not},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D/stereo scene analysis,Autonomous vehicles,Tracking},
number = {6},
pages = {1052--1067},
pmid = {17431302},
title = {{MonoSLAM: Real-time single camera SLAM}},
volume = {29},
year = {2007}
}
@article{Sunderhauf2012a,
author = {S{\"{u}}nderhauf, N and Protzel, P},
doi = {10.1109/ICRA.2012.6224709},
isbn = {9781467314039},
issn = {10504729},
journal = {Robotics and Automation (ICRA),  {\ldots}},
title = {{Towards a robust back-end for pose graph slam}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6224709},
year = {2012}
}
@article{Liu2013,
author = {Liu, Yang and Zhang, Hong and Tion, I I N Troduc},
isbn = {9781467355605},
pages = {1261--1266},
title = {{Towards Improving the Efficiency of Sequence-Based SLAM*}},
year = {2013}
}
@article{Milford2015,
archivePrefix = {arXiv},
arxivId = {1505.04548},
author = {Milford, Michael and Kim, Hanme and Mangan, Michael and Stone, Tom and Leutenegger, Stefan and Davison, Andrew},
eprint = {1505.04548},
journal = {ICRA Workshop},
pages = {3--10},
title = {{Place Recognition with Event-based Cameras and a Neural Implementation of SeqSLAM}},
year = {2015}
}
@article{Hansen2014,
abstract = {Visual place recognition and loop closure is critical for the global accuracy of visual Simultaneous Localization and Mapping (SLAM) systems. We present a place recognition algorithm which operates by matching local query image sequences to a database of image sequences. To match sequences, we calculate a matrix of low-resolution, contrast-enhanced image similarity probability values. The optimal sequence alignment, which can be viewed as a discontinuous path through the matrix, is found using a Hidden Markov Model (HMM) framework reminiscent of Dynamic Time Warping from speech recognition. The state transitions enforce local velocity constraints and the most likely path sequence is recovered efficiently using the Viterbi algorithm. A rank reduction on the similarity probability matrix is used to provide additional robustness in challenging conditions when scoring sequence matches. We evaluate our approach on seven outdoor vision datasets and show improved precision-recall performance against the recently published seqSLAM algorithm.},
author = {Hansen, Peter and Browning, Brett},
doi = {10.1109/IROS.2014.6943207},
isbn = {978-1-4799-6934-0},
journal = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
keywords = {Databases,HMM sequence matching,Hidden Markov Model,Hidden Markov models,Image recognition,Image sequences,SLAM systems,Simultaneous localization and mapping,Visualization,Viterbi algorithm,dynamic time warping,hidden Markov models,image matching,image sequence database,image sequences,image similarity probability values,matrix calculation,optimal sequence alignment,path sequence,place recognition algorithm,query image sequences,simultaneous localization and mapping,speech recognition,visual databases,visual place recognition},
pages = {4549--4555},
title = {{Visual place recognition using HMM sequence matching}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6943207},
year = {2014}
}
@article{Glover2015,
author = {Glover, Arren and Pepperell, Edward and Wyeth, Gordon and Upcroft, Ben and Milford, Michael},
journal = {Proceedings of the Australasian Conference on Robotics and Automation (ACRA)},
title = {{Repeatable Condition-Invariant Visual Odometry for Sequence-Based Place Recognition}},
year = {2015}
}
@article{Meilland2011,
author = {Meilland, Maxime and Comport, Andrew and Rives, Patrick},
doi = {10.5244/C.25.45},
isbn = {1-901725-43-X},
journal = {Proceedings of the British Machine Vision Conference},
number = {JANUARY 2011},
pages = {45.1----45.11},
title = {{Real-time Dense Visual Tracking under Large Lighting Variations}},
url = {http://www.bmva.org/bmvc/2011/proceedings/paper45/abstract.pdf{\%}5Cnhttp://www.bmva.org/bmvc/2011/proceedings/paper45/},
year = {2011}
}
@article{Comport2010,
abstract = {In this paper we describe a new image-based approach to tracking the six-degree-of-freedom trajectory of a stereo camera pair. The proposed technique estimates the pose and subsequently the dense pixel matching between temporal image pairs in a sequence by performing dense spatial matching between images of a stereo reference pair. In this way a minimization approach is employed which directly uses all grayscale information available within the stereo pair (or stereo region) leading to very robust and precise results. Metric 3D structure constraints are imposed by consistently warping corresponding stereo images to generate novel viewpoints at each stereo acquisition. An iterative non-linear trajectory estimation approach is formulated based on a quadrifocal relationship between the image intensities within adjacent views of the stereo pair. A robust M-estimation technique is used to reject outliers corresponding to moving objects within the scene or other outliers such as occlusions and illumination changes. The technique is applied to recovering the trajectory of a moving vehicle in long and difficult sequences of images.},
author = {Comport, A.I. and Malis, Ezio and Rives, Patrick},
doi = {10.1177/0278364909356601},
isbn = {0278364909356},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
number = {2-3},
pages = {245--266},
title = {{Real-time Quadrifocal Visual Odometry}},
volume = {29},
year = {2010}
}
@article{Whelan2013,
author = {Whelan, Thomas and Johannsson, Hordur and Kaess, Michael and Leonard, John J. and Mcdonald, John},
doi = {10.1109/ICRA.2013.6631400},
isbn = {978-1-4673-5643-5},
journal = {IEEE International Conference on Robotics and Automation},
keywords = {dense,rgbd,vo},
mendeley-tags = {dense,rgbd,vo},
number = {i},
pages = {5724--5731},
title = {{Robust Real-Time Visual Odometry for Dense RGB-D Mapping}},
year = {2013}
}
@article{Pepperell2015,
author = {Pepperell, Edward},
title = {{Visual Sequence-Based Place Recognition for Changing Conditions and Varied Viewpoints}},
year = {2015}
}
@article{Scaramuzza2011,
abstract = {Visual odometry (VO) is the process of estimating the egomotion of an agent (e.g., vehicle, human, and robot) using only the input of a single or If multiple cameras attached to it. Application domains include robotics, wearable computing, augmented reality, and automotive. The term VO was coined in 2004 by Nister in his landmark paper. The term was chosen for its similarity to wheel odometry, which incrementally estimates the motion of a vehicle by integrating the number of turns of its wheels over time. Likewise, VO operates by incrementally estimating the pose of the vehicle through examination of the changes that motion induces on the images of its onboard cameras. For VO to work effectively, there should be sufficient illumination in the environment and a static scene with enough texture to allow apparent motion to be extracted. Furthermore, consecutive frames should be captured by ensuring that they have sufficient scene overlap.},
author = {Scaramuzza, Davide and Fraundorfer, Friedrich},
doi = {10.1109/MRA.2011.943233},
isbn = {1070-9932},
issn = {1070-9932},
journal = {IEEE Robotics {\&} Automation Magazine},
number = {4},
pages = {80--92},
pmid = {6153423},
title = {{Visual Odometry Part II}},
volume = {18},
year = {2011}
}
@article{Tykkala2011,
abstract = {In RGB-D sensor based visual odometry the goal is to estimate a sequence of camera movements using image and/or range measurements. Direct methods solve the problem by minimizing intensity error. In this work a depth map obtained from a RGB-D sensor is considered as a new measurement which is combined with a direct photometric cost function. The minimization of the bi-objective cost function produces 3D camera motion parameters which registers two 3D surfaces within a same coordinate system. The given formulation does not require any predetermined temporal correspondencies nor feature extraction when having a sufficient frame rate. It is shown how incorporating the depth measurement robustifies the cost function in case of insufficient texture information and non-Lambertian surfaces. Finally the method is demonstrated in the Planetary Robotics Vision Ground Processing (PRoVisG) competition where visual odometry and 3D reconstruction results are solved for a stereo image sequence captured using a Mars rover.},
author = {Tykk{\"{a}}l{\"{a}}, Tommi and Audras, C{\'{e}}dric and Comport, Andrew I.},
doi = {10.1109/ICCVW.2011.6130500},
isbn = {9781467300629},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2050--2056},
title = {{Direct iterative closest point for real-time visual odometry}},
year = {2011}
}
@article{Comport2007,
abstract = {This paper describes a new image-based approach to tracking the 6DOF trajectory of a stereo camera pair using a corresponding reference image pairs instead of explicit 3D feature reconstruction of the scene. A dense minimisation approach is employed which directly uses all grey-scale information available within the stereo pair (or stereo region) leading to very robust and precise results. Metric 3D structure constraints are imposed by consistently warping corresponding stereo images to generate novel viewpoints at each stereo acquisition. An iterative non-linear trajectory estimation approach is formulated based on a quadrifocal relationship between the image intensities within adjacent views of the stereo pair. A robust M-estimation technique is used to reject outliers corresponding to moving objects within the scene or other outliers such as occlusions and illumination changes. The technique is applied to recovering the trajectory of a moving vehicle in long and difficult sequences of images.},
author = {Comport, a. I. and Malis, E. and Rives, P.},
doi = {10.1109/ROBOT.2007.363762},
isbn = {1424406021},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {40--45},
title = {{Accurate quadrifocal tracking for robust 3D visual odometry}},
year = {2007}
}
@article{Konolige2007,
abstract = {Motion estimation from imagery, sometimes called visual odometry, is a well-known process. However, it is difficult to achieve good performance using standard techniques. In this paper, we present the results of several years of work on an integrated system to localize a mobile robot in rough outdoor terrain using visual odometry, with an increasing degree of precision. We discuss issues that are important for realtime, high-precision performance: choice of features, matching strategies, incremental bundle adjustment, and filtering with inertial measurement sensors. Using data with ground truth from an RTK GPS system, we show experimentally that our algorithms can track motion, in off-road terrain, over distances of 10 km, with an error of less than 10 m (0.1{\%}).},
author = {Konolige, Kurt and Agrawal, Motilal and Sola, J},
doi = {10.1007/978-3-642-14743-2_18},
isbn = {9783642147425},
issn = {16107438},
journal = {Proc International Symposium on Robotics Research},
keywords = {IMU,VO},
mendeley-tags = {IMU,VO},
pages = {1150--1157},
title = {{Large scale visual odometry for rough terrain}},
volume = {2},
year = {2007}
}
@article{Niko2013,
author = {Niko, S and Protzel, Peter},
isbn = {9781467356428},
pages = {5178--5183},
title = {{Switchable Constraints vs . Max-Mixture Models vs . RRR – A Comparison of Three Approaches to Robust Pose Graph SLAM}},
year = {2013}
}
@article{Gui2015,
author = {Gui, Jianjun and Gu, Dongbing and Hu, Huosheng},
doi = {10.1109/ICMA.2015.7237603},
isbn = {9781479970988},
title = {{Robust Direct Visual Inertial Odometry via Entropy-based Relative Pose Estimation}},
year = {2015}
}
@article{Forster2014,
abstract = {We propose a semi-direct monocular visual odometry algorithm that is precise, robust, and faster than current state-of-the-art methods. The semi-direct approach eliminates the need of costly feature extraction and robust matching techniques for motion estimation. Our algorithm operates directly on pixel intensities, which results in subpixel precision at high frame-rates. A probabilistic mapping method that explicitly models outlier measurements is used to estimate 3D points, which results in fewer outliers and more reliable points. Precise and high frame-rate motion estimation brings increased robustness in scenes of little, repetitive, and high-frequency texture. The algorithm is applied to micro-aerial-vehicle state- estimation in GPS-denied environments and runs at 55 frames per second on the onboard embedded computer and at more than 300 frames per second on a consumer laptop. We call our approach SVO (Semi-direct Visual Odometry) and release our implementation as open-source software.},
author = {Forster, Christian and Pizzoli, Matia and Scaramuzza, Davide},
doi = {10.1109/ICRA.2014.6906584},
isbn = {9781479936847},
issn = {10504729},
journal = {IEEE International Conference on Robotics and Automation},
keywords = {VO,mono},
mendeley-tags = {VO,mono},
pmid = {6576973927449638915},
title = {{SVO : Fast Semi-Direct Monocular Visual Odometry}},
year = {2014}
}
@article{Lui2015,
author = {Lui, Vincent and Drummond, Tom},
doi = {10.1109/ICRA.2015.7140011},
isbn = {9781479969234},
journal = {IEEE International Conference on Robotics and Automation (ICRA)},
pages = {5799--5806},
title = {{Image Based Optimisation without Global Consistency for Constant Time Monocular Visual SLAM}},
year = {2015}
}
@article{Niko2015,
abstract = {Place recognition has long been an incompletely solved problem in that all approaches involve significant com- promises. Current methods address many but never all of the critical challenges of place recognition – viewpoint-invariance, condition-invariance and minimizing training requirements. Here we present an approach that adapts state-of-the-art object proposal techniques to identify potential landmarks within an image for place recognition. We use the astonishing power of convolutional neural network features to identify matching landmark proposals between images to perform place recognition over extreme appearance and viewpoint variations. Our system does not require any form of training, all components are generic enough to be used off-the-shelf.We present a range of challenging experiments in varied viewpoint and environmental conditions. We demonstrate superior performance to current state-of-the- art techniques. Furthermore, by building on existing and widely used recognition frameworks, this approach provides a highly compatible place recognition system with the potential for easy integration of other techniques such as object detection and semantic scene interpretation.},
author = {Niko, S and Shirazi, Sareh and Jacobson, Adam and Dayoub, Feras and Pepperell, Edward and Upcroft, Ben and Milford, Michael},
journal = {Robotics Science and Systems},
title = {{Place Recognition with ConvNet Landmarks: Viewpoint-Robust, Condition-Robust, Training-Free}},
year = {2015}
}
@article{Milford2015a,
author = {Milford, Michael and Lowry, Stephanie and Shirazi, Sareh and Pepperell, Edward and Shen, Chunhua and Lin, Guosheng and Liu, Fayao and Cadena, Cesar and Reid, Ian},
isbn = {9781467367592},
pages = {18--25},
title = {{Sequence Searching with Deep-learnt Depth for Condition- and Viewpoint- invariant Route-based Place Recognition}},
year = {2015}
}
@article{Grasa2014,
abstract = {Simultaneous Localisation And Mapping (SLAM) methods provide real-time estimation of 3D models from the sole input of a hand-held camera, routinely in mobile robotics scenarios. Medical endoscopic sequences mimic a robotic scenario in which a hand-held camera (monocular endoscope) moves along an unknown trajectory while observing an unknown cavity. However, the feasibility and accuracy of SLAM methods have not been extensively validated with human in-vivo image sequences. In this work, we propose a monocular visual SLAM algorithm tailored to deal with medical image sequences in order to provide an up-to-scale 3D map of the observed cavity and the endoscope trajectory at frame rate. The algorithm is validated over synthetic data and human in-vivo sequences corresponding to fifteen laparoscopic hernioplasties where accurate ground-truth distances are available. It can be concluded that the proposed procedure is: 1) non invasive, because only a standard monocular endoscope and a surgical tool are used; 2) convenient, because only a handcontrolled exploratory motion is needed; 3) fast, because the algorithm provides the 3D map and the trajectory in real time; 4) accurate, because it has been validated with respect to groundtruth; and 5) robust to inter-patient variability, because it has performed successfully over the validation sequences.},
author = {Grasa, Oscar G. and Bernal, Ernesto and Casado, Santiago and Gil, Ismael and Montiel, J. M M},
doi = {10.1109/TMI.2013.2282997},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Abdomen,computer vision,endoscopy,medical robotics,simultaneous localization and mapping (SLAM),virtual/augmented reality},
number = {1},
pages = {135--146},
pmid = {24107925},
title = {{Visual slam for handheld monocular endoscope}},
volume = {33},
year = {2014}
}
@article{Maddern2012,
abstract = {The challenge of persistent appearance-based navigation and mapping is to develop an autonomous robotic vision system that can simultaneously localize, map and navigate over the lifetime of the robot. However, the computation time and memory requirements of current appearance-based methods typically scale not only with the size of the environment but also with the operation time of the platform; also, repeated revisits to locations will develop multiple competing representations which reduce recall performance. In this paper we present a solution to the persistent localization, mapping and global path planning problem in the context of a delivery robot in an office environment over a one-week period. Using a graphical appearance-based SLAM algorithm, CAT-Graph, we demonstrate constant time and memory loop closure detection with minimal degradation during repeated revisits to locations, along with topological path planning that improves over time without using a global metric representation. We compare the localization performance of CAT-Graph to openFABMAP, an appearance-only SLAM algorithm, and the path planning performance to occupancy-grid based metric SLAM. We discuss the limitations of the algorithm with regard to environment change over time and illustrate how the topological graph representation can be coupled with local movement behaviors for persistent autonomous robot navigation.},
author = {Maddern, Will and Milford, Michael and Wyeth, Gordon},
doi = {10.1109/IROS.2012.6386186},
isbn = {9781467317375},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {4224--4230},
title = {{Towards persistent indoor appearance-based localization, mapping and navigation using CAT-Graph}},
year = {2012}
}
@article{Glover2012,
abstract = {Appearance-based loop closure techniques, which leverage the high information content of visual images and can be used independently of pose, are now widely used in robotic applications. The current state-of-the-art in the field is Fast Appearance-Based Mapping (FAB-MAP) having been demonstrated in several seminal robotic mapping experiments. In this paper, we describe OpenFABMAP, a fully open source implementation of the original FAB-MAP algorithm. Beyond the benefits of full user access to the source code, OpenFABMAP provides a number of configurable options including rapid codebook training and interest point feature tuning. We demonstrate the performance of OpenFABMAP on a number of published datasets and demonstrate the advantages of quick algorithm customisation. We present results from OpenFABMAP's application in a highly varied range of robotics research scenarios.},
author = {Glover, Arren and Maddern, William and Warren, Michael and Reid, Stephanie and Milford, Michael and Wyeth, Gordon},
doi = {10.1109/ICRA.2012.6224843},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4730--4735},
title = {{OpenFABMAP: An open source toolbox for appearance-based loop closure detection}},
year = {2012}
}
@article{Milford2008,
abstract = {This paper describes a biologically inspired approach to vision-only simultaneous localization and mapping (SLAM) on ground-based platforms. The core SLAM system, dubbed RatSLAM, is based on computational models of the rodent hip- pocampus, and is coupled with a lightweight vision system that provides odometry and appearance information. RatSLAM builds a map in an online manner, driving loop closure and relocaliza- tion through sequences of familiar visual scenes. Visual ambiguity is managed by maintaining multiple competing vehicle pose esti- mates,while cumulative errors in odometry are correctedafter loop closure by a map correction algorithm.We demonstrate the map- ping performance of the system on a 66 km car journey through a complex suburban road network. Using only a web camera oper- ating at 10 Hz, RatSLAM generates a coherent map of the entire environment at real-time speed, correctly closing more than 51 loops of up to 5 km in length.},
author = {Milford, Michael J and Wyeth, Gordon F},
doi = {10.1109/TRO.2008.2004520},
isbn = {1552-3098},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
keywords = {Bio},
mendeley-tags = {Bio},
number = {5},
pages = {1038--1053},
title = {{Mapping a Suburb With a Single Camera Using a Biologically Inspired SLAM System}},
volume = {24},
year = {2008}
}
@article{Lowry2014,
author = {Lowry, Stephanie and S{\"{u}}nderhauf, Niko and Newman, Paul and Leonard, John J and Cox, David and Corke, Peter and Milford, Michael J},
journal = {TRO'15 (Under Review)},
title = {{Visual Place Recognition : A Survey (PRIVATE)}},
year = {2014}
}
@article{Milford2012,
abstract = {Learning and then recognizing a route, whether travelled during the day or at night, in clear or inclement weather, and in summer or winter is a challenging task for state of the art algorithms in computer vision and robotics. In this paper, we present a new approach to visual navigation under changing conditions dubbed SeqSLAM. Instead of calculating the single location most likely given a current image, our approach calculates the best candidate matching location within every local navigation sequence. Localization is then achieved by recognizing coherent sequences of these “local best matches”. This approach removes the need for global matching performance by the vision front-end - instead it must only pick the best match within any short sequence of images. The approach is applicable over environment changes that render traditional feature-based techniques ineffective. Using two car-mounted camera datasets we demonstrate the effectiveness of the algorithm and compare it to one of the most successful feature-based SLAM algorithms, FAB-MAP. The perceptual change in the datasets is extreme; repeated traverses through environments during the day and then in the middle of the night, at times separated by months or years and in opposite seasons, and in clear weather and extremely heavy rain. While the feature-based method fails, the sequence-based algorithm is able to match trajectory segments at 100{\%} precision with recall rates of up to 60{\%}.},
author = {Milford, Michael J. and Wyeth, Gordon F.},
doi = {10.1109/ICRA.2012.6224623},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1643--1649},
title = {{SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights}},
year = {2012}
}
